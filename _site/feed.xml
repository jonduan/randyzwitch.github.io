<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>randyzwitch - Articles</title>
    <description></description>
    <link>
    http://randyzwitch.com</link>
    
      
      <item>
        <title>Bulk Downloading Adobe Analytics Data</title>
        
          <description>&lt;p&gt;&lt;em&gt;This blog post also serves as release notes for RSiteCatalyst v1.4.9, as only one feature was added (batch report request and download). But it’s a feature big enough for its own post!&lt;/em&gt;&lt;/p&gt;

</description>
        
        <pubDate>Thu, 21 Jul 2016 04:25:02 -0400</pubDate>
        <link>
        http://randyzwitch.com/rsitecatalyst-bulk-download-version-1-4-9-release-notes/</link>
        <guid isPermaLink="true">http://randyzwitch.com/rsitecatalyst-bulk-download-version-1-4-9-release-notes/</guid>
        <content type="html" xml:base="/rsitecatalyst-bulk-download-version-1-4-9-release-notes/">_This blog post also serves as release notes for RSiteCatalyst v1.4.9, as only one feature was added (batch report request and download). But it's a feature big enough for its own post!_

Recently, I was asked how I would approach replicating the [market basket analysis](http://33sticks.com/rsitecatalyst-market-basket-analysis-adobe-analytics/) blog post I wrote for [33 Sticks](http://33sticks.com/), but using a lot more data. Like, months and months of order-level data. While you _might_ be able to submit multiple months worth of data in a single RSiteCatalyst call, it's a lot more elegant to request data from the Adobe Analytics API in several calls. With the new batch-submit and batch-receive functionality in RSiteCatalyst, this process can be a LOT faster.

## Non-Batched Method

Prior to version 1.4.9 of RSiteCatalyst, API calls could only be made in a serial fashion:

{% highlight R linenos %}
library(RSiteCatalyst)
library(dplyr)

SCAuth(Sys.getenv(&quot;USER&quot;, &quot;&quot;), Sys.getenv(&quot;SECRET&quot;, &quot;&quot;))

combined_orders &lt;- data.frame()
for(d in seq(as.Date(&quot;2016-06-01&quot;), as.Date(&quot;2016-06-30&quot;), by = &quot;day&quot;)){

  d_ &lt;- as.character(as.Date(d, origin = &quot;1970-01-01&quot;))
  print(d_)

  order_details &lt;- QueueRanked(
    reportsuite.id = &quot;reportsuite&quot;,
    date.from = d_,
    date.to = d_,
    elements = c('evar13', 'product'),
    metrics = c('revenue', 'units', 'orders'),
    top = c(50000, 10000),
    interval.seconds = 60
  )

  order_details$order_date &lt;- d_
  combined_orders &lt;- rbind.fill(combined_orders, order_details)
  rm(order_details)

}
{% endhighlight %}

The underlying assumption from a package development standpoint was that the user would be working in an interactive fashion; submit a report request, wait to get the answer back. There's nothing inherently wrong with this code from an R standpoint that made this a slow process, you just had to wait until one report was calculated by the Adobe Analytics API until the next one was submitted.

## Batch Method

Of course, most APIs can process multiple calls simultaneously, and the Adobe Analytics API is no exception. Thanks to user [shashispace](https://github.com/shashispace), it's now possible to submit all of your report calls at once, then retrieve the results:

{% highlight R linenos %}
queued &lt;- list()
for(d in seq(as.Date(&quot;2016-06-01&quot;), as.Date(&quot;2016-06-30&quot;), by = &quot;day&quot;)){
  d_ &lt;- as.character(as.Date(d, origin = &quot;1970-01-01&quot;))

  print(d_)

  reportid &lt;- QueueRanked(
    reportsuite.id = &quot;reportsuite&quot;,
    date.from = d_,
    date.to = d_,
    elements = c('evar13', 'product'),
    metrics = c('revenue', 'units', 'orders'),
    top = c(50000, 10000),
    interval.seconds = 1,
    enqueueOnly = TRUE
  )

  queued &lt;- cbind(queued, reportid)

}

queued_df &lt;- data.frame()
for (i in queued){
  queued_df &lt;- bind_rows(queued_df, GetReport(i))
}
{% endhighlight %}

This code is nearly identical to the serial snippet above, except for 1) the addition of the `enqueueOnly = TRUE` keyword argument and 2) lowering the `interval.seconds` keyword argument to `1` second instead of `60`. When you use the enqueueOnly keyword, instead of returning the report results back, a `Queue*` function will return the `report.id`; by accumulating these `report.id` values in a list, we can next retrieve the reports and bind them together using dplyr.

## Performance gain: 4x speed-up

Although the code snippets are nearly identical, it is way faster to submit the reports all at once then retrieve the results. By submitting the requests all at once, the API will process numerous calls at once, and while you are retrieving the results of one call the others will continue to process in the background.

I wouldn't have thought this would make such a difference, but retrieving one month of daily order-level data went from taking 2420 seconds to 560 seconds! If you were to retrieve the same amount of daily data, but for an entire year, that would mean saving 6 hours in processing time.

## Keep The Pull Requests Coming!

The last several RSiteCatalyst releases have been driven by contributions from the community and I couldn't be happier! Given that I don't spend much time in my professional life now using Adobe Analytics, having improvements driven by a community of users using the library daily is just so rewarding.

So please, if you have a comment for improvement (and especially if you find a bug), please submit an [issue on GitHub](https://github.com/randyzwitch/RSiteCatalyst/issues). Submitting questions and issues to GitHub is the easiest way for me to provide support, while also giving other users the possibility to answer your question before I might. It will also provide a means for others to determine if they are experiencing a new or previously-known problem.</content>
      </item>
      
    
      
      <item>
        <title>Adobe Analytics Clickstream Data Feed: Calculations and Outlier Analysis</title>
        
          <description>&lt;p&gt;In a previous post, I outlined how to load &lt;a href=&quot;http://randyzwitch.com/adobe-analytics-clickstream-data-feed-relational-database/&quot;&gt;daily Adobe Analytics Clickstream data feeds&lt;/a&gt; into a PostgreSQL database. While this isn’t a long-term scalable solution for large e-commerce companies doing millions of page views per day, for exploratory analysis a relational database structure can work well until a more robust solution is put into place (such as Hadoop/Spark).&lt;/p&gt;

</description>
        
        <pubDate>Tue, 24 May 2016 07:11:20 -0400</pubDate>
        <link>
        http://randyzwitch.com/adobe-analytics-clickstream-data-feed-calculations/</link>
        <guid isPermaLink="true">http://randyzwitch.com/adobe-analytics-clickstream-data-feed-calculations/</guid>
        <content type="html" xml:base="/adobe-analytics-clickstream-data-feed-calculations/">In a previous post, I outlined how to load [daily Adobe Analytics Clickstream data feeds](http://randyzwitch.com/adobe-analytics-clickstream-data-feed-relational-database/) into a PostgreSQL database. While this isn't a long-term scalable solution for large e-commerce companies doing millions of page views per day, for exploratory analysis a relational database structure can work well until a more robust solution is put into place (such as Hadoop/Spark).

## Data Validation &lt;groan&gt;

Before digging too deeply into the data, we should validate that data from the data feed in our database ([custom database view code](https://gist.github.com/randyzwitch/7a9c48e7132e6ed9dfb0d02ec906961c)) matches what we observe from other sources (mainly, the Adobe Analytics interface and/or [RSiteCatalyst](http://randyzwitch.com/tag/rsitecatalyst/)). Given the Adobe Analytics data feed represents an export of the underlying data, and Adobe provides the formulas in the [data feed documentation](https://marketing.adobe.com/resources/help/en_US/sc/clickstream/datafeeds_calculate.html), _in theory_ you should be able to replicate the numbers exactly:

{% highlight R linenos %}
# &quot;Source 1&quot;: Pull data from the API using RSiteCatalyst
library(&quot;RSiteCatalyst&quot;)
SCAuth(Sys.getenv(&quot;USER&quot;, &quot;&quot;), Sys.getenv(&quot;SECRET&quot;, &quot;&quot;))
overtime &lt;- QueueOvertime(&quot;zwitchdev&quot;,
                           date.from = &quot;2016-04-01&quot;,
                           date.to = &quot;2016-05-17&quot;,
                           metrics = c(&quot;pageviews&quot;, &quot;visits&quot;, &quot;visitors&quot;),
                           date.granularity = &quot;day&quot;)

# &quot;Source 2&quot;: Pull data from Postgres database
library(RPostgreSQL)

# Connect to database
conn &lt;- dbConnect(dbDriver(&quot;PostgreSQL&quot;),
                 user=&quot;postgres&quot;,
                 password=&quot;&quot;,
                 host=&quot;localhost&quot;,
                 port=5432,
                 dbname=&quot;adobe&quot;)

dbdata &lt;- dbGetQuery(conn,
                     &quot;select
                     date(date_time) as date_localtime,
                     sum(CASE WHEN post_page_event = '0' THEN 1 END) as pageviews,
                     count(distinct ARRAY_TO_STRING(ARRAY[post_visid_high::text, post_visid_low::text, visit_num::text], '')) as visits,
                     count(distinct ARRAY_TO_STRING(ARRAY[post_visid_high::text, post_visid_low::text], '')) as visitors
                     from usefuldata
                     where date_time between '2016-04-01' and '2016-05-18' and exclude_hit = '0'
                     group by 1
                     order by 1;&quot;)

# Compare data sources
&gt; diff_pv = table(overtime$pageviews - dbdata$pageviews)
&gt; diff_pv

0
47

&gt; diff_visits = table(overtime$visits - dbdata$visits)
&gt; diff_visits

0
47

&gt; diff_visitors = table(overtime$visitors - dbdata$visitors)
&gt; diff_visitors

0
47
{% endhighlight %}

The code snippet above shows the validation, and sure enough, the &quot;two different sources&quot; show the same exact values (i.e. differences are 0), so everything has been loaded properly into the PostgreSQL database.

## Finding Anomalies For Creating Bot Rules

With the data validated, we can now start digging deeper into the data. As an example, although I have [bot filtering](https://marketing.adobe.com/resources/help/en_US/reference/bot_rules.html) enabled, this only handles bots on the [IAB bot list](http://www.iab.com/guidelines/iab-abc-international-spiders-bots-list/) but not necessarily people trying to scrape my site (or worse).

To create a [custom bot rule in Adobe Analytics](https://marketing.adobe.com/resources/help/en_US/reference/t_create_bot_rules.html), you can use IP address(es) and/or User-Agent string. However, as part of data exploration we are not limited to just these features (assuming, of course, that you can map your feature set back to an IP/User-Agent combo). To identify outlier behavior, I'm going to use a technique called '[local outlier factors](http://www.dbs.ifi.lmu.de/Publikationen/Papers/LOF.pdf)' using the [Rlof](https://cran.r-project.org/web/packages/Rlof/index.html) package in R with the following data features:

  * Distinct Days Visited
  * Total Pageviews
  * Total Visits
  * Distinct Pages Viewed
  * Pageviews Per Visit
  * Average Views Per Page

These aren't the only features I could've used, but it should be pretty easy to view bot/scraper traffic using these metrics. Here's the code:

{% highlight R linenos %}
# Local outlier factor calculation
library(RPostgreSQL)
library(Rlof)

conn &lt;- dbConnect(dbDriver(&quot;PostgreSQL&quot;),
                  user=&quot;postgres&quot;,
                  password=&quot;&quot;,
                  host=&quot;localhost&quot;,
                  port=5432,
                  dbname=&quot;adobe&quot;)

metrics_lof &lt;- dbGetQuery(conn,
                          &quot;select
                          ip,
                          distinct_days_visited,
                          pageviews,
                          visits,
                          distinct_pages_viewed,
                          pageviews/visits::double precision as pv_per_visit,
                          pageviews/distinct_pages_viewed::double precision as avg_views_per_page
                          from
                          (
                          select
                          ip,
                          sum(CASE WHEN post_page_event = '0' THEN 1 END) as pageviews,
                          count(distinct ARRAY_TO_STRING(ARRAY[post_visid_high::text, post_visid_low::text, visit_num::text, visit_start_time_gmt::text], '')) as visits,
                          count(distinct post_pagename) as distinct_pages_viewed,
                          count(distinct date(date_time)) as distinct_days_visited
                          from usefuldata
                          where exclude_hit = '0'
                          group by 1
                          ) a
                          where visits &gt; 1 and pageviews &gt; 1;&quot;)


# The higher the value of k, the more likely lof will be calculated...
# ...but more generic the clusters
# NaN/Inf occurs with points on top of one another/div by zero, which is likely...
# ...with web data when most visitors have 1-2 sessions
df_lof&lt;-lof(metrics_lof[, 2:7],k = 20)

# Append results, get top 500 worst scoring IP addresses
results &lt;- cbind(metrics_lof, df_lof)[order(-df_lof),]
worst500 &lt;- head(subset(results, !is.infinite(df_lof)), 500)
{% endhighlight %}

A local outlier factor greater than 1 is classified as a potential outlier. Here's a visual of the lof scores for the top 500 _worst_ scoring IP addresses [(vegalite R graph code)](https://gist.github.com/randyzwitch/178d72e01e30943f6af82c48a47c4478):

&lt;div id=&quot;vis&quot;&gt;&lt;/div&gt;

We can see from the graph that there are at least 500 IP addresses that are potential outliers (since the line doesn't go below a lof value of 1). These points are now a good starting place to go back to our overall table and inspect the entire datafeed records by IP address.

## But what about business value?

The example above just scratches the surface on what's possible when you have access to the raw data from Adobe Analytics. It's possible to do these calculations on my laptop using R because I only have a few hundred-thousand records and IP addresses. But this kind of ops work is pretty low-value, since unless you are trying to detect system hacking, trying to find hidden scrapers/spiders in your data to filter out just modifies the denominator of your KPIs it doesn't lead to real money per se.

In the last post of this series, I'll cover how to work with the datafeed using Spark, and provide an example of using [Spark MLLib](http://spark.apache.org/docs/latest/mllib-guide.html) to increase site engagement.</content>
      </item>
      
    
      
      <item>
        <title>Adobe: Give Credit. You DID NOT Write RSiteCatalyst.</title>
        
          <description>&lt;p&gt;&lt;strong&gt;EDIT 5/10/2016 1:30pm: Several folks from Adobe Analytics/Adobe Marketing Cloud have contacted me, and everything is resolved. I can’t untweet other people’s retweets/shares or delete comments on LinkedIn, but if everyone could stop sharing any more that would be great. 🙂&lt;/strong&gt;
* * *&lt;/p&gt;

</description>
        
        <pubDate>Mon, 09 May 2016 08:31:51 -0400</pubDate>
        <link>
        http://randyzwitch.com/rsitecatalyst-not-adobe-product/</link>
        <guid isPermaLink="true">http://randyzwitch.com/rsitecatalyst-not-adobe-product/</guid>
        <content type="html" xml:base="/rsitecatalyst-not-adobe-product/">**EDIT 5/10/2016 1:30pm: Several folks from Adobe Analytics/Adobe Marketing Cloud have contacted me, and everything is resolved. I can't untweet other people's retweets/shares or delete comments on LinkedIn, but if everyone could stop sharing any more that would be great. 🙂**
* * *

As an [author of several open-source software projects](https://github.com/randyzwitch), I've taken for granted that people using the software share the same community values as I do. Open-source authors provide their code &quot;[free](http://www.howtogeek.com/howto/31717/what-do-the-phrases-free-speech-vs.-free-beer-really-mean/)&quot; to the community so that others may benefit without having to re-invent the wheel. The only _expectation_ (but not an actual _requirement_ per se), is attribution to the package author(s) as a thank you for the time and effort they put into writing and maintaining a quality piece of software.

However, when others take direct credit for writing a package they did not, it crosses into a different realm. Adobe, you DID NOT write RSiteCatalyst, nor have you made any [meaningful contributions](https://github.com/randyzwitch/RSiteCatalyst/graphs/contributors). To take credit for RSiteCatalyst, either implicitly or explicitly, is a slight to the work of those who have contributed.

## Adobe Summit 2014: Attribution!

In the beginning, there seemed to be no problem providing [proper attribution](https://blogs.adobe.com/digitalmarketing/analytics/playing-hits-summit-2014-filtered-metrics-error-monitoring/). I count Ben Gaines as one of my stronger professional acquaintances (dare I say, even a friend), so I was honored that he not only mentioned me on stage at his Adobe Summit 2014 presentation, but also followed up with an [official Adobe blog post](https://blogs.adobe.com/digitalmarketing/analytics/playing-hits-summit-2014-filtered-metrics-error-monitoring/) re-capping his main points:

![rsitecatalyst-attribution](/wp-content/uploads/2016/05/rsitecatalyst-attribution-1024x603.png)

Perfect. My package got wide exposure to the intended audience, which in turn makes it easier to devote time for development and maintenance. The recognition also helped me professionally in that time period, so if I never thanked you publicly Ben, thank you!

## Adobe Summit 2015: An Inconspicuous Absence

In 2015, RSiteCatalyst moved from a &quot;Tip&quot; to a [full-fledged presentation](http://video.tv.adobe.com/v/2314t_876d7009-77fb-4a67-86bc-70475fddf88e/). I was honored when I first heard that an entire hour would be dedicated to reviewing the package, but no attribution was given:

![rsitecatalyst-resources](/wp-content/uploads/2016/05/rsitecatalyst-resources-1024x569.png)

I mean, it was obviously okay to link to non-Adobe websites like [statmethods.net](http://statmethods.net/) (a great reference btw) and to [Shiny](http://shiny.rstudio.com/)...but okay, attribution is not a requirement.

## Adobe Summit 2016: 'We at Adobe...'

The non-mention at Adobe Summit 2015 could be attributed to an oversight; the following during the [2016 RSiteCatalyst Adobe Summit presentation](http://summit.adobe.com/na/sessions/summit-online/online2016/#/video/15150t_b09a171f-dc7c-4ff3-b71c-cf79dedb6e94) cannot:

![rsitecatalyst-randy-zwitch](/wp-content/uploads/2016/05/rsitecatalyst-randy-zwitch-1024x659.png)

Just so we're clear, this isn't me noticing the slide notes in a PDF or PPT I shouldn't have access to. The screenshot above is directly from the [Adobe Summit video](http://summit.adobe.com/na/sessions/summit-online/online2016/#/video/15150t_b09a171f-dc7c-4ff3-b71c-cf79dedb6e94) and the statement was said nearly verbatim during the presentation. And it's not like this was a one-off comment...it's the same damn presentation as 2015, and I KNOW this script went through several rounds of review and practice by the presenters.

## It Costs $0 To Do What Is Right

It may be hard for RSiteCatalyst users to believe, but this was the first open-source project I ever wrote AND the means by which I learned how to write R code AND the first time I ever accessed an API. Since then, [Willem Paling](https://github.com/WillemPaling) did an amazing job refactoring/re-writing a majority of the package when the Adobe Analytics API was updated from version 1.3 to 1.4, and there have been numerous other contributions from the user community. Maybe even one day, [the repo](https://github.com/randyzwitch/RSiteCatalyst) will reach even 100 stars on GitHub...

But save for a [single commit](https://github.com/randyzwitch/RSiteCatalyst/commit/706d2997cc9ec3a95eff756308110c12e217e1ca) to a README file from an employee, Adobe you have contributed _zero_to the development and maintenance of this package. To claim otherwise is beyond distasteful to the ethos of open-source software. I've never asked for compensation of any kind; and again, I recognize that you don't even need to attribute the work at all.

Just don't take credit yourselves for providing this functionality to your customers. You did not write [RSiteCatalyst](https://github.com/randyzwitch/RSiteCatalyst) Adobe, a community of (unpaid) volunteers did.</content>
      </item>
      
    
      
      <item>
        <title>Travis CI: &quot;You Have Too Many Tests LOLZ!&quot;</title>
        
          <description>&lt;p&gt;As part of getting &lt;a href=&quot;http://randyzwitch.com/rsitecatalyst-version-1-4-8-release-notes/&quot;&gt;RSiteCatalyst 1.4.8 ready for CRAN&lt;/a&gt;, I’ve managed to accumulate hundreds of &lt;a href=&quot;https://github.com/hadley/testthat&quot;&gt;testthat&lt;/a&gt; tests across 63 test files. Each of these tests runs on &lt;a href=&quot;http://randyzwitch.com/authentication-travis-ci/&quot;&gt;Travis CI against an authenticated API&lt;/a&gt;, and the API frequently queues long-running reports. Long-story-short, my builds started failing, creating the error log message quoted below:&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;No output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.&lt;/p&gt;
&lt;/blockquote&gt;

</description>
        
        <pubDate>Tue, 05 Apr 2016 07:41:53 -0400</pubDate>
        <link>
        http://randyzwitch.com/travisci-10minute-timeout-build-error/</link>
        <guid isPermaLink="true">http://randyzwitch.com/travisci-10minute-timeout-build-error/</guid>
        <content type="html" xml:base="/travisci-10minute-timeout-build-error/">As part of getting [RSiteCatalyst 1.4.8 ready for CRAN](http://randyzwitch.com/rsitecatalyst-version-1-4-8-release-notes/), I've managed to accumulate hundreds of [testthat](https://github.com/hadley/testthat) tests across 63 test files. Each of these tests runs on [Travis CI against an authenticated API](http://randyzwitch.com/authentication-travis-ci/), and the API frequently queues long-running reports. Long-story-short, my builds started failing, creating the error log message quoted below:

&gt; No output has been received in the last 10m0s, this potentially indicates a stalled build or something wrong with the build itself.

## Stalled Build?

The most frustrating about this error is that all my tests run (albeit, a looooong time) successfully through RStudio, so I wasn't quite sure what the problem was with the [Travis CI](https://travis-ci.org/) build. Travis CI does provide a comment about this in their [documentation](https://docs.travis-ci.com/user/common-build-problems/#My-builds-are-timing-out), but even then it didn't solve my problem:

&gt; When a long running command or compile step regularly takes longer than 10 minutes without producing any output, you can adjust your build configuration to take that into consideration.
&gt;
&gt; The shell environment in our build system provides a function that helps to work around that, at least for longer than 10 minutes.
&gt;
&gt; If you have a command that doesn’t produce output for more than 10 minutes, you can prefix it with `travis_wait`, a function that’s exported by our build environment.

The `travis_wait` command would work if I were installing packages, but my errors were during tests, so this parameter isn't the answer. Luckily, `testthat` provides a test filtering mechanism, providing a solution by allowing the tests to be broken up into smaller chunks.

## Regex To The Rescue...

For many applications, the default testthat configuration example will work just well:

&gt; R CMD check  
&gt; Create tests/testthat.R that contains:  
&gt; library(testthat)  
&gt; library(yourpackage)  
&gt; test_check(&quot;yourpackage&quot;)  

However, hidden within the `test_check()` arguments is `filter`, which will take a regular expression to filter which files in the `test` folder will get run when the command is triggered by R CMD check. Why is this important? Because each time a new `test_check()` function gets called, output gets written to stdout, and thus avoids 10 minutes passing without producing any output. Here's an example of what my successful build logs now look like ([GitHub code for the testthat code structure](https://github.com/randyzwitch/RSiteCatalyst/tree/master/tests)):

&gt; checking tests...  
&gt; Running ‘testthat-build.R’  
&gt; Running ‘testthat-get.R’ [5s/267s]  
&gt; Running ‘testthat-queuefallout.R’ [1s/59s]  
&gt; Running ‘testthat-queueovertime.R’ [3s/210s]  
&gt; Running ‘testthat-queuepathing.R’ [2s/55s]  
&gt; Running ‘testthat-queueranked.R’ [2s/183s]  
&gt; Running ‘testthat-queuesummary.R’ [2s/136s]  
&gt; Running ‘testthat-queuetrended.R’ [17s/346s]  
&gt; Running ‘testthat-save.R’ [1s/46s]  
&gt; OK  

You can now see that instead of getting a single output message of `Running testthat.R`, I have nine separate test files running, none of which take 10 minutes to complete. For my package, each of my test files is labeled based on the function name, and I can end up using really simple regex literals such as the following:

{% highlight R linenos %}
library(testthat)
test_check(&quot;RSiteCatalyst&quot;, filter = &quot;get&quot;)
{% endhighlight %}

So each file with the word &quot;get&quot; in the filename will be run by this function; I'm not worried about writing complex regexes here, since at worst I my matching is too broad and I run the same test multiple times.

## ...But Be Careful Of Case-Sensitivity!

The one caveat to simple regex filtering above is that if you're not careful, you'll get no match from your `test_check()` function, which will fail the build on Travis CI. I spent hours trying to figure out why my tests ran fine on OSX, but failed on Travis. Eventually, I even [filed an issue](https://github.com/hadley/testthat/issues/434) against hadley's repo, feeling silly as soon as I found out that my error was due to case-sensitivity in Linux by not OSX (or Windows for that matter).

So, pay attention, and if all else fails, go with `filter = &quot;summary|Summary&quot;` or similar to match the case of your filenames!

## You Can Never Really Have Too Many Tests

Obviously, the title of this blog post is in jest; Travis CI doesn't care what you're running or comments on how many tests you run. But hopefully this blog post provides the answer to the next person down the line running into this issue. Don't delete your tests, run multiple `test_check()` functions and the printing every few minutes of the file name to the console should resolve the problem.</content>
      </item>
      
    
      
      <item>
        <title>RSiteCatalyst Version 1.4.8 Release Notes</title>
        
          <description>&lt;p&gt;For being in RSiteCatalyst retirement, I’m ending up working on more functionality lately ¯_(ツ)_/¯. Here are the changes for RSiteCatalyst 1.4.8, which should be &lt;a href=&quot;https://cran.r-project.org/web/packages/RSiteCatalyst/index.html&quot;&gt;available on CRAN&lt;/a&gt; shortly:&lt;/p&gt;

</description>
        
        <pubDate>Mon, 04 Apr 2016 06:05:15 -0400</pubDate>
        <link>
        http://randyzwitch.com/rsitecatalyst-version-1-4-8-release-notes/</link>
        <guid isPermaLink="true">http://randyzwitch.com/rsitecatalyst-version-1-4-8-release-notes/</guid>
        <content type="html" xml:base="/rsitecatalyst-version-1-4-8-release-notes/">For being in RSiteCatalyst retirement, I'm ending up working on more functionality lately ¯\_(ツ)_/¯. Here are the changes for RSiteCatalyst 1.4.8, which should be [available on CRAN](https://cran.r-project.org/web/packages/RSiteCatalyst/index.html) shortly:

## Segment Stacking

RSiteCatalyst now has the ability to take multiple values in the `segment.id` keyword for the `Queue*` functions. This functionality was graciously provided by [Adam Gitzes](https://twitter.com/FootballActuary), closing an [issue](https://github.com/randyzwitch/RSiteCatalyst/issues/129) that was nearly a year old. At times it felt like I was hazing him with change requests, but for Adam's first open-source contribution, this is a huge addition in functionality.

So now you are able to pass multiple segments into a function call and get an 'AND' behavior like so:

{% highlight R linenos %}
stacked_seg &lt;- QueueRanked(&quot;zwitchdev&quot;,
                          &quot;2016-03-08&quot;,
                          &quot;2016-03-09&quot;,
                          &quot;pageviews&quot;,
                          &quot;page&quot;,
                          segment.id = c(&quot;5433e4e6e4b02df70be4ac63&quot;, &quot;54adfe3de4b02df70be5ea08&quot;)
                          )
{% endhighlight %}

The result (Visits from Social AND Visits from Apple Browsers):

![rsitecatalyst-segment-stacking](/wp-content/uploads/2016/04/rsitecatalyst-segment-stacking-1024x58.png)

## QueueSummary: Now with `date.to` and `date.from` keywords

In response to [GitHub issue #158](https://github.com/randyzwitch/RSiteCatalyst/issues/158), `date.to` and `date.from` parameters were added; this was a minor, but long-term oversight (it's always been possible to do this in the Adobe Analytics API). So now rather than just specifying the `date` keyword and getting a full-year summary or a full-month, you can specify any arbitrary start/end dates.

## Trivial Fixes: Silenced `httr` message, clarified documentation

Starting with the newest version of httr, you get a message for any API call where the encoding wasn't set. So for long running `Queue*` requests, you may have received dozens of warnings to stdout about `&quot;No encoding supplied: defaulting to UTF-8.&quot;` This has been remedied, and the warning should no longer occur.

Also, the [documentation for the `Queue*` functions](https://github.com/randyzwitch/RSiteCatalyst/blob/master/man/QueueRanked.Rd#L86-#L93) was clarified to show an example of using SAINT classifications as the report breakdown; hopefully this didn't cause too much confusion to anyone else.

## Volunteers Wanted!

As I referenced in the first paragraph, while I'm fully committed to maintaining RSiteCatalyst, I don't currently have the time/desire to continue to develop the package to improve functionality. Given that I don't use this package for my daily work, it's hard for me to dedicate time to the project.

Thanks again to Adam Gitzes who stepped up and provided significant effort to close an outstanding feature request. I would love if others in the digital analytics community would follow Adam's lead; don't worry about whether you are 'good enough', get a working solution together and we'll figure out how to harden the code and get it merged. Be the code change you want to see the world 🙂</content>
      </item>
      
    
      
      <item>
        <title>Adobe Analytics Clickstream Data Feed: Loading To Relational Database</title>
        
          <description>&lt;p&gt;In my previous post about the &lt;a href=&quot;http://randyzwitch.com/adobe-analytics-clickstream-raw-data-feed/&quot;&gt;Adobe Analytics Clickstream Data Feed&lt;/a&gt;, I showed how it was possible to take a single day worth of data and build a dataframe in R. However, most likely your analysis will require using multiple days/weeks/months of data, and given the size and complexity of the feed, loading the files into a relational database makes a lot of sense.&lt;/p&gt;

</description>
        
        <pubDate>Fri, 18 Mar 2016 10:42:23 -0400</pubDate>
        <link>
        http://randyzwitch.com/adobe-analytics-clickstream-data-feed-relational-database/</link>
        <guid isPermaLink="true">http://randyzwitch.com/adobe-analytics-clickstream-data-feed-relational-database/</guid>
        <content type="html" xml:base="/adobe-analytics-clickstream-data-feed-relational-database/">In my previous post about the [Adobe Analytics Clickstream Data Feed](http://randyzwitch.com/adobe-analytics-clickstream-raw-data-feed/), I showed how it was possible to take a single day worth of data and build a dataframe in R. However, most likely your analysis will require using multiple days/weeks/months of data, and given the size and complexity of the feed, loading the files into a relational database makes a lot of sense.

Although there may be database-specific &quot;fast-load&quot; tools more appropriate for this application, this blog post will show how to handle this process using only R and [PostgresSQL](http://www.postgresql.org/download/).

## File Organization

Before getting into the loading of the data into PostgreSQL, I like to sort my files by type into separate directories (remember from the [previous post](http://randyzwitch.com/adobe-analytics-clickstream-raw-data-feed/), you'll receive three files per day). R makes OS-level operations simple enough:

{% highlight R linenos %}
#### 1. Setting directory to FTP folder where files incoming from Adobe
## Has ~2000 files in it from 2 years of data
setwd(&quot;~/Downloads/datafeed/&quot;)

#### 2. Sort files into three separate folders
## Manifests - plain text files
if(!dir.exists(&quot;manifest&quot;)){
  dir.create(&quot;manifest&quot;)
  lapply(list.files(pattern = &quot;*.txt&quot;), function(x) file.rename(x, paste(&quot;manifest&quot;, x, sep = &quot;/&quot;)))
}

## Server calls tsv.gz
if(!dir.exists(&quot;servercalls&quot;)){
  dir.create(&quot;servercalls&quot;)
  lapply(list.files(pattern = &quot;*.tsv.gz&quot;), function(x) file.rename(x, paste(&quot;servercalls&quot;, x, sep = &quot;/&quot;)))
}

## Lookup files .tar.gz
if(!dir.exists(&quot;lookup&quot;)){
  dir.create(&quot;lookup&quot;)
  lapply(list.files(pattern = &quot;*.tar.gz&quot;), function(x) file.rename(x, paste(&quot;lookup&quot;, x, sep = &quot;/&quot;)))
}
{% endhighlight %}

Were there more file types, I could've abstracted this into a function instead of copying the code three times, but the idea is the same: Check to see if the directory exists, if it doesn't then create it and move the files into the directory.

## Connecting and Loading Data to PostgreSQL from R

Once we have our files organized, we can begin the process of loading the files into PostgreSQL using the [RPostgreSQL](https://cran.r-project.org/web/packages/RPostgreSQL/index.html) R package.  RPostgreSQL is [DBI-compliant](https://github.com/rstats-db/DBI), so the connection string is the same for any other type of database engine; the biggest caveat of loading your `servercall` data into a database is the first load is almost guaranteed to require loading as text (using `colClasses = &quot;character&quot;` argument in R). The reason that you'll need to load the data as text is that Adobe Analytics implementations necessarily change over time; text is the only column format that allows for no loss of data (we can fix the schema later within Postgres either by using `ALTER TABLE` or by writing a view).

{% highlight R linenos %}
library(RPostgreSQL)

# Connect to database
conn = dbConnect(dbDriver(&quot;PostgreSQL&quot;),
                 user=&quot;postgres&quot;,
                 password=&quot;&quot;,
                 host=&quot;localhost&quot;,
                 port=5432,
                 dbname=&quot;adobe&quot;)

#Set directory to avoid having to use paste to build urls
setwd(&quot;~/Downloads/datafeed/servercalls&quot;)

#Set column headers for server calls
column_headers &lt;- read.delim(&quot;~/Downloads/datafeed/lookup/column_headers.tsv&quot;, stringsAsFactors=FALSE)

#Loop over entire list of files
#Setting colClasses to character only way to guarantee all data loads
#File formats or implementations can change over time; fix schema in database after data loaded
for(file in list.files()){
  print(file)
  df &lt;- read.csv2(file, sep = &quot;\t&quot;, header = FALSE, stringsAsFactors = FALSE, colClasses = &quot;character&quot;)
  dbWriteTable(conn, name = 'servercalls', value = df, row.names=FALSE, append = TRUE)
  rm(df)
}

#Run analyze in PostgreSQL so that query planner has accurate information
dbGetQuery(conn, &quot;analyze servercalls&quot;)
{% endhighlight %}

With this small amount of code, we've generated the table definition structure ([see here for the underlying Postgres code](https://gist.github.com/randyzwitch/e26b97d26689b6b31044)), loaded the data, and told Postgres to analyze the table to gather statistics for efficient queries. Sweet, two years of data loaded with minimal effort!

## Loading Lookup Tables Into PostgreSQL

With the server call data loaded into our database, we now need to load our lookup tables. Lucky for us, these do maintain a constant format, so we don't need to worry about setting all the fields to text, RPostgreSQL should get the column types correct.

{% highlight R linenos %}
library(RPostgreSQL)

# Connect to database
conn = dbConnect(dbDriver(&quot;PostgreSQL&quot;),
                 user=&quot;postgres&quot;,
                 password=&quot;&quot;,
                 host=&quot;localhost&quot;,
                 port=5432,
                 dbname=&quot;adobe&quot;)

setwd(&quot;~/Downloads/datafeed/lookup/&quot;)

#Create function due to repetitiveness
#Since we're loading lookup tables with mostly same values each time, put source file in table
loadlookup &lt;- function(tblname){
  df &lt;- read.csv2(paste(tblname,&quot;.tsv&quot;, sep=&quot;&quot;), sep = &quot;\t&quot;, header = FALSE, stringsAsFactors = FALSE)
  df$file &lt;- file
  dbWriteTable(conn, name = tblname, value = df, row.names=FALSE, append = TRUE)
}

#untar files, place in directory by day
for(file in list.files(pattern = &quot;*.tar.gz&quot;)){
  print(file)
  untar(file)

  for(tbl in c(&quot;browser_type&quot;, &quot;browser&quot;, &quot;color_depth&quot;, &quot;column_headers&quot;,
               &quot;connection_type&quot;, &quot;country&quot;, &quot;event&quot;, &quot;javascript_version&quot;,
               &quot;languages&quot;, &quot;operating_systems&quot;, &quot;plugins&quot;, &quot;referrer_type&quot;,
               &quot;resolution&quot;, &quot;search_engines&quot;)){

    loadlookup(tbl)

  }
}
{% endhighlight %}

**SHORTCUT**: The dimension tables that are common to all report suites don't really change over time, although that isn't guaranteed.  In the 758 days of files I loaded ([code](https://gist.github.com/randyzwitch/5ed2f4fc8574b91efd29)), the only files having more than one value for a given key were: `browser`, `browser_type`, `operating_system`, `search_engines`, `event` (report suite specific for every company) and `column_headers` (report suite specific for every company). So if you're doing a bulk load of data, it's generally sufficient to use the newest lookup table and save yourself some time. If you are processing the data every day, you can use an [upsert process](https://wiki.postgresql.org/wiki/UPSERT) and generally there will be few if any updates.

## Let's Do Analytics!!!!???!!!

_&lt;moan&gt;Why is there always so much ETL work, I want to data science the hell out of some data&lt;/moan&gt;_

At this point, if you were uploading the same amount of data for the traffic my blog does (not much), you'd be about 1-2 hours into loading data, still having done no analysis. In fact, in order to do analysis, you'd still need to modify the column names and types in your `servercalls` table, update the lookup tables to have the proper column names, and maybe you'd even want to pre-summarize the tables into views/materialized views for Page View/Visit/Visitor level. Whew, that's a lot of work just to calculate daily page views.

Yes it is. But taking on a project like this isn't for page views; just use the Adobe Analytics UI!

In a future blog post or two, I'll demonstrate how to use this relational database layout to perform analyses not possible within the Adobe Analytics interface, and also show how we can skip this ETL process altogether using a [schema-on-read process](http://blog.cask.co/2015/03/schema-on-read-in-action/) with Spark.</content>
      </item>
      
    
      
      <item>
        <title>Calling RSiteCatalyst From Python</title>
        
          <description>&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;
  &lt;p dir=&quot;ltr&quot; lang=&quot;en&quot;&gt;
    &lt;a href=&quot;https://twitter.com/randyzwitch&quot;&gt;@randyzwitch&lt;/a&gt; Do you know if anyone has gotten RSiteCat running in a Jupyter Notebook that ran RPY2? Tired of using 2 different environments
  &lt;/p&gt;

&lt;/blockquote&gt;
</description>
        
        <pubDate>Mon, 22 Feb 2016 05:34:44 -0500</pubDate>
        <link>
        http://randyzwitch.com/rsitecatalyst-adobe-analytics-python/</link>
        <guid isPermaLink="true">http://randyzwitch.com/rsitecatalyst-adobe-analytics-python/</guid>
        <content type="html" xml:base="/rsitecatalyst-adobe-analytics-python/">&lt;blockquote class=&quot;twitter-tweet&quot; data-lang=&quot;en&quot;&gt;
  &lt;p dir=&quot;ltr&quot; lang=&quot;en&quot;&gt;
    &lt;a href=&quot;https://twitter.com/randyzwitch&quot;&gt;@randyzwitch&lt;/a&gt; Do you know if anyone has gotten RSiteCat running in a Jupyter Notebook that ran RPY2? Tired of using 2 different environments
  &lt;/p&gt;

  &lt;p&gt;
    — Adam Gitzes (@FootballActuary) &lt;a href=&quot;https://twitter.com/FootballActuary/status/700350988842995712&quot;&gt;February 18, 2016&lt;/a&gt;
  &lt;/p&gt;
&lt;/blockquote&gt;

This will be a very short post, because the only &quot;new&quot; information I'm going to provide is the minimal example to answer the question. Yes, it is in fact possible to call RSiteCatalyst from Python and seems to work well. The most important things are 1) making sure you install [rpy2](http://rpy2.readthedocs.org/en/version_2.7.x/) and 2) loading [Pandas](http://pandas.pydata.org/) (since so much of RSiteCatalyst is API calls returning data frames). It doesn't hurt to already have experience using [RSiteCatalyst](http://randyzwitch.com/tag/rsitecatalyst/) in [R](/tags/#r), since all we're doing here is using Python to pass code to R.

## Setup Code: rpy2 and Pandas

To call an R package from Python, the rpy2 package works very well, both from the REPL and Jupyter Notebook. For RSiteCatalyst, here is the set up code:

{% highlight python linenos %}
import pandas as pd

import rpy2.robjects.packages as rpackages
from rpy2.robjects import pandas2ri

# Activate ability to translate R objects to pandas data frames
pandas2ri.activate()

# Load RSiteCatalyst into Python
rsc = rpackages.importr('RSiteCatalyst')
{% endhighlight %}

With this code run, now you can make calls to the RSiteCatalyst R package, just as if you were in R itself.

## Sample Call: GetReportSuites

Just to prove it works, here's a code snippet using `GetReportSuites()`:

{% highlight python linenos %}
# Call GetReportSuites to confim it works
grs = rsc.GetReportSuites()
pandas2ri.ri2py_dataframe(grs)
{% endhighlight %}

And in Jupyter Notebook, you would see something similar to:

![rsitecatalyst-rpy2](/wp-content/uploads/2016/02/rsitecatalyst-rpy2-1-1024x424.png)

## But, Why?

So that's about it...if you wanted to, you could call RSiteCatalyst from Python without much effort. There aren't a whole lot of reasons to do so, unless like Adam above, you'd rather just use Python. I suppose if you wanted to use some other Python packages, such as [Flask](http://flask.pocoo.org/docs/0.10/) to create a dashboard or [Seaborn](http://stanford.edu/~mwaskom/software/seaborn/) for visualization you might want to do this. Until I got this tweet, it never occurred to me to do this, so YMMV.

_Edit, 2/26/16: Adam Gitzes, who originally asked the question, also provides a different solution using Jupyter Notebook magics at his [blog post here](http://maassmedia.com/r-site-catalyst-python.php)_</content>
      </item>
      
    
      
      <item>
        <title>RSiteCatalyst Version 1.4.7 (and 1.4.6.) Release Notes</title>
        
          <description>&lt;p&gt;It seems as though I missed release notes for version RSiteCatalyst 1.4.6, so we’ll do those and RSiteCatalyst 1.4.7 (now on CRAN) and the same time…&lt;/p&gt;

</description>
        
        <pubDate>Mon, 01 Feb 2016 04:24:04 -0500</pubDate>
        <link>
        http://randyzwitch.com/rsitecatalyst-version-1-4-7-release-notes/</link>
        <guid isPermaLink="true">http://randyzwitch.com/rsitecatalyst-version-1-4-7-release-notes/</guid>
        <content type="html" xml:base="/rsitecatalyst-version-1-4-7-release-notes/">It seems as though I missed release notes for version RSiteCatalyst 1.4.6, so we'll do those and RSiteCatalyst 1.4.7 (now on CRAN) and the same time...

## RSiteCatalyst 1.4.6

This release was mostly tweaking some settings, specifically:

  * Adding a second `top` argument within the `Queue*` functions for more control on results returned. It used to be the case that a breakdown report with the `top` argument would return, say, the top 10 values of the first variable and up to 50,000 values for the breakdown. Now you can control the second level breakdown as well, such as the top 10 pages and top 5 browsers for those pages.
  * Disable checking of the API call before submitting. I never ran into this, but a user was seeing that the API would return errors in validation under high volume. So if you have any weird issues, disable validation using the `validate = FALSE` keyword argument.
  * The package now handles situation where API returns an unexpected type for the `reportID` and automatically converts it to the proper type (low-level issue, not a user-facing issue)

Those changes carry forward into version RSiteCatalyst 1.4.7, so there is no reason for a user to stick with this release.

## RSiteCatalyst 1.4.7 - No more Unicode Errors!

I was surprised it took so long for someone to report this error, but [#151](https://github.com/randyzwitch/RSiteCatalyst/issues/151) finally reported a case from a user in Germany where search keywords were being mangled due to the presence of an umlaut. UTF-8 encoding is now the default for both calling the API and processing the results, so this issue will hopefully not arise again.

Additionally, a `locale` argument has been added, to set the proper locale for your report suite. This is specified through the `SCAuth()` function, with the list of possible &lt;a href=&quot;https://marketing.adobe.com/developer/documentation/analytics-reporting-1-4/r-reportdescriptionlocale&quot; target=&quot;_blank&quot;&gt;locales provided by the Adobe documentation&lt;/a&gt;. So if the even after using 1.4.7 with UTF-8 encoding by default, you are still seeing errors, try setting the locale to the country you are in/country setting of the report suite.

## Feature Requests/Bugs

As always, if you come across bugs or have feature requests, please continue to use [RSiteCatalyst GitHub Issues](https://github.com/randyzwitch/RSiteCatalyst/issues) page to submit issues. Don’t worry about cluttering up the page with tickets, please fill out a new issue for anything you encounter (with code you’ve already tried and is failing), unless you are SURE that it is the same problem someone else is facing.

However, outside of patching really serious bugs, I will likely **not spend any more time improving this package in the future**; my interests have changed, and RSiteCatalyst is pretty much complete as far as I’m concerned. That said, contributors are also _very welcomed_. If there is a feature you’d like added, and especially if you can fix an outstanding issue reported at GitHub, we’d love to have your contributions. Willem and I are both parents of young children and have real jobs outside of open-source software creation, so we welcome any meaningful contributions to RSiteCatalyst that anyone would like to contribute.</content>
      </item>
      
    
      
      <item>
        <title>A Million Text Files And A Single Laptop</title>
        
          <description>&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2016/01/million-files-size.png&quot; alt=&quot;GNU Parallel Cat Unix&quot; /&gt;&lt;/p&gt;

</description>
        
        <pubDate>Thu, 28 Jan 2016 04:53:42 -0500</pubDate>
        <link>
        http://randyzwitch.com/gnu-parallel-medium-data/</link>
        <guid isPermaLink="true">http://randyzwitch.com/gnu-parallel-medium-data/</guid>
        <content type="html" xml:base="/gnu-parallel-medium-data/">![GNU Parallel Cat Unix](/wp-content/uploads/2016/01/million-files-size.png)

More often that I would like, I receive datasets where the data has only been partially cleaned, such as the picture on the right: hundreds, thousands...even millions of tiny files. Usually when this happens, the data all have the same format (such as having being generated by sensors or other memory-constrained devices).

The problem with data like this is that 1) it's inconvenient to think about a dataset as a million individual pieces 2) the data in aggregate are too large to hold in RAM but 3) the data are small enough where using Hadoop or even a relational database seems like overkill.

Surprisingly, with judicious use of [GNU Parallel](http://www.gnu.org/software/parallel/), stream processing and a relatively modern computer, you can efficiently process annoying, &quot;medium-sized&quot; data as described above.

## Data Generation

For this blog post, I used a combination of R and Python to generate the data: the &quot;Groceries&quot; dataset from the [arules](https://cran.r-project.org/web/packages/arules/vignettes/arules.pdf) package for sampling transactions (with replacement), and the Python [Faker (fake-factory)](https://github.com/joke2k/faker) package to generate fake customer profiles and for creating the 1MM+ text files:

{% highlight python linenos %}
#R Code
library(arules)
data(&quot;Groceries&quot;)
write(Groceries, &quot;groceries.txt&quot;, sep = &quot;,&quot;)

#Python Code
import random, csv
from faker import Faker
fake = Faker()
from pandas import DataFrame
import pandas as pd

# Create customer file of 1,234,567 customers with fake data
# Use dataframe index as a way to generate unique customer id
customers = [fake.simple_profile() for x in range(0,1234567)]
customer_df = pd.DataFrame(customers)
customer_df[&quot;cust_id&quot;] = customer_df.index

#Read in transactions file from arules package
with open(&quot;grocerydata.txt&quot;) as f:
    transactions = f.readlines()

#Remove new line character
transactions = [x[0:-1] for x in transactions]

#Generate transactions by cust_id

#file format:
#cust_id::int
#store_id::int
#transaction_datetime::string/datetime
#items::string

#for each customer...
for i in range(0,1234567):
    #...create a file...
    with open('/transactions/custfile_%s' % i, 'w') as csvfile:
        trans = csv.writer(csvfile, delimiter=' ', quotechar='&quot;', quoting=csv.QUOTE_MINIMAL)
        #...that contains all of the transactions they've ever made
        for j in range(1, random.randint(1,365)):
            trans.writerow([i, fake.zipcode(), fake.date_time_this_decade(before_now=True, after_now=False), transactions[random.randint(0,len(transactions) - 1)]])

{% endhighlight %}

## Problem 1: Concatenating (`cat * &gt;&gt; out.txt` ?!)

The [cat](http://man7.org/linux/man-pages/man1/cat.1.html) utility in Unix-y systems is familiar to most anyone who has ever opened up a Terminal window. Take some or all of the files in a folder, concatenate them together....one big file. But something funny happens once you get enough files...

{% highlight shell %}
$ cat * &gt;&gt; out.txt

-bash: /bin/cat: Argument list too long
{% endhighlight %}

That's a fun thought...too many files for the computer to keep track of. As it turns out, many Unix tools will only accept about 10,000 arguments; the use of the asterisk in the `cat` command gets expanded before running, so the above statement passes 1,234,567 arguments to `cat` and you get an error message.

One (naive) solution would be to loop over every file (a completely serial operation):

{% highlight shell %}
for f in *; do cat &quot;$f&quot; &gt;&gt; ../transactions_cat/transactions.csv; done
{% endhighlight %}

Roughly **10,093 seconds** later, you'll have your concatenated file. Three hours is quite a coffee break...

## Solution 1: GNU Parallel &amp; Concatenation

Above, I mentioned that looping over each file gets you past the error condition of too many arguments, but it is a serial operation. If you look at your computer usage during that operation, you'll likely see that only a fraction of a core of your computer's CPU is being utilized. We can greatly improve that through the use of GNU Parallel:

{% highlight shell %}
ls | parallel -m -j $f &quot;cat {} &gt;&gt; ../transactions_cat/transactions.csv&quot;
{% endhighlight %}

The `$f` argument in the code is to highlight that you can choose the level of parallelism; however, you will not get infinitely linear scaling, as shown below ([graph code, Julia](https://gist.github.com/randyzwitch/ee0f738b5895e059fa2a)):

&lt;div id=&quot;cat&quot;&gt;
&lt;/div&gt;

Given that the graph represents a single run at each level of parallelism, it's a bit difficult to say _exactly_ where the parallelism gets maxed out, but at roughly 10 concurrent jobs, there's no additional benefit. It's also interesting to point out what the `-m` argument represents; by specifying `m`, you allow multiple arguments (i.e. multiple text files) to be passed as inputs into parallel. This _alone_ leads to an 8x speedup over the naive loop solution.

## Problem 2: Data &gt; RAM

Now that we have a single file, we've removed the &quot;one million files&quot; cognitive dissonance, but now we have a second problem: at 19.93GB, the amount of data exceeds the RAM in my laptop (2014 MBP, 16GB of RAM). So in order to do analysis, either a bigger machine is needed or processing has to be done in a streaming or &quot;chunked&quot; manner (such as using the [&quot;chunksize&quot; keyword in pandas](http://pandas.pydata.org/pandas-docs/stable/io.html#iterating-through-files-chunk-by-chunk)).

But continuing on with our use of GNU Parallel, suppose we wanted to answer the following types of questions about our transactions data:

  1. How many unique products were sold?
  2. How many transactions were there per day?
  3. How many total items were sold per store, per month?

If it's not clear from the list above, in all three questions there is an &quot;embarrassingly parallel&quot; portion of the computation. Let's take a look at how to answer all three of these questions in a time- and RAM-efficient manner:

##### Q1: Unique Products

Given the format of the data file (transactions in a single column array), this question is the hardest to parallelize, but using a neat trick with the `[tr](http://www.linfo.org/tr.html)` (transliterate) utility, we can map our data to one product per row as we stream over the file:

{% highlight shell linenos %}
# Serial method (i.e. no parallelism)
# This is a simple implementation of map &amp; reduce; tr statements represent one map, sort -u statements one reducer

# cut -d ' ' -f 5- transactions.csv | \     - Using cut, take everything from the 5th column and over from the transactions.csv file
# tr -d \&quot; | \                              - Using tr, trim off double-quotes. This leaves us with a comma-delimited string of products representing a transaction
# sort -u | \                               - Using sort, put similar items together, but only output the unique values
# wc -l                                     - Count number of unique lines, which after de-duping, represents number of unique products

$ time cut -d ' ' -f 5- transactions.csv | tr -d \&quot; | tr ',' '\n' | sort -u | wc -l
331

real	292m7.116s

# Parallelized version, default chunk size of 1MB. This will use 100% of all CPUs (real and virtual)
# Also map &amp; reduce; tr statements a single map, sort -u statements multiple reducers (8 by default)

$ time cut -d ' ' -f 5- transactions.csv | tr -d \&quot; | tr ',' '\n' | parallel --pipe --block 1M sort -u | sort -u | wc -l
331

# block size performance - Making block size smaller might improve performance
# Number of jobs can also be manipulated (not evaluated)
# --500K:               73m57.232s
# --Default 1M:         75m55.268s (3.84x faster than serial)
# --2M:                 79m30.950s
# --3M:                 80m43.311s
{% endhighlight %}

The trick here is that we swap the comma-delimited transactions with the newline character; the effect of this is taking a single transaction row and returning multiple rows, one for each product. Then we pass that down the line, eventually using `sort -u` to de-dup the list and `wc -l` to count the number of unique lines (i.e. products).

In a serial fashion, it takes quite some time to calculate the number of unique products. Incorporating GNU Parallel, just using the defaults, gives nearly a 4x speedup!

##### Q2. Transactions By Day

If the file format could be considered undesirable in question 1, for question 2 the format is perfect. Since each row represents a transaction, all we need to do is perform the equivalent of a SQL `Group By` on the date and sum the rows:

{% highlight shell linenos %}
# Data is at transaction level, so just need to do equivalent of 'group by' operation
# Using cut again, we choose field 3, which is the date part of the timestamp
# sort | uniq -c is a common pattern for doing a 'group by' count operation
# Final tr step is to trim the leading quotation mark from date string

time cut -d ' ' -f 3 transactions.csv | sort | uniq -c | tr -d \&quot;

real	76m51.223s

# Parallelized version
# Quoting can be annoying when using parallel, so writing a Bash function is often much easier than dealing with escaping quotes
# To do 'group by' operation using awk, need to use an associative array
# Because we are doing parallel operations, need to pass awk output to awk again to return final counts

awksub () { awk '{a[$3]+=1;}END{for(i in a)print i&quot; &quot;a[i];}';}
export -f awksub
time parallel --pipe awksub &lt; transactions.csv | awk '{a[$1]+=$2;}END{for(i in a)print i&quot; &quot;a[i];}' | tr -d \&quot; | sort

real	8m22.674s (9.05x faster than serial)
{% endhighlight %}

Using GNU Parallel starts to become complicated here, but you do get a 9x speed-up by calculating rows by date in chunks, then &quot;reducing&quot; again by calculating total rows by date (a trick I picked up at this [blog post](http://www.rankfocus.com/use-cpu-cores-linux-commands/).

##### Q3. Total items Per store, Per month

For this example, it could be that my command-line fu is weak, but the serial method actually turns out to be the fastest. Of course, at a 14 minute run time, the real-time benefits to parallelization aren't that great.

It may be possible that one of you out there knows how to do this correctly, but an interesting thing to note is that the serial version already uses 40-50% of the available CPU available. So parallelization might yield a 2x speedup, but seven minutes extra per run isn't worth spending hours trying to the optimal settings.

## But, I've got MULTIPLE files...

The three examples above showed that it's possible to process datasets larger than RAM in a realistic amount of time using GNU Parallel. However, the examples also showed that working with Unix utilities can become complicated rather quickly. Shell scripts can help move beyond the &quot;one-liner&quot; syndrome, when the pipeline gets so long you lose track of the logic, but eventually problems are more easily solved using other tools.

The data that I generated at the beginning of this post represented two concepts: transactions and customers. Once you get to the point where you want to do joins, summarize by multiple columns, estimate models, etc., loading data into a database or an analytics environment like R or Python makes sense. But hopefully this post has shown that a laptop is capable of analyzing WAY more data than most people believe, using many tools written decades ago.</content>
      </item>
      
    
      
      <item>
        <title>21st Century C: Error 64 on OSX When Using Make</title>
        
          <description>&lt;p&gt;To end 2015, I decided to finally learn C, instead of making it a 2016 resolution! I had previously done the &lt;a href=&quot;http://c.learncodethehardway.org/book/&quot;&gt;‘Learn C The Hard Way’&lt;/a&gt; tutorials, taken about a year off, and thus forgotten everything.&lt;/p&gt;

</description>
        
        <pubDate>Thu, 31 Dec 2015 08:17:30 -0500</pubDate>
        <link>
        http://randyzwitch.com/21st-century-c-error-64-osx/</link>
        <guid isPermaLink="true">http://randyzwitch.com/21st-century-c-error-64-osx/</guid>
        <content type="html" xml:base="/21st-century-c-error-64-osx/">To end 2015, I decided to finally learn C, instead of making it a 2016 resolution! I had previously done the ['Learn C The Hard Way'](http://c.learncodethehardway.org/book/) tutorials, taken about a year off, and thus forgotten everything.

Rather than re-do the same material, I decided to get ['21st Century C'](http://shop.oreilly.com/product/0636920033677.do) from O'Reilly and work through that. Unfortunately, there is an error/misprint in the very beginning chapters that makes doing the exercises near impossible on OSX. This error manifests itself as `c99: invalid argument 'all' to -W Error 64`. If you encounter this error on OSX (I'm using OSX 10.11.2 El Capitan as of writing this post), here are three methods for fixing the issue.

## Error 64!

When the discussion of using [Makefiles](https://www.gnu.org/software/make/) begins on page 15, there is a discussion of the &quot;smallest practicable makefile&quot;, which is just six lines long:

{% highlight shell linenos %}
P=program_name
OBJECTS=
CFLAGS= -g -Wall -O3
LDLIBS=
CC=c99

$(P): $(OBJECTS)
{% endhighlight %}

Unfortunately, this doesn't _quite_ work on OSX. Page 11 in the book sort-of references that a fix is needed, but the directions aren't so clear...

## Error 64, solution 1: Book Fix, updated

To use the book fix, you are supposed to:

  1. Create a file named _c99_
  2. Put the lines `gcc -std=c99 $\*` OR `clang $\*` in the _c99_ file
  3. Add the file to your PATH in Terminal (such as `export PATH=&quot;/Users/computeruser:$PATH&quot;` if the `c99` file were located in `/Users/computeruser` directory)
  4. Run `chmod +x c99` on the file to make it executable

Once you add this work-around to your PATH, then open a fresh Terminal window (or run `source .bash_profile` to refresh the Bash settings), you should be able to use Make to compile your C code.

But to be honest, this seems like a really weird &quot;fix&quot; to me, as it overrides the C compiler settings for any program run via Terminal. I prefer one of two alternate solutions.

## Error 64, solution 2: Makefile Change

As I was researching this, a helpful Twitter user noted:

&lt;blockquote class=&quot;twitter-tweet&quot; lang=&quot;en&quot; data-conversation=&quot;none&quot;&gt;
  &lt;p&gt;
    &lt;a href=&quot;https://twitter.com/randyzwitch&quot;&gt;@randyzwitch&lt;/a&gt; Remove space between CFLAGS and =, and replace c99 with cc. See man c99, -W is not -Wwarnings.
  &lt;/p&gt;

  &lt;p&gt;
    — Eugene Teo (@datajottings) &lt;a href=&quot;https://twitter.com/datajottings/status/682214537341190145&quot;&gt;December 30, 2015&lt;/a&gt;
  &lt;/p&gt;
&lt;/blockquote&gt;

When you switch the 'c99' reference to just 'cc' in the Makefile, everything works fine. Here's the subtlety different, corrected Makefile:

{% highlight shell linenos %}
P=program_name
OBJECTS=
CFLAGS= -g -Wall -O3
LDLIBS=
CC=cc

$(P): $(OBJECTS)
{% endhighlight %}

## Error 64, solution 3: Switch to Clang

The final solution I came across is rather than using the GCC compiler, you can use an alternate compiler called Clang, which is also generally available on OSX (especially with XCode installed). Like solution 2 above, the Makefile is just subtlety different:

{% highlight shell linenos %}
P=program_name
OBJECTS=
CFLAGS= -g -Wall -O3
LDLIBS=
CC=clang

$(P): $(OBJECTS)
{% endhighlight %}

Whether to use GCC or Clang as your compiler is really beyond the scope of this blog post; as _21st Century C_ discusses, it really shouldn't matter (especially when you are just learning the mechanics of the language).

## Error 64, Be Gone!

There's not really much more to say at this point; this blog post is mainly documentation for anyone who comes across this error in the future. I've continued on through the book using Clang, but suffice to say, it's not the compiler that writes poor-quality, non-compiling code, it's the user. Ah, the fun of learning 🙂</content>
      </item>
      
    
      
      <item>
        <title>Four Tactics For Well Thought Out Business Requirements</title>
        
          <description>&lt;blockquote class=&quot;twitter-tweet&quot; data-partner=&quot;tweetdeck&quot;&gt;
  &lt;p dir=&quot;ltr&quot; lang=&quot;en&quot;&gt;
    &lt;a href=&quot;https://twitter.com/randyzwitch&quot;&gt;@randyzwitch&lt;/a&gt; how to get (reasonably) well thought-through requirements from business people?
  &lt;/p&gt;

&lt;/blockquote&gt;
</description>
        
        <pubDate>Fri, 21 Aug 2015 06:42:13 -0400</pubDate>
        <link>
        http://randyzwitch.com/well-thought-out-business-requirements/</link>
        <guid isPermaLink="true">http://randyzwitch.com/well-thought-out-business-requirements/</guid>
        <content type="html" xml:base="/well-thought-out-business-requirements/">&lt;blockquote class=&quot;twitter-tweet&quot; data-partner=&quot;tweetdeck&quot;&gt;
  &lt;p dir=&quot;ltr&quot; lang=&quot;en&quot;&gt;
    &lt;a href=&quot;https://twitter.com/randyzwitch&quot;&gt;@randyzwitch&lt;/a&gt; how to get (reasonably) well thought-through requirements from business people?
  &lt;/p&gt;

  &lt;p&gt;
    — Art Webb (@arthurlwebb) &lt;a href=&quot;https://twitter.com/arthurlwebb/status/634710548685418496&quot;&gt;August 21, 2015&lt;/a&gt;
  &lt;/p&gt;
&lt;/blockquote&gt;



One of the most common issues in business (especially large corporations) is trying to nail down the requirements for a given analysis request. The &quot;business people&quot; on the front-lines are talking to their higher-ups about what they think are important questions for the business to solve, but by the time the question gets to the analyst or developer, it sounds something like:

&gt; It would be interesting to model using SAS how our customers shop for our merchandise by channel and what overlaps there are between demographics, geography, product type and tenure. But we also have to timebox this, we can't be boiling-the-ocean just looking for needles-in-a-haystack.

Say WHAT? Mr. Business Person, I cannot help you if you do not run that mess through [Unsuck-It](http://unsuck-it.com/) first.

In all seriousness, I've found there are a few great ways for an analyst to refine a &quot;question&quot; like the one above into an actionable plan of attack. So the next time you get a jargon-filled, completely generic analysis request such as the one above, try these four tactics.

### 1. All Requests Should Be Phrased In The Form Of A Question

The first thing to notice about the mock interaction above is that there are no question marks; it's not a question! For an analyst or developer to work effectively, _questions_ need to be presented, not bland _statements_. For example, a refinement series of questions from the analyst might include:

  * You need a _model_? What type of model? Do you mean a predictive model, a decision tree for understanding, a PivotTable for you to poke at, a one-page PowerPoint slide to give your boss?
  * You specified four attributes (demographics, geography, product type and tenure). Do you have a hypothesis around these attributes (or are you just brain-blabbing)?
  * What is meant by &quot;shop&quot;? Do you mean how do customers browse our goods online and in stores, the purchase cycle, what goods are frequently purchased together or something else?

Note that in all three of the refinement questions above, you are taking a generic idea and really drilling into what is needed. It is _the analyst_ that is the expert in the techniques for analyzing data, so the analyst should be helping the business person to take a raw analysis request and make it into answerable questions.

### 2. Separate The Tools From The Question

The second thing to notice in the mock interaction above is the statement &quot;_using SAS_&quot;. I didn't write that to pick on SAS, but rather, this exact statement was said to me early in my career. I had a boss who would try and guess which tool was appropriate for the question he was asking. I presume that he was trying to gauge how hard he thought the problem was, or try to signal to me how hard he thought the problem was. In the end, a plain SQL query with the results copied into an Excel table was all that was necessary.

As the analyst, confirm whether _the tool_ is actually part of the _deliverable_. Meaning, if you need to deliver a Tableau workbook, ok, specifying &quot;use Tableau&quot; is an important part of the business question. But if the requirement is &quot;production-quality visualizations&quot;, Tableau may or may not be the right tool or might just be one part of a larger workflow.

### 3. Every Question Is Interesting To Someone. Solve The Valuable Ones.

Paraphrasing the aphorism &quot;_The path to hell is paved with good intentions_&quot;, the path to doing low-value work your entire career is answering questions that start &quot;Wouldn't it be interesting if...&quot;.

The basis for these statements are often tangents in other meetings, where high-level executives think there is information that should just be available at everyone's fingertips. But if you were to ask &quot;What business action would you take if you knew this piece of information?&quot; or &quot;Is it worth me stopping a project worth $1 million in Pre-Tax Profit per month to answer this for you?&quot;, you'll suddenly the question becomes a lot less _interesting_.

So always have estimates of the business impact of what you are currently working on and ask for the same estimate of those who ask for your time. Projects that are _valuable_ to the business are &quot;interesting&quot;, everything else is just _making work_ for other people.

### 4. Don't Just Solved The Stated Question. Solve The Unstated Question Too.

Finally, when I read the mock interaction above, there are actually _two_ questions:

  * Stated: Do we understand our customer's purchasing behaviors?
  * Unstated: How do we optimize our business to take into account our customer's purchasing behaviors?

For sure, a deep understanding of the customer base is important no matter the product. But the unstated question of &quot;What are we doing to _do about it_?&quot; is so much more valuable to answer (i.e. tactic #3).

So even if the refined question becomes 'Build a customer segmentation based on past purchases', go one step further and figure out how to implement your findings. Create a test plan for increasing email click-through-rates based on the segments or optimize your display bidding, maybe build a recommender system for your website...implementation of new ideas is always going to be more valuable than just analyzing the past.

### Always Be Assertive.

If the key to sales is &quot;Always Be Closing&quot;, the key to quality analysis is &quot;Always Be Assertive&quot;. Ask questions. Make people think about what they are doing, what they ask of others and what can be done to improve the business. It's a rare, ego-centric co-worker who doesn't appreciate collaborating to get to a better quality question (and answer!) than they originally started with.

Being able to read into what other people are asking for, estimating its value, then delivering more than they even knew they were asking for has helped me tremendously throughout my career. Hopefully by doing some or all of the tactics above, you'll see a marked improvement in your analysis and career as well!</content>
      </item>
      
    
      
      <item>
        <title>RSiteCatalyst Version 1.4.5 Release Notes</title>
        
          <description>&lt;p&gt;It’s only been a month since the &lt;a href=&quot;http://randyzwitch.com/rsitecatalyst-version-1-4-4-release-notes/&quot;&gt;last RSiteCatalyst update&lt;/a&gt;, and this update is also a pretty minor update in terms of functionality.&lt;/p&gt;

</description>
        
        <pubDate>Mon, 17 Aug 2015 05:43:36 -0400</pubDate>
        <link>
        http://randyzwitch.com/rsitecatalyst-version-1-4-5-release-notes/</link>
        <guid isPermaLink="true">http://randyzwitch.com/rsitecatalyst-version-1-4-5-release-notes/</guid>
        <content type="html" xml:base="/rsitecatalyst-version-1-4-5-release-notes/">It's only been a month since the [last RSiteCatalyst update](http://randyzwitch.com/rsitecatalyst-version-1-4-4-release-notes/), and this update is also a pretty minor update in terms of functionality.

## Set Your Own Endpoint

For the overseas users (or companies with weird setups), you can now use the `endpoint` argument in the `SCAuth()` function to specify your API endpoint. For the most part, this is not recommended, as RSiteCatalyst pings the Adobe Analytics API to evaluate the proper API endpoint to use, but if for some reason you are having issues, you can override what the Adobe API says.

## New Functions

For this release, I briefly looked through the API explorer to see if  there were any useful methods that had been missed. `GetFunctions` (Get definitions of all formula/functions in Adobe Analytics), `QueueSummary` (Get summary metrics for numerous report suites at once), `GetPrivacySettings` (Privacy Settings at a report suite level), and `GetTemplate` (Get template that a current report suite was built from). With the exception of `QueueSummary()`, none of these functions will likely get you much in the way of additional analytics capabilities, but they are there should you want to use them.

## Feature Requests/Bugs

As always, if you come across bugs or have feature requests, please continue to use the [RSiteCatalyst GitHub Issues](https://github.com/randyzwitch/RSiteCatalyst/issues) page to submit issues. Don’t worry about cluttering up the page with tickets, please fill out a new issue for anything you encounter (with code you’ve already tried and is failing), unless you are SURE that it is the same problem someone else is facing.

Outside of patching really serious bugs, I will likely **not spend any more time improving this package in the future**; my interests have changed, and RSiteCatalyst is pretty much complete as far as I'm concerned. That said, contributors are also _very welcomed_. If there is a feature you'd like added, and especially if you can fix an outstanding issue reported at GitHub, we'd love to have your contributions. Willem and I are both parents of young children and have real jobs outside of open-source software creation, so we welcome any meaningful contributions to RSiteCatalyst that anyone would like to contribute.</content>
      </item>
      
    
      
      <item>
        <title>JuliaCon 2015: Everyday Analytics and Visualization (video)</title>
        
          <description>&lt;p&gt;At long last, here’s the video of my presentation from JuliaCon 2015, discussion common analytics tasks and visualization. This is really two talks, the first being an example of using the citibike NYC API to analyze ridership of their public bike program, and the second a discussion of the Vega.jl package.&lt;/p&gt;

</description>
        
        <pubDate>Fri, 14 Aug 2015 06:29:41 -0400</pubDate>
        <link>
        http://randyzwitch.com/juliacon-2015-everyday-analytics-and-visualization-video/</link>
        <guid isPermaLink="true">http://randyzwitch.com/juliacon-2015-everyday-analytics-and-visualization-video/</guid>
        <content type="html" xml:base="/juliacon-2015-everyday-analytics-and-visualization-video/">At long last, here's the video of my presentation from JuliaCon 2015, discussion common analytics tasks and visualization. This is really two talks, the first being an example of using the citibike NYC API to analyze ridership of their public bike program, and the second a discussion of the Vega.jl package.

Speaking at JuliaCon 2015 at MIT CSAIL is the professional highlight of my year; hopefully even more of you will attend next year.

Enjoy!

_Edit: For those of you who would like to follow-along using the actual [presentation code](https://github.com/randyzwitch/juliacon2015), it is available on GitHub._

&lt;iframe src=&quot;https://www.youtube.com/embed/0F8tC3ofH4g&quot; width=&quot;640&quot; height=&quot;360&quot; frameborder=&quot;0&quot; allowfullscreen=&quot;allowfullscreen&quot;&gt;&lt;/iframe&gt;</content>
      </item>
      
    
      
      <item>
        <title>Apple MacBook Pro Model A1286 Declared Vintage - The End Of An Era</title>
        
          <description>&lt;p&gt;It’s hard to believe it’s been over 2.5 years since I wrote about my experience with Apple trying to get my &lt;a href=&quot;http://randyzwitch.com/broken-macbook-pro-hinge-fixed-free/&quot;&gt;Broken MacBook Pro Hinge&lt;/a&gt; fixed. Since that time, my Late 2008 MacBook Pro continued to work flawlessly, most of the time keeping up with the scientific programming I would do in R, Python or Julia.&lt;/p&gt;

</description>
        
        <pubDate>Fri, 07 Aug 2015 16:07:12 -0400</pubDate>
        <link>
        http://randyzwitch.com/apple-macbook-pro-model-a1286-late-2008-vintage/</link>
        <guid isPermaLink="true">http://randyzwitch.com/apple-macbook-pro-model-a1286-late-2008-vintage/</guid>
        <content type="html" xml:base="/apple-macbook-pro-model-a1286-late-2008-vintage/">It's hard to believe it's been over 2.5 years since I wrote about my experience with Apple trying to get my [Broken MacBook Pro Hinge](http://randyzwitch.com/broken-macbook-pro-hinge-fixed-free/) fixed. Since that time, my Late 2008 MacBook Pro continued to work flawlessly, most of the time keeping up with the scientific programming I would do in R, Python or Julia.

Unfortunately, it seems near impossible (if not completely impossible) to get an OEM A1281 battery as a drop-in replacement. When I went to the Apple Store at Suburban Square, PA, the &quot;Genius&quot; that looked at my computer took 15-20 minutes to look on the Apple website (which I obviously did before arriving, so no value-add there), only to show me a battery in stock that didn't fit my model of computer. Only after shaming him into looking up the actual part number, was he able to utter the phrase:

&gt; Oh, no, we don't have those any more. Your model MacBook Pro was declared &quot;Vintage&quot;. No more original parts are available from Apple.

_Of course_ it is. After getting home, I was able to find this [service bulletin](https://support.apple.com/en-us/HT201624) from Apple, which outlines which models are obsolete. Apparently, it's a hard and fast rule that once five years from the end of manufacturing arrives, a model is declared vintage (unless local laws require longer service). So even though the only &quot;problem&quot; with my MacBook Pro is that I was only getting one hour of battery life per charge (or less if I'm compiling code), the computer is destined for a new life somewhere else.

### &quot;Vintage&quot; For Me, Powerful For Thee

While I realize I could go the 3rd-party route and get a replacement battery, at some point, you can only spend so much money keeping older technology alive. Since I use computers pretty intensively, I ended up getting a &quot;new&quot; (used) 2011 MacBook Pro from a neighborhood listing that has decent life on the OEM battery. Surprisingly, I was able to get $360 for my Late-2008 MacBook Pro, being fully honest about the condition, issues and battery life. The older woman who I sold it to fully understood, but worked at a desk and didn't care about the battery! She also said:

&gt; This is easily the most powerful computer I've ever owned.

Apple, like I said in my original post, you've got a customer for life. And while I've moved on to a newer machine, it's beyond amazing to me that a 7-year old computer will continue to live on and work at a high level of performance. And with my 2011 MacBook Pro, I still have the option to upgrade the parts (though I don't need to...SSD, 16GB of RAM and a quad-core i7 processor already!)

The Retina MacBook's are nice, but very incremental. Here's hoping the 2011 MacBook Pro lasts as long as my Late 2008 MacBook Pro did!</content>
      </item>
      
    
      
      <item>
        <title>Authenticated API Testing Using Travis CI</title>
        
          <description>&lt;p&gt;As I’ve become more serious about contributing in the open-source community, having quality tests for my packages has been something I’ve spent much more time on than when I was just writing quick-and-dirty code for my own purposes. My most used open-sourced package is &lt;a href=&quot;http://randyzwitch.com/tag/rsitecatalyst/&quot;&gt;RSiteCatalyst&lt;/a&gt;, which accesses the Adobe Analytics (authenticated) API, which poses a problem: how do you maintain a project on GitHub with a full test suite, while at the same time not hard-coding your credentials in plain sight for everyone to see?&lt;/p&gt;

</description>
        
        <pubDate>Thu, 06 Aug 2015 16:21:50 -0400</pubDate>
        <link>
        http://randyzwitch.com/authentication-travis-ci/</link>
        <guid isPermaLink="true">http://randyzwitch.com/authentication-travis-ci/</guid>
        <content type="html" xml:base="/authentication-travis-ci/">As I've become more serious about contributing in the open-source community, having quality tests for my packages has been something I've spent much more time on than when I was just writing quick-and-dirty code for my own purposes. My most used open-sourced package is [RSiteCatalyst](http://randyzwitch.com/tag/rsitecatalyst/), which accesses the Adobe Analytics (authenticated) API, which poses a problem: how do you maintain a project on GitHub with a full test suite, while at the same time not hard-coding your credentials in plain sight for everyone to see?

The answer ends up being using [encrypted environment variables](http://docs.travis-ci.com/user/environment-variables/#Encrypted-Variables) within [Travis CI](https://travis-ci.org/).

### Testthat!

In terms of a testing framework, Hadley Wickham provides a great testing framework in [testthat](https://github.com/hadley/testthat); while I wouldn't go as far as he does to say that the package makes testing _fun_, it certainly makes testing _easy_. Let's take a look at some of the tests in RSiteCatalyst from the `QueueOvertime` function:

{% highlight R linenos %}
test_that(&quot;Validate QueueOvertime using legacy credentials&quot;, {

  skip_on_cran()

  #Correct [masked] credentials
  SCAuth(Sys.getenv(&quot;USER&quot;, &quot;&quot;), Sys.getenv(&quot;SECRET&quot;, &quot;&quot;))

  #Single Metric, No granularity (summary report)
  aa &lt;- QueueOvertime(&quot;zwitchdev&quot;,
                      &quot;2014-01-01&quot;,
                      &quot;2014-12-31&quot;,
                      &quot;visits&quot;,
                      &quot;&quot;)

  #Validate returned value is a data.frame
  expect_is(aa, &quot;data.frame&quot;)

  #Single Metric, Daily Granularity
  bb &lt;- QueueOvertime(&quot;zwitchdev&quot;,
                      &quot;2014-01-01&quot;,
                      &quot;2014-12-31&quot;,
                      &quot;visits&quot;,
                      &quot;day&quot;)

  #Validate returned value is a data.frame
  expect_is(bb, &quot;data.frame&quot;)

  #Single Metric, Week Granularity
  cc &lt;- QueueOvertime(&quot;zwitchdev&quot;,
                      &quot;2014-01-01&quot;,
                      &quot;2014-12-31&quot;,
                      &quot;visits&quot;,
                      &quot;week&quot;)

  #Validate returned value is a data.frame
  expect_is(cc, &quot;data.frame&quot;)

  #Two Metrics, Week Granularity
  dd &lt;- QueueOvertime(&quot;zwitchdev&quot;,
                      &quot;2014-01-01&quot;,
                      &quot;2014-12-31&quot;,
                      c(&quot;visits&quot;, &quot;pageviews&quot;),
                      &quot;week&quot;)

  #Validate returned value is a data.frame
  expect_is(dd, &quot;data.frame&quot;)

  #Two Metrics, Month Granularity, Social Visitors
  ee &lt;- QueueOvertime(&quot;zwitchdev&quot;,
                      &quot;2014-01-01&quot;,
                      &quot;2014-12-31&quot;,
                      c(&quot;visits&quot;, &quot;pageviews&quot;),
                      &quot;month&quot;,
                      &quot;5433e4e6e4b02df70be4ac63&quot;)

  #Validate returned value is a data.frame
  expect_is(ee, &quot;data.frame&quot;)

  #Two Metrics, Day Granularity, Social Visitors, Anomaly Detection
  ff &lt;- QueueOvertime(&quot;zwitchdev&quot;,
                      &quot;2014-01-01&quot;,
                      &quot;2014-12-31&quot;,
                      c(&quot;visits&quot;, &quot;pageviews&quot;),
                      &quot;day&quot;,
                      &quot;5433e4e6e4b02df70be4ac63&quot;,
                      anomaly.detection = &quot;1&quot;)

  #Validate returned value is a data.frame
  expect_is(ff, &quot;data.frame&quot;)



})
{% endhighlight %}

From the code above, you can see the tests are fairly simplistic; for a given number of permutations of arguments of the function, I test to see if a data frame was returned. This is because, for the most part, RSiteCatalyst is just a means of generating JSON calls, submitting them to the Adobe Analytics API, then parsing the results into an R data frame.

Since there is very little additional logic in the package, I don't spend a bunch of time testing what data is actually returned (i.e. what is returned depends on the Adobe Analytics API, not R). What is interesting is line 6; I reference `Sys.getenv()` twice in order to pass in my username and key for the Adobe Analytics API, which feels very &quot;interactive R&quot;, but the goal is automated testing. Filling in those two environment variables is where Travis CI comes in.

### Travis CI Configuration

In order to have any automation using Travis CI, you need to create a `.travis.yml` configuration file. While you can read the [Travis docs to create the .travis.yml file for R](http://docs.travis-ci.com/user/languages/r/), you're probably better off just using the `use_travis` function from [devtools](https://github.com/hadley/devtools) (also from Hadley, little surprise!) to create the file for you. In terms of [creating encrypted keys to use with Travis](http://docs.travis-ci.com/user/encryption-keys/), you'll need to use the [Travis CLI tool](https://github.com/travis-ci/travis.rb), which is distributed as a Ruby gem (i.e. package).  If you view the [RSiteCatalyst .travis.yml file](https://github.com/randyzwitch/RSiteCatalyst/blob/master/.travis.yml), you can see that I define two global &quot;secure&quot; variables, the value of which are the output from running a command similar to the following in the Travis CLI tool:

{% highlight shell %}
$ travis encrypt RANDY=ZWITCH
Please add the following to your .travis.yml file:

  secure: &quot;b6S4dBc7arvox8UpuFqkz+VP2UmAW/S/B/vgaAdZiZQqUp78YDR6VYdAYN3WisCK1VLGjOVVPQvGxLik0pQokF8FU3sjX0ekH6vSJeqg4utrEZmVtNvdDLEVAmagFy8Fyduow3U4CPW7rzXqvAE4cIVqGR5Lv2KLf8ANUGn+y3E=&quot;

Pro Tip: You can add it automatically by running with --add.
{% endhighlight %}

Note that if this seems insecure, every time you run the `encrypt` command with the same arguments, you get a different value; Travis CI is creating new public and private RSA keys each time.

### Setting Up Authenticated Testing Locally

If you get as far as setting up encrypted Travis CI keys and tests using `testthat`, the final step is really for convenience. With the `.travis.yml` file, Travis CI sets the R environment variables on THEIR system; on your local machine, the environment variables aren't set. Even if the environment variables were set, they would be set to the Travis CI hashed values, which is not what I want to pass to my authentication function in my R package.

To set the authentication variables locally, so that each time you hit 'check' to build and check against CRAN errors, you just need to modify the `.Renviron` file for R:

{% highlight R %}
USER=&quot;myusername&quot;
SECRET=&quot;mysecret&quot;
{% endhighlight %}

With that minor change, in addition to the `.travis.yml` file, you'll have a seamless environment for developing and testing R packages.

### Testing Is Like Flossing...

As easy as the `testthat` and `devtools` packages make testing, and as inexpensively as Travis CI is as a service (free for open source projects!), there's really no excuse to provide packaged-up code and not include tests. Hopefully this blog post has demonstrated that it's possible to include tests even when authentication is necessary without compromising your credentials.

So let's all be sure to include tests, not just pay lip service to the idea that testing is useful. Code testing only works if you actually do it 🙂</content>
      </item>
      
    
      
      <item>
        <title>Getting Started: Adobe Analytics Clickstream Data Feed</title>
        
          <description>&lt;blockquote&gt;
  &lt;p&gt;“Well, first you need a TMS and a three-tiered data layer, then some jQuery with a node backend to inject customer data into the page asynchronously if you want to avoid cookie-based limitations with cross-domain tracking and be Internet Explorer 4 compatible…”&lt;/p&gt;
&lt;/blockquote&gt;

</description>
        
        <pubDate>Tue, 04 Aug 2015 05:00:35 -0400</pubDate>
        <link>
        http://randyzwitch.com/adobe-analytics-clickstream-raw-data-feed/</link>
        <guid isPermaLink="true">http://randyzwitch.com/adobe-analytics-clickstream-raw-data-feed/</guid>
        <content type="html" xml:base="/adobe-analytics-clickstream-raw-data-feed/">&gt; &quot;Well, first you need a TMS and a three-tiered data layer, then some jQuery with a node backend to inject customer data into the page asynchronously if you want to avoid cookie-based limitations with cross-domain tracking and be Internet Explorer 4 compatible...&quot;

Blah Blah Blah. There's a whole cottage industry around jargon-ing each other to death about digital data collection. But why? Why do we focus on _tools_, instead of _the data_? Because the tools are necessarily inflexible, so we work backwards from the pre-defined reports we have to the data needed to populate them correctly. Let's go the other way for once: clickstream data to analysis &amp; reporting.

In this blog post, I will show the structure of the Adobe Analytics Clickstream Data Feed and how to work with a day worth of data within R. Clickstream data isn't as raw as pure server logs, but the only limit to what we can calculate from clickstream data is what we can accomplish with a bit of programming and imagination. In later posts, I'll show how to store a year worth of data in a relational database, storing the same data in Hadoop and doing analysis using modern tools such as Apache Spark.

This blog post will not cover the mechanics of getting the feed delivered via FTP. The [Adobe Clickstream Feed documentation](https://marketing.adobe.com/resources/help/en_US/whitepapers/clickstream/datafeeds_configure.html) is sufficiently clear in how to get started.

### FTP/File Structure

Once your Adobe Clickstream Feed starts being delivered via FTP, you'll have a file listing that looks similar to the following:

![adobe-clickstream-data-ftp](/wp-content/uploads/2015/07/adobe-clickstream-data-ftp.png)

What you'll notice is that with daily delivery, three files are provided, each having a consistent file naming format:

  * \d+-\S+_\d+-\d+-\d+.tsv.gz

  This is the main file containing the server call level data

  * \S+_\d+-\d+-\d+-lookup_data.tar.gz

  These are the lookup tables, header files, etc.

  * \S+_\d+-\d+-\d+.txt

  Manifest file, delivered last so that any automated processes know that Adobe is finished transferring

The regular expressions will be unnecessary for working with our single day of data, but it's good to realize that there is a consistent naming structure.

### Checking md5 hashes

As part of the manifest file, Adobe provides [md5 hashes](https://en.wikipedia.org/wiki/MD5) of the files. There are at least two purposes to this, including 1) making sure that the files truly were delivered in full and 2) that the files haven't been manipulated/tampered with. In order to check that your md5 hashes match the values provided by Adobe, we can do the following in R:

{% highlight R linenos %}
setwd(&quot;~/Downloads/datafeed/&quot;)

#Read in Adobe manifest file
manifest &lt;- read.table(&quot;zwitchdev_2015-07-13.txt&quot;, stringsAsFactors=FALSE)
names(manifest) &lt;- c(&quot;key&quot;, &quot;value&quot;)

#Use digest library to calculate md5 hashes
library(digest)
servercalls_md5 &lt;- digest(&quot;01-zwitchdev_2015-07-13.tsv.gz&quot;, algo=&quot;md5&quot;, file=TRUE)
lookup_md5 &lt;- digest(&quot;zwitchdev_2015-07-13-lookup_data.tar.gz&quot;, algo=&quot;md5&quot;, file=TRUE)

#Check to see if hashes contained in manifest file
servercalls_md5 %in% manifest$value #[1] TRUE
lookup_md5 %in% manifest$value #[1] TRUE
{% endhighlight %}

As we can see, both calculated hashes are contained within the manifest, so we can be confident that the files we downloaded haven't been modified.

### Unzipping and Loading Raw Files to Data Frames

Now that our file hashes are validated, it's time to load the files into R. For the example files, I would be able to fit the entire day into RAM because my blog does very little traffic. However, I'm going to still limit the rows brought in, as if we were working with a large e-commerce website with millions of visits per day:

{% highlight R linenos %}
#Get list of lookup files from tarball
files_tar &lt;- untar(&quot;zwitchdev_2015-07-13-lookup_data.tar.gz&quot;, list = TRUE)

#Extract files to _temp directory. Directory will be created if it doesn't exist
untar(&quot;zwitchdev_2015-07-13-lookup_data.tar.gz&quot;, exdir = &quot;_temp&quot;)

#Read each file into a data frame
#If coding like this in R offends you, keep it to yourself...
for (file in files_tar) {
  df_name &lt;- unlist(strsplit(file, split = &quot;.tsv&quot;, fixed = TRUE))
  temp_df &lt;- read.delim(paste(&quot;_temp&quot;, file, sep = &quot;/&quot;), header=FALSE, stringsAsFactors=FALSE)
  #column_headers not used as lookup table
  if (df_name != &quot;column_headers&quot;){
    names(temp_df) &lt;- c(&quot;id&quot;, df_name)
  }
  assign(df_name, temp_df)
  rm(temp_df)
}

#gz files can be read directly into dataframes from base R
#Could also use `readr` library for performance
servercall_data &lt;- read.delim(&quot;~/Downloads/datafeed/01-zwitchdev_2015-07-13.tsv.gz&quot;,
                       header=FALSE, stringsAsFactors=FALSE, nrows = 500)

#Use column_headers to label servercall_data data frame using first row of data
names(servercall_data) &lt;- column_headers[1,]
{% endhighlight %}

If we were to be loading this data into a database, we'd be done with our processing; we have all of our data read into R and it would be a trivial exercise to load the data into a database (we'll do this in a separate blog post). But since we're going to be analyze this single day of clickstream data, we need to join these 14 data frames together.

### SQL: The Most Important Language for Analytics

_As a slight tangent, if you don't know SQL, then you're going to have a really hard time doing any sort of advanced analytics. There are literally millions of tutorials on the Internet (including [this one from me](http://randyzwitch.com/sqldf-package-r/)), and understanding how to join and retrieve data from databases is the key to being more than just a report monkey._

The reason why the prior code creates 14 data frames is because the data is delivered in a [normalized](http://www.studytonight.com/dbms/database-normalization.php) structure from Adobe. Now we are going to [de-normalize](http://searchdatamanagement.techtarget.com/definition/denormalization) the data, which is just a fancy way of saying &quot;join the files together in order to make a gigantic table.&quot;

There are probably a dozen different ways to join data frames using just R code, but I'm going to do it using the [sqldf](https://cran.r-project.org/web/packages/sqldf/index.html) package so that I can use SQL. This will allow for a single, declarative statement that shows the relationship between the lookup and fact tables:

{% highlight R linenos %}
library(sqldf)

query &lt;-
&quot;select
sc.*,
browser.browser as browser_name,
browser_type,
connection_type.connection_type as connection_name,
country.country as country_name,
javascript_version,
languages.languages as languages,
operating_systems,
referrer_type,
resolution.resolution as screen_resolution,
search_engines
from servercall_data as sc
left join browser on sc.browser = browser.id
left join browser_type on sc.browser = browser_type.id
left join connection_type on sc.connection_type = connection_type.id
left join country on sc.country = country.id
left join javascript_version on sc.javascript = javascript_version.id
left join languages on sc.language = languages.id
left join operating_systems on sc.os = operating_systems.id
left join referrer_type on sc.ref_type = referrer_type.id
left join resolution on sc.resolution = resolution.id
left join search_engines on sc.post_search_engine = search_engines.id
;
&quot;

denormalized_df &lt;- sqldf(query)
{% endhighlight %}

There are three lookup tables that weren't used: `color_depth`, `plugins` and `event`. The first two don't have a lookup column in my data feed (click link for a full listing of [Adobe Clickstream data feed](https://marketing.adobe.com/resources/help/en_US/sc/clickstream/datafeeds_reference.html) columns available). These columns aren't really useful for my purposes anyway, so not a huge loss. The third table, the `event` list, requires a separate processing step.

### Processing Event Data

As normalized as the Adobe Clickstream Data Feed is, there is one oddity: the events per server call come in a comma-delimited string in a single column with a lookup table. This implies that a separate level of processing is necessary, outside of SQL, since the column &quot;key&quot; is actually multiple keys and the lookup table specifies one event type per row. So if you were to try and join the data together, you wouldn't get any matches.

To deal with this in R, we are going to do an EXTREMELY wasteful operation: we are going to create a data frame with a column for each possible event, then evaluate each row to see if that event occurred. This will use a massive amount of RAM, but of course, this is a feature/limitation of R which wouldn't be an issue if the data were stored in a database.

{% highlight R linenos %}
#Create friendly names in events table replacing spaces with underscores
event$names_filled &lt;- tolower(gsub(&quot; &quot;, &quot;_&quot;, event$event))

#Initialize a data frame with all 0 values
#Dimensions are number of observations as rows, with a column for every possible event
event_df &lt;- data.frame(matrix(data = 0, ncol = nrow(event), nrow = nrow(servercall_data)))
names(event_df) &lt;- event$id

#Parse comma-delimited string into vector
#Each vector value represents column name in event_df, assign value of 1
for(row in servercall_data$post_event_list){
    if(!is.na(row)){
        for(event_ in strsplit(row, &quot;,&quot;)){
          event_df[as.character(event_)] &lt;- 1
        }
    }
}

#Rename columns with &quot;friendly&quot; names
names(event_df) &lt;- event$names_filled

#Horizontally join datasets to create final dataset
oneday_df &lt;- cbind(denormalized_df, event_df)
{% endhighlight %}

With the final `cbind` command, we've created a 500 row x 1562 column dataset representing a sample of rows from one day of the Adobe Clickstream Data Feed. Having the data denormalized in this fashion takes 6.13 MB of RAM...extrapolating to 1 million rows, you would need 12.26GB of RAM (per day of data you want to analyze, if stored solely in memory).

### Next Step: Analytics?!

A thousand words in and 91 lines of R code and we still haven't done any actual analytics. But we've completed the first step in any analytics project: data prep!

In future blog posts in this series, I'll demonstrate how to actually use this data in analytics, from re-creating reports available in the Adobe Analytics UI (to prove the data is the same) to more advanced analysis such as using association rules, which can be one method for creating a &quot;You may also like...&quot; functionality such as the one at the bottom of this blog.

## Example Files:

* &lt;a href=&quot;http://randyzwitch.com/wp-content/uploads/2015/08/zwitchdev_2015-07-13.txt&quot; target=&quot;_blank&quot;&gt;http://randyzwitch.com/wp-content/uploads/2015/08/zwitchdev_2015-07-13.txt&lt;/a&gt;
* &lt;a href=&quot;http://randyzwitch.com/wp-content/uploads/2015/08/zwitchdev_2015-07-13-lookup_data.tar.gz&quot; target=&quot;_blank&quot;&gt;http://randyzwitch.com/wp-content/uploads/2015/08/zwitchdev_2015-07-13-lookup_data.tar.gz&lt;/a&gt;
* &lt;a href=&quot;http://randyzwitch.com/wp-content/uploads/2015/08/01-zwitchdev_2015-07-13.tsv.gz&quot; target=&quot;_blank&quot;&gt;http://randyzwitch.com/wp-content/uploads/2015/08/01-zwitchdev_2015-07-13.tsv.gz&lt;/a&gt;</content>
      </item>
      
    
      
      <item>
        <title>RSiteCatalyst Version 1.4.4 Release Notes</title>
        
          <description>&lt;p&gt;It’s been about six months since the last &lt;a href=&quot;http://randyzwitch.com/rsitecatalyst-version-1-4-3-release-notes/&quot;&gt;RSiteCatalyst update&lt;/a&gt;, and this update is really just a single bug fix, but a big bug fix at that!&lt;/p&gt;

</description>
        
        <pubDate>Mon, 13 Jul 2015 05:40:20 -0400</pubDate>
        <link>
        http://randyzwitch.com/rsitecatalyst-version-1-4-4-release-notes/</link>
        <guid isPermaLink="true">http://randyzwitch.com/rsitecatalyst-version-1-4-4-release-notes/</guid>
        <content type="html" xml:base="/rsitecatalyst-version-1-4-4-release-notes/">It's been about six months since the last [RSiteCatalyst update](http://randyzwitch.com/rsitecatalyst-version-1-4-3-release-notes/), and this update is really just a single bug fix, but a big bug fix at that!

### Sparse Data = Opaque Error Messages

Numerous people have reported receiving an error message from RSiteCatalyst similar to the following:

&gt; 'names' attribute [1] must be the same length as the vector [0]

This is about the least helpful message that could've been returned, but it was an R message indicating an internal function trying to overwrite the column names vector (which had non-zero length) with a vector of length zero (which is an error in the context of a data frame). Thankfully, Willem Paling was able to squash this bug (hopefully) once-and-for-all; the error occurs when a user tries to do a `Queue*`` report with multiple breakdowns, where NULL data is returned by the Adobe API for one of the breakdowns.

So hopefully, if you've run into this error before (which I have to imagine was quite frustrating), you shouldn't see this again with v1.4.4 of RSiteCatalyst. Additionally, tests will be added to the test suite to attempt to trigger this warning, so that this horrible monster of a bug doesn't appear again.

### Authentication Messaging

The only other change of substance was to modify the message returned after calling `SCAuth()`; some users were having issues with API calls not working, after RSiteCatalyst having returned `'Authentication Succeeded'` to the console. RSiteCatalyst never actually validates that your credentials are correct, just that they are stored within the session. The console message has been updated to reflect this.

### Proper Punctuation Prevents Poor Documentation!

![](/wp-content/uploads/2015/07/title-case.png)

The Eagle-Eyed among you might have noticed that my DESCRIPTION file was out of CRAN spec for many months. This has now been fixed, so that the meaning is as clear as possible.

## Feature Requests/Bugs

As always, if you come across bugs or have feature requests, please continue to use the [RSiteCatalyst GitHub Issues](https://github.com/randyzwitch/RSiteCatalyst/issues) page to submit issues. Don’t worry about cluttering up the page with tickets, please fill out a new issue for anything you encounter (with code you’ve already tried and is failing), unless you are SURE that it is the same problem someone else is facing.

Contributors are also _very welcomed_. If there is a feature you'd like added, and especially if you can fix an outstanding issue reported at GitHub, we'd love to have your contributions. Willem and I are both parents of young children and have real jobs outside of open-source software creation, so we welcome any meaningful contributions to RSiteCatalyst that anyone would like to contribute.</content>
      </item>
      
    
      
      <item>
        <title>Vega.jl, Rebooted</title>
        
          <description>&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2015/05/pie-300x251.png&quot; alt=&quot;pie&quot; /&gt;
&lt;img src=&quot;/wp-content/uploads/2015/05/donut-e1432224478621.png&quot; alt=&quot;donut&quot; /&gt;&lt;/p&gt;

</description>
        
        <pubDate>Thu, 21 May 2015 08:56:07 -0400</pubDate>
        <link>
        http://randyzwitch.com/vega-jl-julia/</link>
        <guid isPermaLink="true">http://randyzwitch.com/vega-jl-julia/</guid>
        <content type="html" xml:base="/vega-jl-julia/">![pie](/wp-content/uploads/2015/05/pie-300x251.png)
![donut](/wp-content/uploads/2015/05/donut-e1432224478621.png)

&lt;p style=&quot;text-align: center;&quot;&gt;
  Mmmmm, baked goods!
&lt;/p&gt;

### Rebooting Vega.jl

Recently, I've found myself without a project to hack on, and I've always been interested in learning more about browser-based visualization. So I decided to revive the work that &lt;a href=&quot;https://github.com/johnmyleswhite&quot; target=&quot;_blank&quot;&gt;John Myles White&lt;/a&gt; had done in building [Vega.jl](https://github.com/johnmyleswhite/Vega.jl) nearly two years ago. And since I'll be giving an analytics &amp; visualization workshop at &lt;a href=&quot;http://juliacon.org/&quot; target=&quot;_blank&quot;&gt;JuliaCon 2015&lt;/a&gt;, I figure I better study the topic in a bit more depth.

### Back In Working Order!

The first thing I tackled here was to upgrade the syntax to target v0.4 of Julia. This is just my developer preference, to avoid using &lt;a href=&quot;https://github.com/JuliaLang/Compat.jl&quot; target=&quot;_blank&quot;&gt;Compat.jl&lt;/a&gt; when there are so many more visualizations I'd like to support. So if you're using v0.4, you shouldn't see any deprecation errors; if you're using v0.3, well, eventually you'll use v0.4!

Additionally, I modified the package to recognize the traction that Jupyter Notebook has gained in the community. Whereas the original version of Vega.jl only displayed output in a tab in a browser, I've overloaded the `writemime` method to display `:VegaVisualization` inline for any environment that can display HTML. If you use Vega.jl from the REPL, you'll still get the same default browser-opening behavior as existed before.

### The First Visualization You Added Was A Pie Chart...

### ...And Followed With a Donut Chart?

Yup. I'm a troll like that. Besides, being loudly against pie charts is blowhardy (even if studies have shown that people are too stupid to evaluate them).

Adding these two charts (besides trolling) was a proof-of-concept that I understood the codebase sufficiently in order to extend the package. Now that the syntax is working for Julia v0.4, I understand how the package works (important!), and have improved the workflow by supporting Jupyter Notebook, I plan to create all of the visualizations featured in the &lt;a href=&quot;http://trifacta.github.io/vega/editor/&quot; target=&quot;_blank&quot;&gt;Trifacta Vega Editor&lt;/a&gt; and other standard visualizations such as boxplots. If the community has requests for the order of implementation, I'll try and accommodate them. Just add a feature request on &lt;a href=&quot;https://github.com/johnmyleswhite/Vega.jl/issues&quot; target=&quot;_blank&quot;&gt;Vega.jl GitHub issues&lt;/a&gt;.

### Why Not Gadfly? You're Not Starting A Language War, Are You?

No, I'm not that big of a troll. Besides, I don't think we've squeezed all the juice (blood?!) out of the &lt;a href=&quot;http://blog.datacamp.com/r-or-python-for-data-analysis/&quot; target=&quot;_blank&quot;&gt;R vs. Python infographic&lt;/a&gt; yet, we don't need another pointless debate.

My sole reason for not improving &lt;a href=&quot;http://dcjones.github.io/Gadfly.jl/&quot; target=&quot;_blank&quot;&gt;Gadfly&lt;/a&gt; is just that I plain don't understand how the codebase works! There are many amazing computer scientists &amp; developers in the Julia community, and I'm not really one of them. I do, however, understand how to generate JSON strings and in that sense, Vega is the perfect platform for me to contribute.

### Collaborators Wanted!

If you're interested in visualization, as well as learning Julia and/or contributing to a package, Vega.jl might be a good place to start. I'm always up for collaborating with people, and creating new visualizations isn't that difficult (especially with the Trifacta examples). So hopefully some of you will be interested in enough to join me to adding one more great visualization library to the Julia community.</content>
      </item>
      
    
      
      <item>
        <title>Sessionizing Log Data Using data.table [Follow-up #2]</title>
        
          <description>&lt;p&gt;Thanks to user &lt;a title=&quot;dnlbrky comment&quot; href=&quot;http://randyzwitch.com/sessionizing-log-data-dplyr-r-window-functions/#comment-16205&quot; target=&quot;_blank&quot;&gt;dnlbrky&lt;/a&gt;, we now have a third way to accomplish sessionizing log data for any arbitrary time out period (see methods &lt;a href=&quot;/sessionizing-log-data-sql/&quot; title=&quot;Sessionizing Log Data Using SQL&quot;&gt;1&lt;/a&gt; and &lt;a href=&quot;/sessionizing-log-data-dplyr-r-window-functions/&quot; title=&quot;Sessionizing Log Data Using dplyr [Follow-up]&quot;&gt;2&lt;/a&gt;), this time using data.table from R along with magrittr for piping:&lt;/p&gt;

</description>
        
        <pubDate>Tue, 20 Jan 2015 04:01:19 -0500</pubDate>
        <link>
        http://randyzwitch.com/sessionizing-log-data-using-data-table-follow-2/</link>
        <guid isPermaLink="true">http://randyzwitch.com/sessionizing-log-data-using-data-table-follow-2/</guid>
        <content type="html" xml:base="/sessionizing-log-data-using-data-table-follow-2/">Thanks to user &lt;a title=&quot;dnlbrky comment&quot; href=&quot;http://randyzwitch.com/sessionizing-log-data-dplyr-r-window-functions/#comment-16205&quot; target=&quot;_blank&quot;&gt;dnlbrky&lt;/a&gt;, we now have a third way to accomplish sessionizing log data for any arbitrary time out period (see methods [1](/sessionizing-log-data-sql/ &quot;Sessionizing Log Data Using SQL&quot;) and [2](/sessionizing-log-data-dplyr-r-window-functions/ &quot;Sessionizing Log Data Using dplyr [Follow-up]&quot;)), this time using data.table from R along with magrittr for piping:

{% highlight R linenos %}
library(magrittr)
library(data.table)

## Download, unzip, and load data (first 10,000 lines):
single_col_timestamp &lt;- url(&quot;http://randyzwitch.com/wp-content/uploads/2015/01/single_col_timestamp.csv.gz&quot;) %&gt;%
  gzcon %&gt;%
  readLines(n=10000L) %&gt;%
  textConnection %&gt;%
  read.csv %&gt;%
  setDT

## Convert to timestamp:
single_col_timestamp[, event_timestamp:=as.POSIXct(event_timestamp)]

## Order by uid and event_timestamp:
setkey(single_col_timestamp, uid, event_timestamp)

## Sessionize the data (more than 30 minutes between events is a new session):
single_col_timestamp[, session_id:=paste(uid, cumsum((c(0, diff(event_timestamp))/60 &gt; 30)*1), sep=&quot;_&quot;), by=uid]

## Examine the results:
#single_col_timestamp[uid %like% &quot;a55bb9&quot;]
single_col_timestamp[session_id %like% &quot;fc895c3babd&quot;]
{% endhighlight %}

I agree with dnlbrky in that this feels a little better than the dplyr method for heavy SQL users like me, but ultimately, I still think the SQL method is the most elegant and obvious to understand. But that's the great thing with open-source software; pick any tool you want, accomplish whatever you choose using any method you choose.</content>
      </item>
      
    
      
      <item>
        <title>Sessionizing Log Data Using dplyr [Follow-up]</title>
        
          <description>&lt;p&gt;Last week, I wrote a blog post showing how to &lt;a href=&quot;http://randyzwitch.com/sessionizing-log-data-sql&quot;&gt;sessionize log data using standard SQL&lt;/a&gt;. The main idea of that post is that if your analytics platform supports window functions (like Postgres and Hive do), you can make quick work out of sessionizing logs. Here’s the winning query:&lt;/p&gt;

</description>
        
        <pubDate>Tue, 13 Jan 2015 11:24:52 -0500</pubDate>
        <link>
        http://randyzwitch.com/sessionizing-log-data-dplyr-r-window-functions/</link>
        <guid isPermaLink="true">http://randyzwitch.com/sessionizing-log-data-dplyr-r-window-functions/</guid>
        <content type="html" xml:base="/sessionizing-log-data-dplyr-r-window-functions/">Last week, I wrote a blog post showing how to [sessionize log data using standard SQL](http://randyzwitch.com/sessionizing-log-data-sql). The main idea of that post is that if your analytics platform supports window functions (like Postgres and Hive do), you can make quick work out of sessionizing logs. Here's the winning query:

{% highlight sql linenos %}
select
uid,
sum(new_event_boundary) OVER (PARTITION BY uid ORDER BY event_timestamp) as session_id,
event_timestamp,
minutes_since_last_interval,
new_event_boundary
from
			--Query 1: Define boundary events
			(select
			uid,
			event_timestamp,
			(extract(epoch from event_timestamp) - lag(extract(epoch from event_timestamp)) OVER (PARTITION BY uid ORDER BY event_timestamp))/60 as minutes_since_last_interval,
			case when extract(epoch from event_timestamp) - lag(extract(epoch from event_timestamp)) OVER (PARTITION BY uid ORDER BY event_timestamp) &gt; 30 * 60 then 1 ELSE 0 END as new_event_boundary
			from single_col_timestamp
			) a;
{% endhighlight %}

One nested sub-query and two window functions are all it takes to calculate the event boundaries and create a unique identifier for sessions for any arbitrary timeout chosen.

## It's Hadley's House, We're Just Leasing

Up until today, I hadn't really done anything using dplyr.  But having a bunch of free time this week and hearing people talk so much about how great dplyr is, I decided to see what it would take to replicate this same exercise using R. dplyr has support for Postgres as a back-end, and has verbs that translate R code into window functions, so I figured it had to be possible. Here's what I came up with:

{% highlight R linenos %}
###Sessionization using dplyr

library(dplyr)

#Open a localhost connection to Postgres
#Use table 'single_col_timestamp'
#group by uid and sort by timestamp for window function
#Do minutes calculation, working around missing support for extract(epoch from timestamp)
#Calculate event boundary and unique id via cumulative sum window function
sessions &lt;-  
        src_postgres(&quot;logfiles&quot;) %&gt;%
        tbl(&quot;single_col_timestamp&quot;) %&gt;%
        group_by(uid) %&gt;%
        arrange(event_timestamp) %&gt;%
        mutate(minutes_since_last_event = (
                                           DATE_PART('day', event_timestamp - lag(event_timestamp)) * 24 +
                                           DATE_PART('hour', event_timestamp - lag(event_timestamp)) * 60 +
                                           DATE_PART('minute', event_timestamp - lag(event_timestamp)) * 60 +
                                           DATE_PART('second', event_timestamp - lag(event_timestamp))
                                           ) / 60
              ) %&gt;%
        mutate(event_boundary = if(minutes_since_last_event &gt; 30) 1 else 0,
               session_id = order_by(event_timestamp, cumsum(if(minutes_since_last_event &gt; 30) 1 else 0)))

#Show query syntax
show_query(sessions)

#Actually run the query
answer &lt;- collect(sessions)
{% endhighlight %}

Generally, I'm not a fan of the pipe operator, but I figured I'd give it a shot since everyone else seems to like it. This is one nasty bit of R code, but ultimately, it is possible to get the same result as writing SQL directly. I did need to take a few roundabout ways, specifically in calculating the minutes between timestamps and substituting the CASE expression into the window function rather than call it by name, but it's basically the same logic.

## Why Does This Work?

If you compare the SQL code above to the R code, you might be wondering why the dplyr code works. Certainly, working the dplyr way gives me cognitive dissonance, as you generally specify the verbs you are using in reverse order as you do in SQL. But calling `show_query(sessions)`, you actually see that dplyr is generating SQL under-the-hood (I formatted the code for easier viewing):

{% highlight sql linenos %}
SELECT
	&quot;uid&quot;,
	&quot;event_timestamp&quot;,
	&quot;minutes_since_last_event&quot;,
	&quot;event_boundary&quot;,
	&quot;session_id&quot;
FROM (
		SELECT
			&quot;uid&quot;,
			&quot;event_timestamp&quot;,
			&quot;minutes_since_last_event&quot;,
			CASE WHEN &quot;minutes_since_last_event&quot; &gt; 30.0 THEN 1.0 ELSE 0.0 END AS &quot;event_boundary&quot;,
			sum(CASE WHEN &quot;minutes_since_last_event&quot; &gt; 30.0 THEN 1.0 ELSE 0.0 END) OVER (PARTITION BY &quot;uid&quot; ORDER BY &quot;event_timestamp&quot; ROWS UNBOUNDED PRECEDING) AS &quot;session_id&quot;
		FROM
			(
				SELECT
					&quot;uid&quot;,
					&quot;event_timestamp&quot;,
					(DATE_PART('day', &quot;event_timestamp&quot; - LAG(&quot;event_timestamp&quot;, 1, NULL) OVER (PARTITION BY &quot;uid&quot; ORDER BY &quot;event_timestamp&quot;)) * 24.0
						+ DATE_PART('hour', &quot;event_timestamp&quot; - LAG(&quot;event_timestamp&quot;, 1, NULL) OVER (PARTITION BY &quot;uid&quot; ORDER BY &quot;event_timestamp&quot;)) * 60.0
						+ DATE_PART('minute', &quot;event_timestamp&quot; - LAG(&quot;event_timestamp&quot;, 1, NULL) OVER (PARTITION BY &quot;uid&quot; ORDER BY &quot;event_timestamp&quot;)) * 60.0
						+ DATE_PART('second', &quot;event_timestamp&quot; - LAG(&quot;event_timestamp&quot;, 1, NULL) OVER (PARTITION BY &quot;uid&quot; ORDER BY &quot;event_timestamp&quot;))) / 60.0 AS &quot;minutes_since_last_event&quot;
				FROM &quot;single_col_timestamp&quot;
				ORDER BY &quot;uid&quot;, &quot;event_timestamp&quot;
			) AS &quot;_W1&quot;
	) AS &quot;_W2&quot;
{% endhighlight %}

Like all SQL-generating tools, the code is a bit inelegant; however, I have to say that I'm truly impressed the dplyr code was able to handle this scenario at all, given that this example has to be at least an edge-, if not a corner-case of what dplyr is meant for in terms of data manipulation.

## So, dplyr Is Going To Become Part Of Your Toolbox?

While it was possible to re-create the same functionality, ultimately, I don't see myself using dplyr a whole lot. In the case of using databases, it seems more efficient and portable just to write the SQL directly; at the very least, it's what I'm already comfortable doing as part of my analytics workflow. For manipulating data frames, maybe I'd use it (I do use plyr extensively in my [RSiteCatalyst](http://cran.r-project.org/web/packages/RSiteCatalyst/index.html) package), but I'd probably be more inclined to use [sqldf](http://randyzwitch.com/sqldf-package-r/) instead.

But that's just me, not a reflection on the package quality. Happy manipulating, however you choose to do it! 🙂</content>
      </item>
      
    
      
      <item>
        <title>Sessionizing Log Data Using SQL</title>
        
          <description>&lt;p&gt;Over my career as a predictive modeler/data scientist, the most important step(s) in any data project without question have been data cleaning and feature engineering. By taking the data you have, correcting flaws and reformulating raw data into additional business-specific concepts, you ensure that you move beyond pure mathematical optimization and actually solve a &lt;em&gt;business problem&lt;/em&gt;. While “big data” is often held up as the future of knowing everything, when it comes down to it, a Hadoop cluster is more often a “Ha-dump” cluster: the place data gets dumped without any proper ETL.&lt;/p&gt;

</description>
        
        <pubDate>Thu, 08 Jan 2015 06:57:56 -0500</pubDate>
        <link>
        http://randyzwitch.com/sessionizing-log-data-sql/</link>
        <guid isPermaLink="true">http://randyzwitch.com/sessionizing-log-data-sql/</guid>
        <content type="html" xml:base="/sessionizing-log-data-sql/">Over my career as a predictive modeler/data scientist, the most important step(s) in any data project without question have been data cleaning and feature engineering. By taking the data you have, correcting flaws and reformulating raw data into additional business-specific concepts, you ensure that you move beyond pure mathematical optimization and actually solve a _business problem_. While &quot;big data&quot; is often held up as the future of knowing everything, when it comes down to it, a Hadoop cluster is more often a &quot;Ha-dump&quot; cluster: the place data gets dumped without any proper ETL.

For this blog post, I'm going to highlight a common request for time-series data: combining discrete events into sessions. Whether you are dealing with sensor data, television viewing data, digital analytics data or any other stream of events, the problem of interest is usually how a human interacts with a machine over a given period of time, not each individual event.

While I usually use Hive (Hadoop) for daily work, I'm going to use Postgres (via OSX &lt;a title=&quot;Postgres.app OSX&quot; href=&quot;http://postgresapp.com&quot; target=&quot;_blank&quot;&gt;Postgres.app&lt;/a&gt;) to make this as widely accessible as possible. In general, this process will work with any infrastructure/SQL-dialect that supports [window functions](http://www.postgresql.org/docs/9.1/static/tutorial-window.html).

## Connecting to Database/Load Data

For lightweight tasks, I find using psql (command-line tool) is easy enough. Here are the commands to create a database to hold our data and to load our two .csv files (download [here](/wp-content/uploads/2015/01/single_col_timestamp.csv.gz) and [here](/wp-content/uploads/2015/01/two_col_timestamp.csv.gz)):

![psql-load-data](/wp-content/uploads/2015/01/psql-load-data.png)

These files contain timestamps generated for 1000 uid values.

## Query 1 (&quot;Inner&quot;): Determining Session Boundary Using A Window Function

In order to determine the boundary of each session, we can use a window function along with `lag()`, which will allow the current row being processed to compare vs. the prior row. Of course, for all of this to work correctly, we need to have our data sorted in time order by each of our users:

{% highlight sql linenos %}
--Create boundaries at 30 minute timeout
select
uid,
event_timestamp,
(extract(epoch from event_timestamp) - lag(extract(epoch from event_timestamp)) OVER (PARTITION BY uid ORDER BY event_timestamp))/60 as minutes_since_last_interval,
case when extract(epoch from event_timestamp) - lag(extract(epoch from event_timestamp)) OVER (PARTITION BY uid ORDER BY event_timestamp) &gt; 30 * 60 then 1 ELSE 0 END as new_event_boundary
from single_col_timestamp;
{% endhighlight %}

For this query, we use the `lag()` function on the `event_timestamp` column, and we use `over partition by uid order by event_timestamp` to define the window over which we want to do our calculation. To provide additional clarification about how this syntax works, I've added a column showing how many minutes have passed between intervals to validate that the 30-minute window is calculated correctly. The result is as follows:

![sql-session-boundary-definition](/wp-content/uploads/2015/01/sql-session-boundary-definition.png)

For each row where the value of `minutes_since_last_interval &gt; 30`, there is a value of `1` for `new_event_boundary`.

## Query 2 (&quot;Outer&quot;): Creating A Session ID

The query above defines the event boundaries (which is helpful), but if we want to calculate session-level metrics, we need to create a unique id for each set of rows that are part of one session. To do this, we're again going to use a window function:

{% highlight sql linenos %}
select
uid,
sum(new_event_boundary) OVER (PARTITION BY uid ORDER BY event_timestamp) as session_id,
event_timestamp,
minutes_since_last_interval,
new_event_boundary
from
			--Query 1: Define boundary events
			(select
			uid,
			event_timestamp,
			(extract(epoch from event_timestamp) - lag(extract(epoch from event_timestamp)) OVER (PARTITION BY uid ORDER BY event_timestamp))/60 as minutes_since_last_interval,
			case when extract(epoch from event_timestamp) - lag(extract(epoch from event_timestamp)) OVER (PARTITION BY uid ORDER BY event_timestamp) &gt; 30 * 60 then 1 ELSE 0 END as new_event_boundary
			from single_col_timestamp
			) a;
{% endhighlight %}

This query defines the same `over partition by uid order by event_timestamp` window, but rather than using `lag()` this time, we're going to use `sum()` for the outer query. The effect of using `sum()` in our window function is to do a cumulative sum; every time `1` shows up, the `session_id` field gets incremented by `1`. If there is a value of `0`, the sum is still the same as the row above and thus has the same `session_id`. This is easier to understand visually:

![sessionized-data](/wp-content/uploads/2015/01/sessionized-data.png)

At this point, we have a `session_id` for a group of rows where there have been no 30 minute gaps in behavior.

## Final Query: Cleaned Up

Although the previous section is technically done, I usually concatenate the uid and session_id together.  I do this concatenation just to highlight that the value is usually a 'key' value, not a metric in itself (though it can be). Concatenating the keys together and removing the teaching columns results in the following query:

{% highlight R linenos %}
--Query 3:  Outer query uses window function with sum to do cumulative sum as the id, concatentate to uid
select
uid,
uid || '-' || cast(sum(new_event_boundary) OVER (PARTITION BY uid ORDER BY event_timestamp) as varchar) as session_id,
event_timestamp
from
			--Query 1: Define boundary events
			(select
			uid,
			event_timestamp,
			case when extract(epoch from event_timestamp) - lag(extract(epoch from event_timestamp)) OVER (PARTITION BY uid ORDER BY event_timestamp) &gt; 30 * 60 then 1 ELSE 0 END as new_event_boundary
			from single_col_timestamp
			) a;
{% endhighlight %}

![final-sessionized-data](/wp-content/uploads/2015/01/final-sessionized-data.png)

## Window Functions, Will You Marry Me?

The first time I was asked to try and solve sessionization of time-series data using Hive, I was sure the answer would be that I'd have to get a peer to write some nasty custom Java code to be able generate unique ids; in retrospect, the solution is so obvious and simple that I wish I would've tried to do this years ago. This is a pretty easy problem to solve using imperative programming, but if you've got a gigantic amount of hardware in a RDBMS or Hadoop, SQL takes care of all of the calculation without needing think through looping (or more complicated logic/data structures).

Window functions fall into a weird space in the SQL language, given that they allow you to do sequential calculations when SQL should generally be thought of as &quot;set-level&quot; calculations (i.e. no implied order and table-wide calculations vs. row/state-specific). But now that I've got a hang of them, I can't imagine my analytical life without them.</content>
      </item>
      
    
      
      <item>
        <title>RSiteCatalyst Version 1.4.3 Release Notes</title>
        
          <description>&lt;p&gt;It’s a new year, so…new version of RSiteCatalyst on CRAN! For the most part, this release fixes a handful of bugs that weren’t noticed with the prior release 1.4.2 (oops!), but there are pieces of additional functionality.&lt;/p&gt;

</description>
        
        <pubDate>Tue, 06 Jan 2015 08:00:40 -0500</pubDate>
        <link>
        http://randyzwitch.com/rsitecatalyst-version-1-4-3-release-notes/</link>
        <guid isPermaLink="true">http://randyzwitch.com/rsitecatalyst-version-1-4-3-release-notes/</guid>
        <content type="html" xml:base="/rsitecatalyst-version-1-4-3-release-notes/">It's a new year, so...new version of RSiteCatalyst on CRAN! For the most part, this release fixes a handful of bugs that weren't noticed with the prior release 1.4.2 (oops!), but there are pieces of additional functionality.

## New functionality: Data Feed monitoring

For those of you having hourly or daily data feeds delivered via FTP, you can now find out the details of a data feed and all of a company's feeds &amp; the processing status of each using `GetFeed()` and `GetFeeds()` respectively.

For example, calling `GetFeed()` with a specific feed number will return the following information as a data frame:

![rsitecatalyst-getfeed](/wp-content/uploads/2015/01/rsitecatalyst-getfeed.png)

Similarly, if you call `GetFeeds(&quot;report-suite&quot;)`, you'll get the following information as a data frame:

![rsitecatalyst-getfeeds](/wp-content/uploads/2015/01/rsitecatalyst-getfeeds.png)

I only have one feed set up for testing, but if there were more feeds delivered each day, they would show up as additional rows in the data frame. The interpretation here is that the daily feed for 1/5/15 was delivered (the 05:00:00 is GMT).

## Bug Fixes

RSiteCatalyst v1.4.2 attempted to fix an issue where `QueueRanked` would error if two SAINT classifications were used. Unfortunately, by fixing that issue, `QueueRanked` ONLY worked with SAINT Classifications. This was only out in the wild for a month, so hopefully it didn't really affect anyone.

Additionally, the `segment.id` and `segment.name` weren't printing out to the data frame in the `Queue*` functions. This has also been fixed.

## Test Suite Using Travis CI

To avoid future errors like the ones mentioned above, a full test suite using [testthat](https://github.com/hadley/testthat) has been added to RSiteCatalyst and monitored via [Travis CI](https://travis-ci.org/randyzwitch/RSiteCatalyst). While there is coverage for every public function within the package, there are likely additional tests that can be added for functionality I didn't cover. If anyone out there has particularly weird cases they use and aren't incorporated in the [test suite](https://github.com/randyzwitch/RSiteCatalyst/tree/master/tests/testthat), please feel free to file an issue or submit a pull request and I'll figure out how to incorporate it into the test suite.

## &lt;del&gt;DataWarehouse API&lt;/del&gt;

&lt;del&gt;Finally, the last bit of changes to RSiteCatalyst in v1.4.3 are internal preparations for a new package I plan to release in the coming months: &lt;a title=&quot;AdobeDW DataWarehouse&quot; href=&quot;https://github.com/randyzwitch/AdobeDW&quot; target=&quot;_blank&quot;&gt;AdobeDW&lt;/a&gt;. Several folks have asked for the ability to control Data Warehouse reports via R; for various reasons, I thought it made sense to break this out from RSiteCatalyst into its own package. If there are any R-and-Adobe-Analytics enthusiasts out there that would like to help development, please let me know! &lt;/del&gt;

## Feature Requests/Bugs

As always, if you come across bugs or have feature requests, please continue to use the [RSiteCatalyst GitHub Issues](https://github.com/randyzwitch/RSiteCatalyst/issues) page to submit issues. Don’t worry about cluttering up the page with tickets, please fill out a new issue for anything you encounter (with code you’ve already tried and is failing), unless you are SURE that it is the same problem someone else is facing.

And finally, like I end every blog post about RSiteCatalyst, please note that **I’m** **not an Adobe employee**. This hasn't been an issue for a few months, so maybe next time I won't end the post with this boilerplate :)</content>
      </item>
      
    
      
      <item>
        <title>Review: Data Science at the Command Line</title>
        
          <description>&lt;p&gt;Admission: I didn’t &lt;em&gt;really know&lt;/em&gt; how computers worked until around 2012.&lt;/p&gt;

</description>
        
        <pubDate>Mon, 15 Dec 2014 05:22:46 -0500</pubDate>
        <link>
        http://randyzwitch.com/data-science-command-line-review-janssens/</link>
        <guid isPermaLink="true">http://randyzwitch.com/data-science-command-line-review-janssens/</guid>
        <content type="html" xml:base="/data-science-command-line-review-janssens/">Admission: I didn't _really know_ how computers worked until around 2012.

For the majority of my career, I've worked for large companies with centralized IT functions. Like many statisticians, I fell into a comfortable position of learning SAS in a Windows environment, had Ops people to fix any Unix problems I'd run into and DBAs to load data into a relational database environment.

Then I became a consultant at a boutique digital analytics firm. To say I was punching above my weight was an understatement. All of the sudden it was time to go into various companies, have a one-hour kickoff meeting, then start untangling the spaghetti mess that represented their various technology systems. I also needed to figure out the boutique firm's hacked together AWS and Rackspace infrastructure.

[![data-science-command-line](/wp-content/uploads/2014/12/data-science-command-line.png)](http://datascienceatthecommandline.com/)

I'm starting off this review with this admission, because my story of learning to work from the command line parallels [Data Science at the Command Line](http://datascienceatthecommandline.com/) author [Jeroen Janssens](https://twitter.com/jeroenhjanssens):

&gt; Around five years ago, during my PhD program, I gradually switched from using Microsoft Windows to GNU/Linux...Out of necessity I quickly became comfortable using the command line. Eventually, as spare time got more precious, I settled down with a GNU/Linux distribution known as Ubuntu...
&gt;
&gt; - Preface, pg. xi

Because a solid majority of people have never learned anything beyond point-and-click interface (Windows or Mac), the title of the book _Data Science at the Command Line_ is somewhat unfortunate; this is a book for ANYONE looking to start manipulating files efficiently from the command line.

## Getting Started, Safely

One of the best parts of _Data Science at the Command Line_ is that it comes with a [pre-built virtual machine](http://datasciencetoolbox.org/) with 80-100 or more command line tools installed. This is a very fast and safe way to get started with the command line, as the tools are pre-installed and no matter what command you run while you're learning, you won't destroy a computer you actually care about!

Chapters 2 and 3 move through the steps of installing the virtual machine, explaining the essential concepts of the command line, some basic commands showing simple (but powerful!) ways to chain command line tools together and how to obtain data. What I find so refreshing about these two chapters by Janssens is that the author assumes zero knowledge of the command line by the reader; these two chapters are the most accessible summary of how and why to use the command line I've ever read ([Zed Shaw's CLI tutorial](http://cli.learncodethehardway.org/book/) is a close second, but is quite terse).

## The OSEMN model

The middle portion of book covers the [OSEMN model](http://www.dataists.com/2010/09/a-taxonomy-of-data-science/) (Obtain-Scrub-Explore-Model-iNterpret) of data science; another way this book is refreshing is that rather than jump right into machine learning/predictive modeling, the author spends a considerable amount of time covering the gory details of real analysis projects: manipulating data from the format you _receive_ (XML, JSON, sloppy CSV files, etc.) and taking the (numerous) steps required to get the format you _want_.

By introducing tools such as [csvkit](https://csvkit.readthedocs.org/en/0.9.0/) (csv manipulation), [jq](http://stedolan.github.io/jq/) (JSON processor), and classic tools such as [sed](https://www.gnu.org/software/sed/manual/sed.html) (stream editor) and [(g)awk](http://www.gnu.org/software/gawk/manual/gawk.html), the reader gets a full treatment of how to deal with malformed data files (which in my experience are the only type available in the wild!) . Chapter 6 (&quot;Managing Your Data Workflow&quot;) is also a great introduction into [reproducible research](http://en.wikipedia.org/wiki/Reproducibility#Reproducible_research) using [Drake](http://blog.factual.com/introducing-drake-a-kind-of-make-for-data) (Make for Data Analysis). This is an area that I will personally be focusing my time on, as I tend to run a lot of one-off commands in HDFS and as of now, just copy them into a plain-text file. Reproducing = copy-paste in my case, which defeats the purpose of computers and scripting!

## An Idea Can Be Stretched Too Far

Chapters 8 and 9 cover Parallel Processing using [GNU Parallel](http://www.gnu.org/software/parallel/) and Modeling Data respectively. While GNU Parallel is a tool I could see using sometime in the future, I do feel like building models and creating visualizations straight from the command line is getting pretty close to just being a parlor trick. Yes, it's obviously possible to do such things (and the author even wrote his own [command line tool Rio](https://github.com/jeroenjanssens/data-science-at-the-command-line/blob/master/tools/Rio) for using R from the command line), but with the amount of iteration, feature building and fine-tuning that goes on, I'd rather use [IPython Notebook](http://ipython.org/notebook.html) or [RStudio](http://www.rstudio.com/) to give me the flexibility I need to really iterate effectively.

## A Book For Everyone

As I mentioned above, I really feel that _Data Science at the Command Line_ is a book well suited for anyone who does data analysis. Jeroen Janssens has done a fantastic job of taking his original [&quot;7 command-line tools for data science&quot;](http://jeroenjanssens.com/2013/09/19/seven-command-line-tools-for-data-science.html) blog post and extending the idea to a full-fledged book. This book has a prominent place in my work library next to [Python for Data Analysis](http://shop.oreilly.com/product/0636920023784.do) and in the past two months I've referred to each book at roughly the same rate. For under $30 for paperback at Amazon, there's more than enough content to make you a better data scientist.</content>
      </item>
      
    
      
      <item>
        <title>Introducing Twitter.jl</title>
        
          <description>&lt;p&gt;This is possibly the latest “announcement” of a package ever, given that &lt;a href=&quot;https://github.com/randyzwitch/Twitter.jl&quot;&gt;Twitter.jl&lt;/a&gt; has existed on &lt;a href=&quot;https://github.com/JuliaLang/METADATA.jl&quot; title=&quot;Julia METADATA&quot;&gt;METADATA&lt;/a&gt; for nearly a year now, but that’s how things go sometimes. Here’s how to get started with Twitter.jl.&lt;/p&gt;

</description>
        
        <pubDate>Mon, 08 Dec 2014 12:12:58 -0500</pubDate>
        <link>
        http://randyzwitch.com/twitter-api-julia/</link>
        <guid isPermaLink="true">http://randyzwitch.com/twitter-api-julia/</guid>
        <content type="html" xml:base="/twitter-api-julia/">This is possibly the latest &quot;announcement&quot; of a package ever, given that [Twitter.jl](https://github.com/randyzwitch/Twitter.jl) has existed on [METADATA](https://github.com/JuliaLang/METADATA.jl &quot;Julia METADATA&quot;) for nearly a year now, but that's how things go sometimes. Here's how to get started with Twitter.jl.

## Hello, World!

If 'Hello, World!' is the canonical example of getting started with a programming language, the Twitter API is becoming the first place to start for people wanting to learn about APIs. Authenticating with the Twitter API using Julia is similar to using the R or Python packages, except that rather than doing the OAuth &quot;dance&quot;, Twitter.jl takes all four authentication values in one function:

{% highlight julia linenos %}
using Twitter

apikey = &quot;q8Qw7WJTVP...&quot;
apisecret = &quot;FIichPpGJxiOssN...&quot;
accesstoken = &quot;98689850-v0zZNr...&quot;
accesstokensecret = &quot;w7bDg9K0c493T...&quot;

twitterauth(apikey,
            apisecret,
            accesstoken,
            accesstokensecret)
{% endhighlight %}

All four of these values can be found after registering at the [Twitter Developer page](https://dev.twitter.com/) and creating an application. Having all four values in your script is less secure than just providing the api key and api secret, but in the future, I'll likely implement the full OAuth &quot;handshake&quot;. One thing to keep in mind with this function as it currently works is that no validation of your credentials is performed; the only thing this function does is define a global variable `twittercred` for later use by the various functions that create the OAuth headers. To shout &quot;Hello, World!&quot; to all of your Twitter followers, you can use the following code:

{% highlight julia linenos %}
post_status_update(&quot;Hello, World!&quot;)
{% endhighlight %}

## General Package/Function Structure

From the example above, you can see that the function naming follows the [Twitter REST API](https://dev.twitter.com/rest/public) naming convention, with the HTTP verb first and the endpoint as the remainder of the function name. As such, it's a good idea at this early package state to have the Twitter documentation open while using this package, so that you can quickly find the methods you are looking for.

For each function/API endpoint, I've gone through and determined which parameters are required; these are required arguments in the Julia functions. For all other options, each function takes a second optional `Dict{String, String}` for any option shown in the Twitter documentation. While this Dict structure allows for ultimate flexibility (and quick definition of functions!), I do realize that it's less than optimal that you don't know what optional arguments each Twitter endpoint allows.

As an example, suppose you wanted to search for tweets containing the hashtag `#julialang`. The minimum function call is as follows:

{% highlight julia linenos %}
julia_tweets = get_search_tweets(&quot;#julialang&quot;)
{% endhighlight %}

By default, the API will return the 15 most recent tweets containing the `#julialang` hashtag. To return the most recent 100 tweets (the maximum per API 'page'), you can pass the &quot;count&quot; parameter via the `Options` Dict:

{% highlight julia %}
julia_tweets_100 = get_search_tweets(&quot;#julialang&quot;; options = {&quot;count&quot; =&gt; &quot;100&quot;})
{% endhighlight %}

## Composite Types and DataFrames definitions

The Twitter API is structured into 4 return data types ([Places](https://dev.twitter.com/overview/api/places), [Users](https://dev.twitter.com/overview/api/users), [Tweets](https://dev.twitter.com/overview/api/tweets), and [Entities](https://dev.twitter.com/overview/api/entities)), and I've mimicked these types using Julia [Composite Types](http://julia.readthedocs.org/en/latest/manual/types/#composite-types). As such, most functions in Twitter.jl return an array of specific type, such as `Array{TWEETS,1}` from the prior `#julialang` search example. The benefit to defining custom types for the returned Twitter data is that rudimentary DataFrame methods have also been defined:

{% highlight julia linenos %}
df = DataFrame(julia_tweets_100)
{% endhighlight %}

I describe these DataFrames as 'rudimentary' as they parse the top level of JSON into columns, which results in some DataFrame columns having complex data types such as `Dict()` (and within the `Dict()`, nested Dicts!). As a running theme in this post, this is something I hope to get around to improving in the future.

## Want to Get Started Developing Julia? Start Here!

One of the common questions I get asked is how to get started with Julia, both from a learning perspective and from a package development perspective. Hacking away on the core Julia codebase is great if you have the ability, but the code can certainly be intimidating (the people are quite friendly though). Creating a package isn't necessarily hard, but you have to think about an idea you want to implement. The third alternative is...

...improve the Twitter package! If you go to the [GitHub page for Twitter.jl](https://github.com/randyzwitch/Twitter.jl), you'll see a long list of TODO items that need to be worked on. The hardest part (building the OAuth headers) has already been taken care of. What's left is [re-factoring the code for simplification](http://randyzwitch.com/julia-metaprogramming-refactoring/), factoring out the [OAuth code in general into a new Julia library](https://github.com/randyzwitch/OAuth.jl) (also partially started), then building the Streaming API functions, cleaning up the DataFrame methods to remove the `Dict` column types, paging through API results...and so-on.

So if any of you are on the sidelines wanting to get some practice on developing packages, without needing to worry about learning Astrophysics first, I'd love to collaborate. And if any Julia programming masters want to collaborate, well that's great too. All help and pull requests are welcomed.

In the meantime, hopefully some of you will find this package useful for natural language processing, social networking analysis or even creating bots 😉</content>
      </item>
      
    
      
      <item>
        <title>RSiteCatalyst Version 1.4.2 Release Notes</title>
        
          <description>&lt;p&gt;RSiteCatalyst version 1.4.2 is now available on CRAN. This update was primarily bug fixes with one additional feature added.&lt;/p&gt;

</description>
        
        <pubDate>Wed, 03 Dec 2014 18:01:05 -0500</pubDate>
        <link>
        http://randyzwitch.com/rsitecatalyst-version-1-4-2-release-notes/</link>
        <guid isPermaLink="true">http://randyzwitch.com/rsitecatalyst-version-1-4-2-release-notes/</guid>
        <content type="html" xml:base="/rsitecatalyst-version-1-4-2-release-notes/">RSiteCatalyst version 1.4.2 is now available on CRAN. This update was primarily bug fixes with one additional feature added.

  1. Fixed QueueRanked function to allow multiple SAINT classifications to be specified. This allows for breaking down a SAINT classification with another SAINT classification, such as breaking down tracking codes by marketing channel and by campaign
  2. Fixed bug in internal function, to allow for using the same element multiple times in a QueueRanked function call. This was a necessary fix for allowing multiple SAINT classifications in #1
  3. Exported previous internal function `SubmitJsonQueueReport` to allow for submitting JSON requests directly to the Adobe Analytics API without all of the R function scaffolding. This approximates the same functionality as the [Adobe API Explorer](https://marketing.adobe.com/developer/get-started/api-explorer).

For the most part, this isn't a release that most people will notice any differences from version 1.4.1. That said, special thanks go out to Jason Morgan ([@framingeinstein](https://github.com/framingeinstein)) for identifying the two bugs that were fixed AND submitting fixes.

## Feature Requests/Bugs

As always, if you come across bugs or have feature requests, please continue to use the [RSiteCatalyst GitHub Issues](https://github.com/randyzwitch/RSiteCatalyst/issues) page to submit issues. Don’t worry about cluttering up the page with tickets, please fill out a new issue for anything you encounter (with code you've already tried and is failing), unless you are SURE that it is the same problem someone else is facing.

And finally, like I end every blog post about RSiteCatalyst, please note that **I’m** **not an Adobe employee**. Please don’t send me your API credentials, expect immediate replies (especially for you e-commerce folks sweating the holiday season!) or ask to set up phone calls to troubleshoot your problems. This is open-source software…Willem Paling and I did the hard part writing it, you’re expected to support yourself as best as possible unless you believe you’re encountering a bug. Then use GitHub.</content>
      </item>
      
    
      
      <item>
        <title>Destroy Your Data Using Excel With This One Weird Trick!</title>
        
          <description>&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2014/11/pie-charts-are-better.png&quot; alt=&quot;All you pie-chart haters are wishing I used one here&quot; /&gt;&lt;/p&gt;

</description>
        
        <pubDate>Thu, 20 Nov 2014 05:07:55 -0500</pubDate>
        <link>
        http://randyzwitch.com/excel-destroys-data/</link>
        <guid isPermaLink="true">http://randyzwitch.com/excel-destroys-data/</guid>
        <content type="html" xml:base="/excel-destroys-data/">![All you pie-chart haters are wishing I used one here](/wp-content/uploads/2014/11/pie-charts-are-better.png)

&lt;div&gt;
  &lt;p class=&quot;wp-caption-text&quot;&gt;
    All you pie-chart haters are wishing I used one here.
  &lt;/p&gt;
&lt;/div&gt;

I often use Twitter as a place to vent about the horribleness of Excel, from the product itself to analyses its UI and workflow influences. Admittedly, some of this is snobbish preference: if everyone used my preferred tools, then the world would be a better place! But let me back off my snobbishness a bit and just say this: please feel free to use any tool you want, up to and including pencil-and-paper...JUST.STOP.USING.EXCEL.

Excel arbitrarily destroys data for fun, as evidenced by the example below.

## Who Gives A 'F' About Seconds? I'm 10 minutes Late Everywhere!

CSV files have many flaws, but at least they are just plain text. It doesn't take any special software to read them and you can open and close them without loss of fidelity...except if you open them with Excel.

Suppose you have a CSV file with timestamps in ISO8601 format. Depending on which text editor you use, it might look something like this:

![timestamp](/wp-content/uploads/2014/11/timestamp.png)

Now, let's open our file in Excel:

![excel-dates](/wp-content/uploads/2014/11/excel-dates.png)

The first thing you might notice is that not only does Excel change the date formatting in the file to be more &quot;'Murica!&quot;, they don't even have the courtesy to use one of their existing date or time formats! And rather than keep the date the way it was, or standardize the dates to the way the rest of the world writes them, or even keep fixed-width columns, Excel feels like it should also hide the seconds! Makes sense...seconds are for other people to see, if/when they highlight an individual cell.

So, you've opened this file, but can't remember if you made any changes outside of applying auto-width to the columns. The data still _looks_ right, so you hit 'Save' when prompted by Excel. But you remember that your favorite programmer asked for a CSV file, and it's already a CSV file, so you hit save, ignore the 'features' Excel brags about and email it back to your co-worker. Here's what they receive:

![excel-fidelity-loss](/wp-content/uploads/2014/11/excel-fidelity-loss.png)

Reading this back in our plain-text editor, we can now see we have a loss of fidelity of between 37 and 47 seconds on each cell of data. Whereas Excel keeps track of your timestamps while you're in a SPREADSHEET, if you save as plain text, Excel assumes you want to keep the format it automatically applied to your data (automatically! silently!), and thus, destroys your file. In what world would you not care about seconds in your timestamps?

Remember, this mis-feature occurs even if the only thing you do is open a plain-text file in Excel and hit save. No other Excel actions are needed to destroy your data.

## Excel: Only The Proper Tool If You Don't Care

If you don't care about using the proper tool for analytics, don't want to learn something new, don't want numerical accuracy, hate visually interesting graphics, don't need reproducibility...use Excel. For everything else, there's everything else. Don't be a `VLOOKUP` guru, use SQL. Don't store your data in Excel just because it allows for a million rows, use a database. If you need point-and-click graphics, at least spring for Tableau so the defaults look nicer.

Or, learn to code using open-source languages for a total licensing cost of $0. Every analyst would get value from knowing one open-source analytics language, even topically, so that you can write simple calculation scripts and document your thought process. A side benefit is that by coding, you can also use version control like Git or SVN. Then, you can have different versions of thought, and the next analyst down the line can see how your analysis has evolved.

And while I'm ranting, a special message for all you 'top-tier' analytics consultants out there: you should know SEVERAL of the common analytics languages. If you do your &quot;analysis&quot; in Excel, you are a hack or you are just providing _reporting_ for $300/hr. Use better tools, your clients deserve better. I have infinitely more respect for someone who delivers a sloppy set of slides and a documented R script than someone who knows who to put drop-shadows on MS Office documents and makes fancy decks. You are being judged not just by the C-Suite, but also by snobs like me. And when contract renewal time comes around, they do ask my opinion and I do make comments on how sophisticated your toolset was that you used (or lack thereof if you're using Excel).

It's nearly 2015, do better. Stop Using Excel.</content>
      </item>
      
    
      
      <item>
        <title>Code Refactoring Using Metaprogramming</title>
        
          <description>&lt;p&gt;It’s been nearly a year since I wrote &lt;a href=&quot;https://github.com/randyzwitch/Twitter.jl/&quot;&gt;Twitter.jl&lt;/a&gt;, back when I seemingly had MUCH more free time. In these past 10 months, I’ve used Julia quite a bit to develop other packages, and I try to use it at work when I know I’m not going to be collaborating with others (since my colleagues don’t know Julia, not because it’s bad for collaboration!).&lt;/p&gt;

</description>
        
        <pubDate>Tue, 18 Nov 2014 04:11:06 -0500</pubDate>
        <link>
        http://randyzwitch.com/julia-metaprogramming-refactoring/</link>
        <guid isPermaLink="true">http://randyzwitch.com/julia-metaprogramming-refactoring/</guid>
        <content type="html" xml:base="/julia-metaprogramming-refactoring/">It's been nearly a year since I wrote [Twitter.jl](https://github.com/randyzwitch/Twitter.jl/), back when I seemingly had MUCH more free time. In these past 10 months, I've used Julia quite a bit to develop other packages, and I try to use it at work when I know I'm not going to be collaborating with others (since my colleagues don't know Julia, not because it's bad for collaboration!).

One of the things that's obvious from my earlier Julia code is that I didn't understand how powerful metaprogramming can be, so here's a simple example where I can replace 50 lines of Julia code with 10.

## CTRL-A, CTRL-C, CTRL-P. Repeat.

Admittedly, when I started on the Twitter package, I fully meant to go back and clean up the codebase, but moved onto something more fun instead. The Twitter package started out as a means of learning how to use the [Requests.jl](https://github.com/JuliaWeb/Requests.jl) library to make API calls, figure out the OAuth syntax I needed (which itself should be factored out of Twitter.jl), then copied-and-pasted the same basic function structure over and over. While fast, what I was left with was this (currently, the help.jl file in the Twitter package):

{% highlight julia linenos %}
#############################################################
#
# Help section Functions for Twitter API
#
#############################################################

function get_help_configuration(; options=Dict{String, String}())

    r = get_oauth(&quot;https://api.twitter.com/1.1/help/configuration.json&quot;, options)

    return r.status == 200 ? JSON.parse(r.data) : r

end

function get_help_languages(; options=Dict{String, String}())

    r = get_oauth(&quot;https://api.twitter.com/1.1/help/languages.json&quot;, options)

    return r.status == 200 ? JSON.parse(r.data) : r

end

function get_help_privacy(; options=Dict{String, String}())

    r = get_oauth(&quot;https://api.twitter.com/1.1/help/privacy.json&quot;, options)

    return r.status == 200 ? JSON.parse(r.data) : r

end

function get_help_tos(; options=Dict{String, String}())

    r = get_oauth(&quot;https://api.twitter.com/1.1/help/tos.json&quot;, options)

    return r.status == 200 ? JSON.parse(r.data) : r

end

function get_application_rate_limit_status(; options=Dict{String, String}())

    r = get_oauth(&quot;https://api.twitter.com/1.1/application/rate_limit_status.json&quot;, options)

    return r.status == 200 ? JSON.parse(r.data) : r

end
{% endhighlight %}

It's pretty clear that this is the same exact code pattern, right down to the spacing! The way to interpret this code is that for these five Twitter API methods, there are no required inputs. Optionally, there is the 'options' keyword that allows for specifying a `Dict()` of options. For these five functions, there are no options you can pass to the Twitter API, so even this keyword is redundant. These are simple functions so I don't gain a lot by way of maintainability by using metaprogramming, but at the same time, one of the core tenets of programming is 'Don't Repeat Yourself', so let's clean this up.

## For :symbol in symbolslist...

In order to clean this up, we need to take out the unique parts of the function, then pass them as arguments to the `@eval` macro as follows:

{% highlight julia linenos %}
funcname = (:get_help_configuration, :get_help_languages, :get_help_privacy, :get_help_tos, :get_application_rate_limit_status)
endpoint = (&quot;help/configuration.json&quot;, &quot;help/languages.json&quot;, &quot;help/privacy.json&quot;,  &quot;help/tos.json&quot;, &quot;application/rate_limit_status.json&quot;)

for (func, endp) in zip(funcname, endpoint)
	@eval function ($func)(; options=Dict{String, String}())

	        r = get_oauth($&quot;https://api.twitter.com/1.1/$endp&quot;, options)

	        return r.status == 200 ? JSON.parse(r.data) : r

    	end
end
{% endhighlight %}

What's happening in this code is that I define two tuples: one of function names (as symbols, denoted by `:`) and one of the API endpoints. We can then iterate over the two tuples, substituting the function names and endpoints into the code. When the package is loaded, this code evaluates, defining the five functions for use in the Twitter package.

## Wha?

Yeah, so metaprogramming can be simple, but it can also be mind-bending. It's one thing to not repeat yourself, it's another to write something so complex that even YOU can't remember how the code works. But somewhere in between lies a sweet spot where you can re-factor whole swaths of code and streamline your codebase. Metaprogramming is used throughout the Julia codebase, so if you're interested in seeing more examples of metaprogramming, check out the Julia source code, the [Requests.jl](https://github.com/JuliaWeb/Requests.jl/blob/master/src/Requests.jl &quot;Requests.jl code&quot;) package (where I first saw this) or really anyone who actually knows what they are doing. I'm just a metaprogramming pretender at this point 🙂  

To read additional discussion around this specific example, see the Julia-Users discussion at: &lt;https://groups.google.com/forum/#!topic/julia-users/zvJmqB2N0GQ&gt;

**Edit, 11/22/2014:** [DarthToaster on Reddit](http://www.reddit.com/r/Julia/comments/2mvtnr/code_refactoring_using_metaprogramming_in_julia/cma5g25) provided another fantastic way to approach refactoring, using macros:

{% highlight julia linenos %}
macro endpoint(name, path)
    quote
        function $(esc(name))(; options=Dict{String, String}())
            r = get_oauth($&quot;https://api.twitter.com/1.1/$path&quot;, options)
            return r.status == 200 ? JSON.parse(r.data) : r
        end
    end
end

@endpoint get_help_configuration &quot;help/configuration.json&quot;
@endpoint get_help_languages &quot;help/languages.json&quot;
{% endhighlight %}</content>
      </item>
      
    
      
      <item>
        <title>RSiteCatalyst Version 1.4.1 Release Notes</title>
        
          <description>&lt;h2 id=&quot;changes&quot;&gt;Changes&lt;/h2&gt;

</description>
        
        <pubDate>Mon, 10 Nov 2014 05:01:36 -0500</pubDate>
        <link>
        http://randyzwitch.com/rsitecatalyst-version-1-4-1-release-notes/</link>
        <guid isPermaLink="true">http://randyzwitch.com/rsitecatalyst-version-1-4-1-release-notes/</guid>
        <content type="html" xml:base="/rsitecatalyst-version-1-4-1-release-notes/">## Changes

Version 1.4.1 of RSiteCatalyst is now available on CRAN. There were a handful of bug fixes and new features added, including:

  * Fixed bug in `QueueRanked` function where only 10 results were returned when requesting multiple element reports. Function now returns up to 50,000 per breakdown (API limit)
  * Created better error message to inform user to login with credentials instead of making function call without proper API credentials
  * Added support for using SAINT classifications in `QueueRanked/QueueTrended` functions
  * Added more error checking to make functions fail more elegantly
  * Added remaining GET methods from Reporting/Administration API

## Additional GET methods

This version of RSiteCatalyst has roughly 20 new GET methods, mostly providing additional report suite information for those who might desire to generate their documentation programmatically rather than manually. New API methods include (but are not limited to):

  * `GetMarketingChannelRules`: Get a list of all criteria used to build the Marketing Channels report
  * `GetReportDescription`: For a given bookmark_id, get the report definition
  * `GetListVariables`: Get a list of the List Variables defined for a report suite
  * `GetLogins`: Get all logins for a given Company

If you were the type of person who enjoyed this blog post showing how to [auto-generate Adobe Analytics documentation](http://randyzwitch.com/adobe-analytics-implementation-documentation/ &quot;Adobe Analytics Report Suite documentation R&quot;), I encourage you to take a look at these newly incorporated functions and use them to improve your documentation even further.

## Feature Requests/Bugs

If you come across any bugs, or have any feature requests, please continue to use the [RSiteCatalyst GitHub Issues](https://github.com/randyzwitch/RSiteCatalyst/issues) page to make tickets. While I've responded to many of you via the maintainer email provided in the R package itself, it's much more efficient (and you're much more likely to get a response) if you use the GitHub Issues page. Don't worry about cluttering up the page with tickets, please fill out a new issue for anything you encounter, unless you are SURE that it is the same problem someone else is facing.

And finally, like I end every blog post about RSiteCatalyst, please note that **I'm** **not an Adobe employee**. Please don't send me your API credentials, expect immediate replies or ask to set up phone calls to troubleshoot your problems. This is open-source software...Willem Paling and I did the hard part writing it, you're expected to support yourself as best as possible unless you believe you're encountering a bug. Then use GitHub 🙂</content>
      </item>
      
    
      
      <item>
        <title>Declaring Twitter Bankruptcy</title>
        
          <description>&lt;blockquote class=&quot;twitter-tweet&quot; data-partner=&quot;tweetdeck&quot;&gt;
  &lt;p&gt;
    Is Twitter bankruptcy a thing? Delete all the people you follow and force yourself to re-discover them again?
  &lt;/p&gt;

&lt;/blockquote&gt;
</description>
        
        <pubDate>Fri, 07 Nov 2014 09:43:53 -0500</pubDate>
        <link>
        http://randyzwitch.com/declaring-twitter-bankruptcy/</link>
        <guid isPermaLink="true">http://randyzwitch.com/declaring-twitter-bankruptcy/</guid>
        <content type="html" xml:base="/declaring-twitter-bankruptcy/">&lt;blockquote class=&quot;twitter-tweet&quot; data-partner=&quot;tweetdeck&quot;&gt;
  &lt;p&gt;
    Is Twitter bankruptcy a thing? Delete all the people you follow and force yourself to re-discover them again?
  &lt;/p&gt;

  &lt;p&gt;
    — Randy Zwitch (@randyzwitch) &lt;a href=&quot;https://twitter.com/randyzwitch/status/530792792713621504&quot;&gt;November 7, 2014&lt;/a&gt;
  &lt;/p&gt;
&lt;/blockquote&gt;

Maybe I don't have enough to do today, or a long day of vendor calls has made me re-evaluate what I'm doing with my life, but I had a thought:

&lt;blockquote class=&quot;twitter-tweet&quot; data-partner=&quot;tweetdeck&quot;&gt;
  &lt;p&gt;
    I feel like in life, you go through many phases, and people come in and out of your life. But Twitter, you generally just accumulate. — Randy Zwitch (@randyzwitch) &lt;a href=&quot;https://twitter.com/randyzwitch/status/530793430444953600&quot;&gt;November 7, 2014&lt;/a&gt;
  &lt;/p&gt;
&lt;/blockquote&gt;

I started on Twitter in December 2009. Quite ironically from where I sit now, I think I joined Twitter because I was an Omniture/Adobe Analytics newcomer, and probably searched Google for some term I didn't understand. I eventually realized that people were talking about digital analytics on Twitter, so I created an account. Now, of course, I would imagine many would consider me an Adobe Analytics expert, at least in the case of the API and data feeds. And I now use Twitter way differently than I used to.

Since 2009, I've gone from banking and a beginner at digital analytics, to a working at an agency on helping them with Omniture, to being a digital analytics consultant at a specialty firm, to a start-up that didn't work out, to a gigantic media company. I also pretty much never think about 'web' analytics, except for the fact that I created and maintain a very specialized R package that maybe a few hundred people in the world use (if I'm lucky).

&lt;blockquote class=&quot;twitter-tweet&quot; data-conversation=&quot;none&quot; data-cards=&quot;hidden&quot; data-partner=&quot;tweetdeck&quot;&gt;
  &lt;p&gt;
    &lt;a href=&quot;https://twitter.com/chrisolenik&quot;&gt;@chrisolenik&lt;/a&gt; Would I follow the same people again if I started fresh. How did I accumulate the list that I did?
  &lt;/p&gt;

  &lt;p&gt;
    — Randy Zwitch (@randyzwitch) &lt;a href=&quot;https://twitter.com/randyzwitch/status/530794962271883264&quot;&gt;November 7, 2014&lt;/a&gt;
  &lt;/p&gt;
&lt;/blockquote&gt;

When we graduate from high school, then college, get married, the natural progression is that people come into your life and some fall out. But Twitter has a sort of hoarder quality to it. Some people cull their follower list, because they don't like what the person tweets about or they get in stupid Twitter feuds, but for the most part the list just builds and builds. Others stop using Twitter and you never hear from them again, but you still follow them (their silence?). But it occurs to me, this seems at least tangentially like the [Abilene Paradox](http://en.wikipedia.org/wiki/Abilene_paradox): at some point, you arrive at a place and you don't know how you got there. What have I done over the past five years that has lead me to this place where I'm reading about what I do on Twitter?

So I'm going to conduct an experiment. I'm unfollowing all of you without prejudice. Just as if I had a hard drive crash. And I'm going to re-follow all the people I can remember, then re-discover what I'm really interested in getting from Twitter as a platform by re-following friends of friends, people saying interesting things on hashtags, etc. Hope none of you are hurt by my unfollows, but then again, if you are then maybe that says something about what kind of relationship we currently have.</content>
      </item>
      
    
      
      <item>
        <title>Evaluating BreakoutDetection</title>
        
          <description>&lt;p&gt;A couple of weeks ago, Twitter open-sourced their &lt;a href=&quot;https://blog.twitter.com/2014/breakout-detection-in-the-wild&quot;&gt;BreakoutDetection&lt;/a&gt; package for R, a package designed to determine shifts in time-series data. The &lt;a href=&quot;https://blog.twitter.com/2014/breakout-detection-in-the-wild&quot;&gt;Twitter announcement&lt;/a&gt; does a great job of explaining the main technique for detection (E-Divisive with Medians), so I won’t rehash that material here. Rather, I wanted to see how this package works relative to the &lt;a href=&quot;http://randyzwitch.com/anomaly-detection-adobe-analytics-api/&quot;&gt;anomaly detection&lt;/a&gt; feature in the Adobe Analytics API, which I’ve &lt;a href=&quot;http://randyzwitch.com/anomaly-detection-adobe-analytics-api/&quot;&gt;written about previously&lt;/a&gt;.&lt;/p&gt;

</description>
        
        <pubDate>Thu, 06 Nov 2014 16:24:00 -0500</pubDate>
        <link>
        http://randyzwitch.com/twitter-breakoutdetection-r-package-evaluation/</link>
        <guid isPermaLink="true">http://randyzwitch.com/twitter-breakoutdetection-r-package-evaluation/</guid>
        <content type="html" xml:base="/twitter-breakoutdetection-r-package-evaluation/">A couple of weeks ago, Twitter open-sourced their [BreakoutDetection](https://blog.twitter.com/2014/breakout-detection-in-the-wild) package for R, a package designed to determine shifts in time-series data. The [Twitter announcement](https://blog.twitter.com/2014/breakout-detection-in-the-wild) does a great job of explaining the main technique for detection (E-Divisive with Medians), so I won't rehash that material here. Rather, I wanted to see how this package works relative to the [anomaly detection](http://randyzwitch.com/anomaly-detection-adobe-analytics-api/) feature in the Adobe Analytics API, which I've [written about previously](http://randyzwitch.com/anomaly-detection-adobe-analytics-api/).

## Getting Time-Series Data Using RSiteCatalyst

To use a real-world dataset to evaluate this package, I'm going to use roughly ten months of daily pageviews generated from my blog. The hypothesis here is that if the BreakoutDetection package works well, it should be able to detect the boundaries around when I publish a blog post (of which the dates I know with certainty) and when articles of mine get shared on sites such as Reddit. From past experience, I get about a 3-day lift in pageviews post-publishing, as the article gets tweeted out, published on [R-Bloggers](http://www.r-bloggers.com/) or [JuliaBloggers](http://www.juliabloggers.com/) and shared accordingly.

Here's the code to get daily pageviews using [RSiteCatalyst](http://cran.r-project.org/web/packages/RSiteCatalyst/index.html &quot;RSiteCatalyst&quot;) (Adobe Analytics):

{% highlight R linenos %}
#Installing BreakoutDetection package
install.packages(&quot;devtools&quot;)
devtools::install_github(&quot;twitter/BreakoutDetection&quot;)
library(BreakoutDetection)

library(&quot;RSiteCatalyst&quot;)
SCAuth(&quot;company&quot;, &quot;secret&quot;)

#Get pageviews for each day in 2014
pageviews_2014 &lt;- QueueOvertime('report-suite',
                               date.from = '2014-02-24',
                               date.to = '2014-11-05',
                               metric = 'pageviews',
                               date.granularity = 'day')

#v1.0.1 of package requires specific column names and dataframe format
formatted_df &lt;- pageviews_2014[,c(&quot;datetime&quot;,&quot;pageviews&quot;)]
names(formatted_df) &lt;- c(&quot;timestamp&quot;, &quot;count&quot;)
{% endhighlight %}

One thing to notice here is that BreakoutDetection requires either a single R vector or a specifically formatted data frame. In this case, because I have a timestamp, I use lines 17-18 to get the data into the required format.

## BreakoutDetection - Default Example

In the Twitter announcement, they provide an example, so let's evaluate those defaults first:

![breakoutdetection-defaults](/wp-content/uploads/2014/11/breakoutdetection-defaults.png)

In order to validate my hypothesis, the package would need to detect 12 'breakouts' or so, as I've published 12 blog posts during the sample time period. Mentally drawing lines between the red boundaries, we can see three definitive upward mean shifts, but far fewer than the 12 I expected.

## BreakoutDetection - Modifying The Parameters

Given that the chart above doesn't fit how I think my data are generated, we can modify two main parameters: beta and min.size. From the documentation:

 &gt; beta: A real numbered constant used to further control the amount of penalization. This is the default form of penalization, if neither (or both) beta or (and) percent are supplied this argument will be used. The default value is beta=0.008.
 &gt;
 &gt; min.size:  The minimum number of observations between change points

The first parameter I'm going to experiment with is min.size, because it requires no in-depth knowledge of the EDM technique! The value used in the first example was 24 (days) between intervals, which seems extreme in my case. It's reasonable that I might publish a blog post per week, so let's back that number down to 5 and see how the result changes:

![breakout-5](/wp-content/uploads/2014/11/breakout-5.png)

With 17 predicted intervals, we've somewhat overshot the number of blog posts mark. Not that the package is wrong per se; the boundaries are surrounding many of the spikes in the data, but perhaps having this many breakpoints isn't useful from a monitoring standpoint. So setting the min.size parameter somewhere between 5 and 24 points would give us more than 3 breakouts, but less than 17. There is also the `beta` parameter that can be played with, but I'll leave that as an exercise for another day.

## Anomaly Detection - Adobe Analytics

From my prior post about [Anomaly Detection with the Adobe Analytics API](http://randyzwitch.com/anomaly-detection-adobe-analytics-api/), Adobe has chosen to use Holt-Winters/Exponential Smoothing as their technique. Here's what that looks like for the same time-period (code as [GitHub Gist](http://randyzwitch.com/wp-content/uploads/2014/11/adobe_anomaly.png)):

![adobe_analytics](/wp-content/uploads/2014/11/adobe_analytics.png)

Even though the idea of both techniques are similar, it's clear that the two methods don't quite represent the same thing. In the case of the Adobe Analytics Anomaly Detection, it's looking datapoint-by-datapoint, with a smoothing model built from the prior 35 points. If a point exceeds the `upper-` or `lower-control` limits, then it's an anomaly, but not necessarily indicative of a true level shift like the BreakoutDetection package is measuring.

## Conclusion

The [BreakoutDetection package](https://github.com/twitter/BreakoutDetection) is definitely cool, but it is a bit raw, especially the default graphics. But the package definitely does work, as evidenced by how well it put boundaries around the traffic spikes when I set the `min.size` parameter equal to five.

Additionally, I tried to read more about the underlying methodology, but the only references that come up in Google seem to be references to the R package itself! I wish I had a better feeling for how the beta parameter influences the graph, but I guess that will come over time as I use the package more. But I'm definitely glad that Twitter open-sourced this package, as I've often wondered about how to detect level shifts in a more operational setting, and now I have a method to do so.</content>
      </item>
      
    
      
      <item>
        <title>Visualizing Website Pathing With Sankey Charts</title>
        
          <description>&lt;p&gt;In my prior post on &lt;a href=&quot;http://randyzwitch.com/rsitecatalyst-d3-network-graphs/&quot; title=&quot;Visualizing Website Structure With Network Graphs&quot;&gt;visualizing website structure using network graphs&lt;/a&gt;, I referenced that network graphs showed the pairwise relationships between two pages (in a bi-directional manner). However, if you want to analyze how your visitors are pathing through your site, you can visualize your data using a Sankey chart.&lt;/p&gt;

</description>
        
        <pubDate>Wed, 10 Sep 2014 17:27:10 -0400</pubDate>
        <link>
        http://randyzwitch.com/rsitecatalyst-website-pathing-sankey-charts/</link>
        <guid isPermaLink="true">http://randyzwitch.com/rsitecatalyst-website-pathing-sankey-charts/</guid>
        <content type="html" xml:base="/rsitecatalyst-website-pathing-sankey-charts/">In my prior post on [visualizing website structure using network graphs](http://randyzwitch.com/rsitecatalyst-d3-network-graphs/ &quot;Visualizing Website Structure With Network Graphs&quot;), I referenced that network graphs showed the pairwise relationships between two pages (in a bi-directional manner). However, if you want to analyze how your visitors are pathing through your site, you can visualize your data using a Sankey chart.

## Visualizing Single Page-to-Next Page Pathing

Most digital analytics tools allow you to visualize the path between pages. In the case of Adobe Analytics, the Next Page Flow diagram is limited to 10 second-level branches in the visualization. However, the Adobe Analytics API has no such limitation, and as such we can use RSiteCatalyst to create the following visualization ([GitHub Gist containing R code](https://gist.github.com/randyzwitch/008be202b94bde7c4359)):

&lt;iframe src=&quot;http://randyzwitch.com/wp-content/uploads/2014/09/sankey.html&quot; width=&quot;750&quot; height=&quot;650&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;

The data processing for this visualization is near identical to the network diagrams. We can use `QueuePathing()` from RSiteCatalyst to download our pathing data, except in this case, I specified an exact page name as the first level of the pathing pattern instead of using the `::anything::` operator. In all Sankey charts created by `d3Network`, you can hover over the right-hand side nodes to see the values (you can also drag around the nodes on either side if you desire!). It's pretty clear from this diagram that I need to do a better job retaining my visitors, as the most common path from this page is to leave. 🙁

## Many-to-Many Page Pathing

The example above picks a single page related to Hadoop, then shows how my visitors continue through my site; sometimes, they go to other Hadoop pages, some view &lt;a title=&quot;Data Science content&quot; href=&quot;http://randyzwitch.com/category/data-science/&quot; target=&quot;_blank&quot;&gt;Data Science related content&lt;/a&gt; or any number of other paths. If we want, however, we can visualize how all visitors path through all pages. Like the force-directed graph, we can get this information by using the `(&quot;::anything::&quot;, &quot;::anything::&quot;)` path pattern with `QueuePathing()`:

{% highlight R linenos %}
#Multi-page pathing
library(&quot;d3Network&quot;)
library(&quot;RSiteCatalyst&quot;)

#### Authentication
SCAuth(&quot;name&quot;, &quot;secret&quot;)

#### Get All Possible Paths with (&quot;::anything::&quot;, &quot;::anything::&quot;)
pathpattern &lt;- c(&quot;::anything::&quot;, &quot;::anything::&quot;)
next_page &lt;- QueuePathing(&quot;zwitchdev&quot;,
                          &quot;2014-01-01&quot;,
                          &quot;2014-08-31&quot;,
                          metric=&quot;pageviews&quot;,
                          element=&quot;page&quot;,
                          pathpattern,
                          top = 50000)

#Optional step: Cleaning my pagename URLs to remove to domain for clarity
next_page$step.1 &lt;- sub(&quot;http://randyzwitch.com/&quot;,&quot;&quot;,
                        next_page$step.1, ignore.case = TRUE)
next_page$step.2 &lt;- sub(&quot;http://randyzwitch.com/&quot;,&quot;&quot;,
                        next_page$step.2, ignore.case = TRUE)

#Filter out Entered Site and duplicate rows, &gt;120 for chart legibility
links &lt;- subset(next_page, count &gt;= 120 &amp; step.1 != &quot;Entered Site&quot;)

#Get unique values of page name to create nodes df
#Create an index value, starting at 0
nodes &lt;- as.data.frame(unique(c(links$step.1, links$step.2)))
names(nodes) &lt;- &quot;name&quot;
nodes$nodevalue &lt;- as.numeric(row.names(nodes)) - 1

#Convert string to numeric nodeid
links &lt;- merge(links, nodes, by.x=&quot;step.1&quot;, by.y=&quot;name&quot;)
names(links) &lt;- c(&quot;step.1&quot;, &quot;step.2&quot;, &quot;value&quot;, &quot;segment.id&quot;, &quot;segment.name&quot;, &quot;source&quot;)

links &lt;- merge(links, nodes, by.x=&quot;step.2&quot;, by.y=&quot;name&quot;)
names(links) &lt;- c(&quot;step.2&quot;, &quot;step.1&quot;, &quot;value&quot;, &quot;segment.id&quot;, &quot;segment.name&quot;,&quot;source&quot;, &quot;target&quot;)

#Create next page Sankey chart
d3output = &quot;~/Desktop/sankey_all.html&quot;
d3Sankey(Links = links, Nodes = nodes, Source = &quot;source&quot;,
         Target = &quot;target&quot;, Value = &quot;value&quot;, NodeID = &quot;name&quot;,
         fontsize = 12, nodeWidth = 50, file = d3output, width = 750, height = 700)
{% endhighlight %}

Running the code above provides the following visualization:

&lt;iframe src=&quot;http://randyzwitch.com/wp-content/uploads/2014/09/sankey_all1.html&quot; width=&quot;750&quot; height=&quot;700&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;

For legibility purposes, I'm only plotting paths that occur more than 120 times. But given a large enough display, it would be possible to visualize all valid combinations of paths.

One thing to keep in mind is that with the `d3.js` library, there is a weird hiccup where if your dataset contains &quot;duplicate&quot; paths such that both `Source -&gt; Target &amp; Target -&gt; Source` exists, `d3.js` will go into an infinite loop/not show any visualization. My R code doesn't provide a solution to this issue, but it should be trivial to remove these &quot;duplicates&quot; should they arise in your dataset.

## Interpretation

Unlike the network graphs, Sankey Charts are fairly easy to understand. The &quot;worst&quot; path on my site in terms of keeping visitors on site is where I praised Apple for [fixing my MacBook Pro screen](http://randyzwitch.com/broken-macbook-pro-hinge-fixed-free/) out-of-warranty. The easy explanation for this poor performance is that this article attracts people who aren't really my target audience in data science, but looking for information about getting THEIR screens fixed. If I wanted to engage these readers more, I guess I would need to write more Apple-related content.

To the extent there are multi-stage paths, these tend to be [Hadoop](http://randyzwitch.com/tag/hadoop/) and [Julia](http://randyzwitch.com/tag/julia/)-related content. This makes sense as both technologies are fairly new, I have a lot more content in these areas, and especially in the case of Julia, I'm one of the few people writing practical content. So I'm glad to see I'm achieving some level of success in these areas.

Hopefully this blog post and my previous post on [visualizing your website visitors using network graphs](http://randyzwitch.com/rsitecatalyst-d3-network-graphs/) have given a feel for the [new functionality available in RSiteCatalyst v1.4](http://randyzwitch.com/rsitecatalyst-version-1-4-release-notes/), as well providing a new way of thinking about data visualization beyond just the default graphs provided by the Adobe Analytics interface.</content>
      </item>
      
    
      
      <item>
        <title>Creating A Stacked Bar Chart in Seaborn</title>
        
          <description>&lt;p&gt;The other day I was having a heck of a time trying to figure out how to make a stacked bar chart in Seaborn. But in true open-source/community fashion, I ended up getting a response from the creator of Seaborn via Twitter:&lt;/p&gt;

</description>
        
        <pubDate>Tue, 09 Sep 2014 04:01:39 -0400</pubDate>
        <link>
        http://randyzwitch.com/creating-stacked-bar-chart-seaborn/</link>
        <guid isPermaLink="true">http://randyzwitch.com/creating-stacked-bar-chart-seaborn/</guid>
        <content type="html" xml:base="/creating-stacked-bar-chart-seaborn/">The other day I was having a heck of a time trying to figure out how to make a stacked bar chart in Seaborn. But in true open-source/community fashion, I ended up getting a response from the creator of Seaborn via Twitter:

&lt;blockquote class=&quot;twitter-tweet&quot; lang=&quot;en&quot; data-conversation=&quot;none&quot;&gt;
  &lt;p&gt;
    &lt;a href=&quot;https://twitter.com/randyzwitch&quot;&gt;@randyzwitch&lt;/a&gt; I don't really like stacked bar charts, I'd suggest maybe using pointplot / factorplot with kind=point
  &lt;/p&gt;

  &lt;p&gt;
    — Michael Waskom (@michaelwaskom) &lt;a href=&quot;https://twitter.com/michaelwaskom/status/507608729840152578&quot;&gt;September 4, 2014&lt;/a&gt;
  &lt;/p&gt;
&lt;/blockquote&gt;

So there you go. I don't want to put words in Michael's mouth, but if he's not a fan, then it sounded like it was up to me to find my own solution if I wanted a stacked bar chart. I hacked around on the [pandas plotting functionality](http://pandas.pydata.org/pandas-docs/stable/visualization.html) a while, went to the [matplotlib documentation/example for a stacked bar chart](http://matplotlib.org/1.3.1/examples/pylab_examples/bar_stacked.html), tried Seaborn some more and then it hit me...I've gotten so used to these amazing open-source packages that my brain has atrophied! Creating a stacked bar chart is SIMPLE, even in Seaborn (and even if Michael doesn't like them 🙂 )

## Stacked Bar Chart = Sum of Two Series

In trying so hard to create a stacked bar chart, I neglected the most obvious part. Given two series of data, `Series 1` (&quot;bottom&quot;) and `Series 2` (&quot;top&quot;), to create a stacked bar chart you just need to create:

{% highlight python linenos %}
Series 3 = Series 1 + Series 2
{% endhighlight %}

Once you have `Series 3` (&quot;total&quot;), then you can use the overlay feature of matplotlib and Seaborn in order to create your stacked bar chart. Plot &quot;total&quot; first, which will become the base layer of the chart. Because the total by definition will be greater-than-or-equal-to the &quot;bottom&quot; series, once you overlay the &quot;bottom&quot; series on top of the &quot;total&quot; series, the &quot;top&quot; series will now be stacked on top:

#### Background: &quot;Total&quot; Series

![background_total](/wp-content/uploads/2014/09/background_total.png)

#### Overlay: &quot;Bottom&quot; Series

![bottom_plot](/wp-content/uploads/2014/09/bottom_plot1.png)

## End Result: Stacked Bar Chart

Running the code in the same IPython Notebook cell results in the following chart ([download chart data](http://randyzwitch.com/wp-content/uploads/2014/09/stacked_bar.csv)):

![stacked-bar-seaborn](/wp-content/uploads/2014/09/stacked-bar-seaborn.png)

{% highlight python linenos %}
import pandas as pd
from matplotlib import pyplot as plt
import matplotlib as mpl
import seaborn as sns
%matplotlib inline

#Read in data &amp; create total column
stacked_bar_data = pd.read_csv(&quot;C:\stacked_bar.csv&quot;)
stacked_bar_data[&quot;total&quot;] = stacked_bar_data.Series1 + stacked_bar_data.Series2

#Set general plot properties
sns.set_style(&quot;white&quot;)
sns.set_context({&quot;figure.figsize&quot;: (24, 10)})

#Plot 1 - background - &quot;total&quot; (top) series
sns.barplot(x = stacked_bar_data.Group, y = stacked_bar_data.total, color = &quot;red&quot;)

#Plot 2 - overlay - &quot;bottom&quot; series
bottom_plot = sns.barplot(x = stacked_bar_data.Group, y = stacked_bar_data.Series1, color = &quot;#0000A3&quot;)


topbar = plt.Rectangle((0,0),1,1,fc=&quot;red&quot;, edgecolor = 'none')
bottombar = plt.Rectangle((0,0),1,1,fc='#0000A3',  edgecolor = 'none')
l = plt.legend([bottombar, topbar], ['Bottom Bar', 'Top Bar'], loc=1, ncol = 2, prop={'size':16})
l.draw_frame(False)

#Optional code - Make plot look nicer
sns.despine(left=True)
bottom_plot.set_ylabel(&quot;Y-axis label&quot;)
bottom_plot.set_xlabel(&quot;X-axis label&quot;)

#Set fonts to consistent 16pt size
for item in ([bottom_plot.xaxis.label, bottom_plot.yaxis.label] +
             bottom_plot.get_xticklabels() + bottom_plot.get_yticklabels()):
    item.set_fontsize(16)
{% endhighlight %}

## Don't Overthink Things!

In the end, creating a stacked bar chart in Seaborn took me 4 hours to mess around trying everything under the sun, then 15 minutes once I remembered what a stacked bar chart actually represents. Hopefully this will save someone else from my same misery.</content>
      </item>
      
    
      
      <item>
        <title>Visualizing Website Structure With Network Graphs</title>
        
          <description>&lt;p&gt;Last week, &lt;a href=&quot;http://randyzwitch.com/rsitecatalyst-version-1-4-release-notes/&quot;&gt;version 1.4 of RSiteCatalyst&lt;/a&gt; was released, and now it’s possible to get site pathing information directly within R. Now, it’s easy to create impressive looking network graphs from your Adobe Analytics data using &lt;a href=&quot;http://cran.r-project.org/web/packages/RSiteCatalyst/index.html&quot;&gt;RSiteCatalyst&lt;/a&gt; and &lt;a href=&quot;http://cran.r-project.org/web/packages/d3Network/index.html&quot;&gt;d3Network&lt;/a&gt;. In this blog post, I will cover simple and force-directed network graphs, which show the pairwise representation between pages. In a follow-up blog post, I will show how to visualize longer paths using &lt;a href=&quot;http://www.sankey-diagrams.com/&quot;&gt;Sankey diagrams&lt;/a&gt;, also from the d3Network package.&lt;/p&gt;

</description>
        
        <pubDate>Mon, 08 Sep 2014 02:40:38 -0400</pubDate>
        <link>
        http://randyzwitch.com/rsitecatalyst-d3-network-graphs/</link>
        <guid isPermaLink="true">http://randyzwitch.com/rsitecatalyst-d3-network-graphs/</guid>
        <content type="html" xml:base="/rsitecatalyst-d3-network-graphs/">Last week, [version 1.4 of RSiteCatalyst](http://randyzwitch.com/rsitecatalyst-version-1-4-release-notes/) was released, and now it's possible to get site pathing information directly within R. Now, it's easy to create impressive looking network graphs from your Adobe Analytics data using [RSiteCatalyst](http://cran.r-project.org/web/packages/RSiteCatalyst/index.html) and [d3Network](http://cran.r-project.org/web/packages/d3Network/index.html). In this blog post, I will cover simple and force-directed network graphs, which show the pairwise representation between pages. In a follow-up blog post, I will show how to visualize longer paths using [Sankey diagrams](http://www.sankey-diagrams.com/), also from the d3Network package.

## Obtaining Pathing Data With QueuePathing

Although the `QueuePathing()` function is new to RSiteCatalyst, its syntax should feel familiar (even with all of the breaking changes we made!). In the case of creating our network graphs, we want to download all pairwise combinations of pages, which is easy to do using the `::anything::` operator:

{% highlight R linenos %}
library(&quot;RSiteCatalyst&quot;)
library(&quot;d3Network&quot;)

#### Authentication
SCAuth(&quot;username&quot;, &quot;secret&quot;)

#### Get Pathing data using ::anything:: wildcards
# Results are limited by the API to 50000
pathpattern &lt;- c(&quot;::anything::&quot;, &quot;::anything::&quot;)

queue_pathing_pages &lt;- QueuePathing(&quot;zwitchdev&quot;,
                                    &quot;2014-01-01&quot;,
                                    &quot;2014-08-31&quot;,
                                    metric=&quot;pageviews&quot;,
                                    element=&quot;page&quot;,
                                    pathpattern,
                                    top = 50000)
{% endhighlight %}

Because we are using a pathing pattern of `(&quot;::anything::&quot;, &quot;::anything::&quot;)`, the data frame that is returned from this function will have three columns: `step.1`, `step.2` and `count`, which is the number of occurrences of the path.

## Plotting Graph Using d3SimpleNetwork

Before jumping into the plotting, we need to do some quick data cleaning. Lines 1-5 below are optional; I don't set the Adobe Analytics s.pageName on each of my blog pages (a worst practice if there ever was one!), so I use the `sub()` function in Base R to strip the domain name from the beginning of the page. The other data frame modification is to remove the `'Entered Site'` and `'Exited Site'` from the pagename pairs. Although this is important information generally, these behaviors aren't needed to show the pairwise relationship between pages.

{% highlight R linenos %}
#Optional step: Cleaning my pagename URLs to remove to domain for graph clarity
queue_pathing_pages$step.1 &lt;- sub(&quot;http://randyzwitch.com/&quot;,&quot;&quot;,
                                  queue_pathing_pages$step.1, ignore.case = TRUE)
queue_pathing_pages$step.2 &lt;- sub(&quot;http://randyzwitch.com/&quot;,&quot;&quot;,
                                  queue_pathing_pages$step.2, ignore.case = TRUE)

#### Remove Enter and Exit site values
#This information is important for analysis, but not related to website structure
graph_links &lt;- subset(queue_pathing_pages, step.1 != &quot;Entered Site&quot; &amp; step.2 != &quot;Exited Site&quot;)

#### First pass - Simple Network
# Setting standAlone = TRUE creates a full HTML file to view graph
# Set equal to FALSE to just get the d3 JavaScript
simpleoutput1 = &quot;C:/Users/rzwitc200/Desktop/simpleoutput1.html&quot;
d3SimpleNetwork(graph_links, Source = &quot;step.1&quot;, Target = &quot;step.2&quot;, height = 600,
                width = 750, fontsize = 12, linkDistance = 50, charge = -50,
                linkColour = &quot;#666&quot;, nodeColour = &quot;#3182bd&quot;,
                nodeClickColour = &quot;#E34A33&quot;, textColour = &quot;#3182bd&quot;, opacity = 0.6,
                standAlone = TRUE, file = simpleoutput1)
{% endhighlight %}

Running the above code results in the following graph:

&lt;iframe src=&quot;http://randyzwitch.com/wp-content/uploads/2014/09/simpleoutput1.html&quot; width=&quot;750&quot; height=&quot;500&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;

Hmmm...looks like a blob of spaghetti, a common occurrence when creating graphs. We can do better.

## Pruning Edges From The Graph

There are many &lt;a title=&quot;Pruning Edges from Network&quot; href=&quot;http://link.springer.com/chapter/10.1007%2F978-3-642-31830-6_13&quot; target=&quot;_blank&quot;&gt;complex algorithms for determining how to prune edges/nodes from a network&lt;/a&gt;. For the sake of simplicity, I'm going to use a very simple algorithm: each path has to occur more than 5 times for it to be included in the network. This will prune roughly 80% of the pairwise page combinations while keeping ~75% of the occurrences. This is simple to do using the `subset()` function in R:

{% highlight R linenos %}
#### Second pass: thin the spaghetti blob!
#Require path to happen more than some number of times (count &gt; x)
#What constitutes &quot;low volume&quot; will depend on your level of traffic
simpleoutput2 = &quot;C:/Users/rzwitc200/Desktop/simpleoutput2.html&quot;
d3SimpleNetwork(subset(graph_links, count &gt; 5), Source = &quot;step.1&quot;, Target = &quot;step.2&quot;, height = 600,
                width = 750, fontsize = 12, linkDistance = 50, charge = -100,
                linkColour = &quot;#666&quot;, nodeColour = &quot;#3182bd&quot;,
                nodeClickColour = &quot;#E34A33&quot;, textColour = &quot;#3182bd&quot;, opacity = 0.6,
                standAlone = TRUE, file = simpleoutput2)
{% endhighlight %}

The result of pruning the number of edges is a much less cluttered graph:

 &lt;iframe src=&quot;http://randyzwitch.com/wp-content/uploads/2014/09/simpleoutput2.html&quot; width=&quot;750&quot; height=&quot;500&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;

Even with fewer edges in the graph, we still lose some of the information about the pages, since we don't know what topics/groups the pages represent. We can fix that using a slightly more complex version of the d3Network graph code.

## Force-directed graphs

The graphs above outline the structure of randyzwitch.com, but they can be improved by adding color-coding to the nodes to represent the topic of the post, as well as making the edges thicker/thinner based on how frequently the path occurs. This can be done using the `d3ForceNetwork()` function like so:

{% highlight R linenos %}
#### Force directed network

#Limit to more than 5 occurence like in simple network
fd_graph_links &lt;- subset(graph_links, count &gt; 5)

#Get unique values of page name to create nodes df
#Create an index value, starting at 0
fd_nodes &lt;- as.data.frame(unique(c(fd_graph_links$step.1, fd_graph_links$step.2)))
names(fd_nodes) &lt;- &quot;name&quot;
fd_nodes$nodevalue &lt;- as.numeric(row.names(fd_nodes)) - 1

#Create groupings for node colors
#This is user-specific in terms of how to create these groupings
#Due to few number of pages/topics, I am manually coding this

grouping &lt;- function(string){

  if(grepl(&quot;(hadoop|hive|pig)&quot;,string, perl=TRUE)){
    return(1)
  }else if(grepl(&quot;(julia|uaparser-jl)&quot;,string, , perl=TRUE)){
    return(2)
  }else if(grepl((&quot;[r]?sitecatalyst|adobe-analytics|omniture&quot;),string, perl=TRUE)){
    return(3)
  }else if(grepl(&quot;(wordpress|twenty-eleven|scrappy)&quot;,string, perl=TRUE)){
    return(4)
  }else if(grepl(&quot;data-science|ec2&quot;,string, perl=TRUE)){
    return(5)
  }else if(grepl(&quot;python&quot;,string, perl=TRUE)){
    return(6)  
  }else if(grepl(&quot;(digital-analytics|google-analytics|web-analyst)&quot;,string, perl=TRUE)){
    return(8)
  }else if(grepl(&quot;(macbook|iphone)&quot;,string, perl=TRUE)){
    return(9)
  }else if(grepl(&quot;(randyzwitch|about|page)&quot;,string, perl=TRUE)){
    return(10)
  }else if(grepl(&quot;(rstudio|rcmdr|r-language|jsonlite|r-language-oddities|tag/r|automated-re-install-of-packages-for-r-3-0|learning-r-sas|creating-dummy-variables-data-frame-r)&quot;,string, perl=TRUE)){
    return(7)
  }else{
    return(11)
  }

}

#Create group column
fd_nodes$group &lt;- sapply(fd_nodes$name, grouping)

#Append numeric nodeid to pagename
fd_graph_links &lt;- merge(fd_graph_links, fd_nodes[,1:2], by.x=&quot;step.1&quot;, by.y=&quot;name&quot;)
names(fd_graph_links) &lt;- c(&quot;step.1&quot;, &quot;step.2&quot;, &quot;value&quot;, &quot;source&quot;)

fd_graph_links &lt;- merge(fd_graph_links, fd_nodes[,1:2], by.x=&quot;step.2&quot;, by.y=&quot;name&quot;)
names(fd_graph_links) &lt;- c(&quot;step.1&quot;, &quot;step.2&quot;, &quot;value&quot;, &quot;source&quot;, &quot;target&quot;)

d3output = &quot;C:/Users/rzwitc200/Desktop/fd_graph.html&quot;
# Create force-directed graph
d3ForceNetwork(Links = fd_graph_links, Nodes = fd_nodes, Source = &quot;source&quot;,
               Target = &quot;target&quot;, NodeID = &quot;name&quot;,
               Group = &quot;group&quot;, opacity = 0.8, Value = &quot;value&quot;,
               file = d3output,
               charge = -90,
               fontsize=12)
{% endhighlight %}

Running the code results in the following force-directed graph:

&lt;iframe src=&quot;http://randyzwitch.com/wp-content/uploads/2014/09/fd_graph.html&quot; width=&quot;750&quot; height=&quot;500&quot; frameborder=&quot;0&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;

## Interpretation

I'm not going to lie, all three of these diagrams are hard to interpret. Like wordclouds, network graphs can often be visually interesting, yet difficult to ascertain any concrete information. Network graphs also have the tendency to reinforce what you already know (you or someone you know designed your website, you should already have a feel for its structure!).

However, in the case of the force-directed graph above, I do see some interesting patterns. Specifically, there are a considerable number of nodes that aren't attached to the main network structure. This may be occurring due to my method of pruning the network edges. More likely is that these disconnected nodes represent &quot;dead-ends&quot; in my blog, either because few pages link to them, there are technical errors, these are high bounce-rate pages or represent one-off topics that satiate the reader.

In terms of action I can take, I can certainly look up the bounce rate for these disconnected pages/nodes and re-write the content to make it more 'sticky'. There's also the case of the way my &quot;Related Posts&quot; plugin determines related pages. As far as I know, it's quite naive, using the existing words on the page to determine relationships between posts. So one follow-up could be to create an actual recommender system to better suggest content to my readers. Perhaps that's a topic for a different blog post.

Regardless of the actions I'll end up taking from this information, hopefully this blog post has piqued some ideas of how to use RSiteCatalyst in a non-standard way, to extend the standard digital analytics information you are capturing with Adobe Analytics into creating interesting visualizations and potential new insights.

#### Example Data

_For those of you who aren't Adobe Analytics customers (or are, but don't have API access), here are the [data from the `queue_pathing_pages` data frame](/wp-content/uploads/2014/09/queue_pathing_pages.csv) above. Just read this data into R, then you should be able to follow along with the `d3Network` code._</content>
      </item>
      
    
      
      <item>
        <title>RSiteCatalyst Version 1.4 Release Notes</title>
        
          <description>&lt;p&gt;It felt like it would never happen, but &lt;a href=&quot;http://cran.r-project.org/web/packages/RSiteCatalyst/index.html&quot;&gt;RSiteCatalyst v1.4&lt;/a&gt; is now available on CRAN! There are numerous changes in this version of the package, so unlike previous posts, there won’t be any code examples.&lt;/p&gt;

</description>
        
        <pubDate>Mon, 01 Sep 2014 16:30:14 -0400</pubDate>
        <link>
        http://randyzwitch.com/rsitecatalyst-version-1-4-release-notes/</link>
        <guid isPermaLink="true">http://randyzwitch.com/rsitecatalyst-version-1-4-release-notes/</guid>
        <content type="html" xml:base="/rsitecatalyst-version-1-4-release-notes/">It felt like it would never happen, but [RSiteCatalyst v1.4](http://cran.r-project.org/web/packages/RSiteCatalyst/index.html) is now available on CRAN! There are numerous changes in this version of the package, so unlike previous posts, there won't be any code examples.

## THIS VERSION IS ONE BIG BREAKING CHANGE

While not the most important _improvement_, it can't be stressed enough that migrating to v1.4 of RSiteCatalyst is likely going to require re-writing some of your prior code. There are numerous reasons for the breaking changes, including:

  1. Adobe made breaking changes to the API between v1.3 and v1.4, so we had to as well
  2. I partnered with &lt;a title=&quot;Willem Paling GitHub&quot; href=&quot;https://github.com/WillemPaling&quot; target=&quot;_blank&quot;&gt;Willem Paling&lt;/a&gt;, who merged his &lt;a title=&quot;RAA - Original Source for RSiteCatalyst 1.4&quot; href=&quot;https://github.com/WillemPaling/RAA&quot; target=&quot;_blank&quot;&gt;RAA&lt;/a&gt; codebase into RSiteCatalyst to contribute most of the code in this version
  3. Better consistency in R functions around keywords and options

Of the changes listed above, I think #2 and #3 are the biggest benefit to end-users of RSiteCatalyst. The codebase is now much cleaner and more consistent in terms of the keyword arguments, has better error handling, and having a second person helping maintain the project has led to a better overall package.

Where you'll see the most difference is that all keyword arguments are now all lowercase and multi-word keyword arguments are now separated by a period instead of underscores or weird caMelCAse. We tried to maintain the same keyword order where possible to minimize code re-writes.

## Pathing and Fallout Reports

Probably the most useful improvement to RSiteCatalyst comes from those breaking changes by Adobe, which is the inclusion of Pathing and Fallout reports! I can't say with absolute certainty, but I think with these two additional reports, the API is pretty much at parity to the Adobe Analytics interface itself. So now you can create your funnels using &lt;a title=&quot;ggplot2 documentation&quot; href=&quot;http://ggplot2.org/&quot; target=&quot;_blank&quot;&gt;ggplot2&lt;/a&gt;, make force-directed graphs or Sankey charts using &lt;a title=&quot;d3Network documentation&quot; href=&quot;http://christophergandrud.github.io/d3Network/&quot; target=&quot;_blank&quot;&gt;d3Network&lt;/a&gt; or just simple reporting of top 'Next Pages' and the like.

## Support for OAuth Authentication

As part of Adobe's commitment to consolidating systems under the single Adobe Marketing Cloud, authentication with the API using OAuth is now possible. How to set up OAuth authentication is beyond the scope of this blog post, but you can get more information at this link: &lt;a title=&quot;Adobe Marketing Cloud OAuth&quot; href=&quot;https://marketing.adobe.com/resources/help/en_US/mcloud/link_accounts.html&quot; target=&quot;_blank&quot;&gt;Adobe Marketing Cloud OAuth&lt;/a&gt;.

For those of you who don't have OAuth credentials setup yet, the &quot;legacy&quot; version of authentication is still available in RSiteCatalyst.

## GetClassifications, Inline Segmentation and More

Finally, there is now additional functionality on the descriptive side, as you can now download which Classifications are defined for a report suite, segments can be defined inline (i.e. from R) for the 'Queue' reports using the `BuildClassificationValueSegment()` function and functions that existed in previous versions of RSiteCatalyst tend to have more options defined than in previous versions.

## Summary/We Want To Hear From You

While this new version of RSiteCatalyst has some annoying breaking changes, overall the package is much more robust than prior versions. I think the increase in functionality is well worth the minor annoyance of re-writing some code. Additionally, eventually Adobe will deprecate v1.3 of their API, so it's better to move over sooner rather than later.

But for all of the improvements that have been made, there's always room for improvement, whether it's fixing unforeseen bugs, adding new features, improving the documentation or anything else. For all suggestions, bug fixes and the like, please submit them to the &lt;a title=&quot;RSiteCatalyst GitHub&quot; href=&quot;https://github.com/randyzwitch/RSiteCatalyst&quot; target=&quot;_blank&quot;&gt;GitHub repository&lt;/a&gt; so that myself and Willem can evaluate and incorporate them. We're also VERY open to any of you in the R community who are able to patch the code or add new features. As a friend in the data science community says, a Pull Request is always better than a Feature Request 🙂

Happy API'ing everyone!</content>
      </item>
      
    
      
      <item>
        <title>Visualizing Analytics Languages With VennEuler.jl</title>
        
          <description>&lt;p&gt;It often doesn’t take much to get me off track, and on a holiday weekend…well, I was just begging for a fun way to shirk. Enter Harlan Harris:&lt;/p&gt;

</description>
        
        <pubDate>Fri, 29 Aug 2014 11:16:24 -0400</pubDate>
        <link>
        http://randyzwitch.com/visualizing-analytics-languages-venneuler-jl/</link>
        <guid isPermaLink="true">http://randyzwitch.com/visualizing-analytics-languages-venneuler-jl/</guid>
        <content type="html" xml:base="/visualizing-analytics-languages-venneuler-jl/">It often doesn't take much to get me off track, and on a holiday weekend...well, I was just begging for a fun way to shirk. Enter Harlan Harris:

&lt;blockquote class=&quot;twitter-tweet&quot; data-cards=&quot;hidden&quot; data-partner=&quot;tweetdeck&quot;&gt;
  &lt;p&gt;
    someone redo this area-prop'l Venn w/ my Julia pkg! &lt;a href=&quot;http://t.co/Mh8rXZbRgY&quot;&gt;http://t.co/Mh8rXZbRgY&lt;/a&gt; &lt;a href=&quot;http://t.co/RDWNQHTw3S&quot;&gt;http://t.co/RDWNQHTw3S&lt;/a&gt; &lt;a href=&quot;http://t.co/ljujd9DG0T&quot;&gt;http://t.co/ljujd9DG0T&lt;/a&gt; via &lt;a href=&quot;https://twitter.com/revodavid&quot;&gt;@revodavid&lt;/a&gt;
  &lt;/p&gt;

  &lt;p&gt;
    — Harlan Harris (@HarlanH) &lt;a href=&quot;https://twitter.com/HarlanH/statuses/505365468363100160&quot;&gt;August 29, 2014&lt;/a&gt;
  &lt;/p&gt;
&lt;/blockquote&gt;

Hey, I'm someone looking for something to do! And I like writing Julia code! So let's have a look at recreating this diagram in Julia using VennEuler.jl (&lt;a title=&quot;VennEuler.jl example&quot; href=&quot;http://nbviewer.ipython.org/gist/randyzwitch/860e1d9ae5a12cb61b1b&quot; target=&quot;_blank&quot;&gt;IJulia Notebook link&lt;/a&gt;):

&lt;div style=&quot;width: 490px&quot; class=&quot;wp-caption alignnone&quot;&gt;
  &lt;img src=&quot;http://revolution-computing.typepad.com/.a/6a010534b1db25970b01a73e0af9c7970d-800wi&quot; alt=&quot;&quot; width=&quot;480&quot; height=&quot;427&quot; /&gt;

  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Source: Revolution R/KDNuggets
  &lt;/p&gt;
&lt;/div&gt;

&lt;a href=&quot;http://blog.revolutionanalytics.com/2014/08/r-tops-kdnuggets-data-analysis-software-poll-for-4th-consecutive-year.html&quot; target=&quot;_blank&quot;&gt;http://blog.revolutionanalytics.com/2014/08/r-tops-kdnuggets-data-analysis-software-poll-for-4th-consecutive-year.html&lt;/a&gt;

## Installing VennEuler.jl

Because VennEuler.jl is not in METADATA as of the time of writing, instead of using Pkg.add() you'll need to run:

{% highlight julia linenos %}
Pkg.clone(&quot;https://github.com/HarlanH/VennEuler.jl.git&quot;)
{% endhighlight %}

Note that VennEuler uses some of the more exotic packages (at least to me) like NLopt and Cairo, so you might need to have a few additional dependencies installed with the package.

## Data

The data was a bit confusing to me at first, since the percentages add up to more than 100% (people could vote multiple times). In order to create a dataset to use, I took the percentages, multiplied by 1000, then re-created the voting pattern. The data for the graph can be downloaded from &lt;a title=&quot;Dataset&quot; href=&quot;http://randyzwitch.com/wp-content/uploads/2014/08/kdnuggets_language_survey_2014.csv&quot; target=&quot;_blank&quot;&gt;this link&lt;/a&gt;.

## Code - Circles

With a few modifications, I basically re-purposed Harlan's code from the [package test files](https://github.com/HarlanH/VennEuler.jl/blob/master/test/DC2.jl). The circle result is as follows:

{% highlight julia linenos %}
using VennEuler

data, labels = readcsv(&quot;/home/rzwitch/Desktop/kdnuggets_language_survey_2014.csv&quot;, header=true)
data = bool(data)
labels = vec(labels)

#Circles
eo = make_euler_object(labels, data, EulerSpec()) # circles, for now

(minf,minx,ret) = optimize(eo, random_state(eo), ftol=-1, xtol=0.0025, maxtime=120, pop=1000)
println(&quot;got $minf at $minx (returned $ret)&quot;)

render(&quot;/home/rzwitch/Desktop/kd.svg&quot;, eo, minx)
{% endhighlight %}

![venneulercircles](/wp-content/uploads/2014/08/venneulercircles.png)

Since the percentage of R, SAS, and Python users isn't too dramatically different (`49.81%`, `33.42%`, `40.97%` respectively) and the visualizations are circles, it's a bit hard to tell that R is about 16% points higher than SAS and 9% points higher than Python.

## Code - Rectangles

Alternatively, we can use rectangles to represent the areas:

{% highlight julia linenos %}
using VennEuler

data, labels = readcsv(&quot;/home/rzwitch/Desktop/kdnuggets_language_survey_2014.csv&quot;, header=true)
data = bool(data)
labels = vec(labels)

# Rectangles
eo = make_euler_object(labels, data, [EulerSpec(:rectangle), EulerSpec(:rectangle, [.5, .5, .4], [0, 0, 0]),
    EulerSpec(:rectangle)],
    sizesum=.3)


(minf,minx,ret) = optimize_iteratively(eo, random_state(eo), ftol=-1, xtol=0.0025, maxtime=5, pop=100)
println(&quot;phase 1: got $minf at $minx (returned $ret)&quot;)
(minf,minx,ret) = optimize(eo, minx, ftol=-1, xtol=0.001, maxtime=30, pop=100)
println(&quot;phase 2: got $minf at $minx (returned $ret)&quot;)

render(&quot;/home/rzwitch/Desktop/kd-rects.svg&quot;, eo, minx)
{% endhighlight %}

![venneulerrectangles](/wp-content/uploads/2014/08/venneulerrectangles.png)

Here, it's a slight bit easier to see that SAS and Python are about the same area-wise and that R is larger, although the different dimensions do obscure this fact a bit.

## Summary

If I spent more time with this package, I'm sure I could make something even more aesthetically pleasing. And for that matter, it's still a pre-production package that will no doubt get better in the future. But at the very least, there is a way to create an area-proportional representation of relationships using VennEuler.jl in Julia.</content>
      </item>
      
    
      
      <item>
        <title>String Interpolation for Fun and Profit</title>
        
          <description>&lt;p&gt;In a previous post, I showed how I frequently use &lt;a href=&quot;http://randyzwitch.com/julia-odbc-jl/&quot;&gt;Julia as a ‘glue’ language&lt;/a&gt; to connect multiple systems in a complicated data pipeline. For this blog post, I will show two more examples where I use Julia for general programming, rather than for computationally-intense programs.&lt;/p&gt;

</description>
        
        <pubDate>Mon, 14 Jul 2014 08:01:10 -0400</pubDate>
        <link>
        http://randyzwitch.com/string-interpolation-julia/</link>
        <guid isPermaLink="true">http://randyzwitch.com/string-interpolation-julia/</guid>
        <content type="html" xml:base="/string-interpolation-julia/">In a previous post, I showed how I frequently use [Julia as a 'glue' language](http://randyzwitch.com/julia-odbc-jl/) to connect multiple systems in a complicated data pipeline. For this blog post, I will show two more examples where I use Julia for general programming, rather than for computationally-intense programs.

## String Building: Introduction

The [Strings section of the Julia Manual](http://docs.julialang.org/en/latest/manual/strings/) provides a very in-depth treatment of the considerations when using strings within Julia. For the purposes of my examples, there are only three things to know:

  * Strings are immutable within Julia and 1-indexed
  * Strings are easily created through the a syntax familiar to most languages:

{% highlight julia %}
julia&gt; authorname = &quot;randy zwitch&quot;
&quot;randy zwitch&quot;

julia&gt; typeof(authorname)
String
{% endhighlight %}

  * String interpolation is easiest done using dollar-sign notation. Additionally, parenthesis can be used to avoid symbol ambiguity:

{% highlight julia %}
julia&gt; interpolated = &quot;the author of this blog post is $(authorname)&quot;
&quot;the author of this blog post is randy zwitch&quot;
{% endhighlight %}

~~If you are using large volumes of textual data, you'll want to pay attention to the difference between the various string types that Julia provides (_UTF8/16/32, ASCII, Unicode, etc_), but for the purposes of this blog post we'll just be using the _ASCIIString_ type by not explicitly declaring the string type and only using ASCII characters.~~

EDIT, 9/8/2016: Starting with version 0.5, Julia defaults to the `String` type, which is an UTF-8 character encoding.

## Example 1: Repetitive Queries

As part of my data engineering responsibilities at work, I often get requests to pull a sample of every table in a new database in our Hadoop cluster. This type of request is usually from the business owner, who wants to evaluate the data set has been imported correctly, but doesn't actually want to write any sort of queries. So using the [ODBC.jl](https://github.com/quinnj/ODBC.jl) package, I repeatedly do the same `select * from &lt;tablename&gt;` query and save to individual .tab files:

{% highlight julia linenos %}
_
_       _ _(_)_     |  A fresh approach to technical computing
(_)     | (_) (_)    |  Documentation: http://docs.julialang.org
_ _   _| |_  __ _   |  Type &quot;help()&quot; to list help topics
| | | | | | |/ _` |  |
| | |_| | | | (_| |  |  Version 0.3.0-prerelease+4028 (2014-07-02 23:42 UTC)
_/ |\__'_|_|_|\__'_|  |  Commit 2185bd1 (11 days old master)
|__/                   |  x86_64-w64-mingw32
julia&gt; using ODBC
julia&gt; ODBC.connect(&quot;Production hiveserver2&quot;, usr=&quot;&quot;, pwd=&quot;&quot;)
ODBC Connection Object
----------------------
Connection Data Source: Production hiveserver2
Production hiveserver2 Connection Number: 1
Contains resultset? No
julia&gt; tables = query(&quot;show tables in db;&quot;);
elapsed time: 0.167028049 seconds
julia&gt; for tbl in tables[:tab_name]
query(&quot;select * from db.$(tbl) limit 1000;&quot;; output=&quot;C:\\data_dump\\$(tbl).tab&quot;, delim='\t')
end

julia&gt;
{% endhighlight %}

While the query is simple, writing/running this hundreds of times would be a waste of effort. So with a simple loop over the array of tables, I can provide a sample of hundreds of tables in .tab files with five lines of code.

## Example 2: Generating Query Code

In another task, I was asked to join a handful of Hive tables, then transpose the table from &quot;long&quot; to &quot;wide&quot;, so that each id value only had one row instead of multiple. This is fairly trivial to do using `CASE` statements in SQL; the problem arises when you have thousands of potential row values to transpose into columns! Instead of getting carpal tunnel syndrome typing out thousands of `CASE` statements, I decided to use Julia to generate the SQL code itself:

{% highlight julia linenos %}
#Starting portion of query, the groupby columns
julia&gt; groupbycols =&quot;select
interact.interactionid,
interact.agentname,
interact.agentid,
interact.agentgroup,
interact.agentsupervisor,
interact.sitename,
interact.dnis,
interact.agentextension,
interact.interactiondirection,
interact.interactiontype,
interact.customerid,
interact.customercity,
interact.customerstate,
interact.interactiondatetime,
interact.durationinms,&quot;

#Generate CASE statements based on the number of possible values of queryid
julia&gt; function casestatements(repetitions::Int64)
	for queryid in 1:repetitions
		println(&quot;MAX(CASE WHEN q.queryid = $queryid then q.score END) as q$(queryid)score,&quot;)
	end
	for queryid in 1:repetitions
		println(&quot;MIN(CASE WHEN q.queryid = $queryid then q.startoffsetinms END) as q$(queryid)startoffset,&quot;)
	end
	for queryid in 1:repetitions
		println(&quot;MAX(CASE WHEN q.queryid = $queryid then q.endoffsetinms END) as q$(queryid)endoffset,&quot;)
	end
	#Last clause, so repeat it up to number of repetitions minus 1, then do simple print to get line without comma at end
	for queryid in 1:repetitions-1
		println(&quot;SUM(CASE WHEN q.queryid = $queryid and q.score &gt; q.mediumthreshold THEN 1 END) as q$(queryid)hits,&quot;)
	end
	println(&quot;SUM(CASE WHEN q.queryid = $repetitions and q.score &gt; q.mediumthreshold THEN 1 END) as q$(repetitions)hits&quot;)
end

#Ending table statement
julia&gt; tablestatements = &quot;from db.table1 as interact
left join db.table2 as q on (interact.interactionid = q.interactionid)
left join db.table3 as t on (interact.interactionid = t.interactionid)
group by 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15;&quot;

#Submitting all of the statements on one line is usually frowned upon, but this will generate my SQL code
julia&gt; println(groupbycols);casestatements(5);println(tablestatements)
select
interact.interactionid,
interact.agentname,
interact.agentid,
interact.agentgroup,
interact.agentsupervisor,
interact.sitename,
interact.dnis,
interact.agentextension,
interact.interactiondirection,
interact.interactiontype,
interact.customerid,
interact.customercity,
interact.customerstate,
interact.interactiondatetime,
interact.durationinms,
MAX(CASE WHEN q.queryid = 1 then q.score END) as q1score,
MAX(CASE WHEN q.queryid = 2 then q.score END) as q2score,
MAX(CASE WHEN q.queryid = 3 then q.score END) as q3score,
MAX(CASE WHEN q.queryid = 4 then q.score END) as q4score,
MAX(CASE WHEN q.queryid = 5 then q.score END) as q5score,
MIN(CASE WHEN q.queryid = 1 then q.startoffsetinms END) as q1startoffset,
MIN(CASE WHEN q.queryid = 2 then q.startoffsetinms END) as q2startoffset,
MIN(CASE WHEN q.queryid = 3 then q.startoffsetinms END) as q3startoffset,
MIN(CASE WHEN q.queryid = 4 then q.startoffsetinms END) as q4startoffset,
MIN(CASE WHEN q.queryid = 5 then q.startoffsetinms END) as q5startoffset,
MAX(CASE WHEN q.queryid = 1 then q.endoffsetinms END) as q1endoffset,
MAX(CASE WHEN q.queryid = 2 then q.endoffsetinms END) as q2endoffset,
MAX(CASE WHEN q.queryid = 3 then q.endoffsetinms END) as q3endoffset,
MAX(CASE WHEN q.queryid = 4 then q.endoffsetinms END) as q4endoffset,
MAX(CASE WHEN q.queryid = 5 then q.endoffsetinms END) as q5endoffset,
SUM(CASE WHEN q.queryid = 1 and q.score &gt; q.mediumthreshold THEN 1 END) as q1hits,
SUM(CASE WHEN q.queryid = 2 and q.score &gt; q.mediumthreshold THEN 1 END) as q2hits,
SUM(CASE WHEN q.queryid = 3 and q.score &gt; q.mediumthreshold THEN 1 END) as q3hits,
SUM(CASE WHEN q.queryid = 4 and q.score &gt; q.mediumthreshold THEN 1 END) as q4hits,
SUM(CASE WHEN q.queryid = 5 and q.score &gt; q.mediumthreshold THEN 1 END) as q5hits
from db.table1 as interact
left join db.table2 as q on (interact.interactionid = q.interactionid)
left join db.table3 as t on (interact.interactionid = t.interactionid)
group by 1,2,3,4,5,6,7,8,9,10,11,12,13,14,15;

julia&gt;
{% endhighlight %}

The example here only repeats the `CASE` statements five times, which wouldn't really be that much typing. However, for my actual application, the number of possible values was 2153, leading to a query result which was 8157 columns! Suffice to say, I'd still be writing that code if I decided to do it by hand.

## Summary

Like my 'glue language' post, I hope this post has shown that Julia can be used for more than grunting about microbenchmark performance. Whereas I used to use Python for doing weird string operations like this, I'm finding that the dollar-sign syntax in Julia feels more comfortable for me than the Python string formatting mini-language (although that's not particularly difficult either). So if you've been hesitant to jump into learning Julia because you think it's only useful for doing Mandelbrot calculations or complex linear algebra, Julia is just as at-home doing quick general programming tasks as well.</content>
      </item>
      
    
      
      <item>
        <title>Maybe I Don't Really Know R After All</title>
        
          <description>&lt;p&gt;Lately, I’ve been feeling that I’m spreading myself too thin in terms of programming languages. At work, I spend most of my time in &lt;a title=&quot;Hive blog posts&quot; href=&quot;http://randyzwitch.com/tag/hive/&quot; target=&quot;_blank&quot;&gt;Hive&lt;/a&gt;/SQL, with the occasional Python for my smaller data. I really prefer &lt;a href=&quot;http://randyzwitch.com/tag/julia/&quot;&gt;Julia&lt;/a&gt;, but I’m alone at work on that one. And since I maintain a package on CRAN (&lt;a title=&quot;RSiteCatalyst&quot; href=&quot;http://cran.r-project.org/web/packages/RSiteCatalyst/index.html&quot; target=&quot;_blank&quot;&gt;RSiteCatalyst&lt;/a&gt;), I frequently spend my evenings bug fix programming in R. Then, there’s the desire to learn a Java-based language like Scala (or, Java)…maybe Spark for my Hadoop work…&lt;/p&gt;

</description>
        
        <pubDate>Thu, 26 Jun 2014 07:18:36 -0400</pubDate>
        <link>
        http://randyzwitch.com/r-language-oddities/</link>
        <guid isPermaLink="true">http://randyzwitch.com/r-language-oddities/</guid>
        <content type="html" xml:base="/r-language-oddities/">Lately, I've been feeling that I'm spreading myself too thin in terms of programming languages. At work, I spend most of my time in &lt;a title=&quot;Hive blog posts&quot; href=&quot;http://randyzwitch.com/tag/hive/&quot; target=&quot;_blank&quot;&gt;Hive&lt;/a&gt;/SQL, with the occasional Python for my smaller data. I really prefer [Julia](http://randyzwitch.com/tag/julia/), but I'm alone at work on that one. And since I maintain a package on CRAN (&lt;a title=&quot;RSiteCatalyst&quot; href=&quot;http://cran.r-project.org/web/packages/RSiteCatalyst/index.html&quot; target=&quot;_blank&quot;&gt;RSiteCatalyst&lt;/a&gt;), I frequently spend my evenings bug fix programming in R. Then, there's the desire to learn a Java-based language like Scala (or, Java)...maybe Spark for my Hadoop work...

So last night, when I ran into this series of follies with R, it really makes me wonder if I really understand how R works.

## jsonlite:fromJSON

As part of the overall concept of my RSiteCatalyst package, I'm trying to make it as easy as possible for digital analysts to get their data via the &lt;a title=&quot;Adobe Analytics API&quot; href=&quot;https://marketing.adobe.com/developer/en_US&quot; target=&quot;_blank&quot;&gt;Adobe Analytics API&lt;/a&gt;.  As such, I abstract away the need to &lt;a title=&quot;Building JSON in R: Three Methods&quot; href=&quot;http://randyzwitch.com/r-json-jsonlite-sprintf-paste/&quot; target=&quot;_blank&quot;&gt;build JSON&lt;/a&gt; to request reports and parse the API answer from JSON to a data frame. Sometimes it's easy, but sometimes you get something like this:

![nested_r_dataframe](/wp-content/uploads/2014/06/nested_r_dataframe.png)

In case it's not clear what's going on here, `fromJSON()` from &lt;a title=&quot;jsonlite CRAN&quot; href=&quot;http://cran.r-project.org/web/packages/jsonlite/index.html&quot; target=&quot;_blank&quot;&gt;jsonlite&lt;/a&gt; returns a data frame as best as it can, but we have a list (of data frames!) nested inside of a column named &quot;breakdown&quot;. There are 12 rows here, but the proper data structure would be to take the data frame inside of 'breakdown' and append all of the fields from the original 12 rows, repeating the values down the rows. So something like 72 rows (12 original rows, 6 row data frames inside of the 'breakdown' column).

## Loop and Accumulate

Because this is such a small data frame, and because `*apply` functions are too frustrating in most cases, to parse this I went with the tried-and-true loop and accumulate. But instead of immediately getting what I wanted, I got this fantastic R error message:

{% highlight R linenos %}
#Loop over df and accumulate results
parsed_df &lt;- data.frame()
for(i in 1:nrow(df)){
  temp &lt;- cbind(df[i,],breakdown_list[[i]])
  parsed_df &lt;- rbind(parsed_df, temp)
}

There were 12 warnings (use warnings() to see them)
&gt;warnings()
Warning messages:
  1: In data.frame(..., check.names = FALSE) :
  row names were found from a short variable and have been discarded
{% endhighlight %}

Row names from a short variable? Off to StackOverflow, the savior of all language hackers, which lets me know I just need to &lt;a title=&quot;R row names short variable discarded&quot; href=&quot;http://stackoverflow.com/questions/23534066/cbind-warnings-row-names-were-found-from-a-short-variable-and-have-been-discar&quot; target=&quot;_blank&quot;&gt;add an argument to my `cbind()` function&lt;/a&gt;. Trying again:

{% highlight R linenos %}
#Loop over df and accumulate results
#Adding row.names = NULL fixes error message
parsed_df &lt;- data.frame()
for(i in 1:nrow(df)){
  temp &lt;- cbind(df[i,],breakdown_list[[i]], row.names = NULL)
  parsed_df &lt;- rbind(parsed_df, temp)
}

names(parsed_df)

&gt; names(parsed_df)
 [1] &quot;name&quot;           &quot;year&quot;           &quot;month&quot;          &quot;day&quot;            &quot;hour&quot;           &quot;minute&quot;         &quot;breakdownTotal&quot;
 [8] &quot;name&quot;           &quot;trend&quot;          &quot;counts&quot;  
{% endhighlight %}

So I successfully created an (84,10)-sized data frame, but `cbind()` allowed me to name two columns in the data frame &quot;name&quot;! Running 'parsed_df$name' at the REPL returns the first instance. So now, I have to use the unstable method of referring to the second 'name' column by position number if I want to access it (or, rename it using `names()` of course). The way I realized this behavior was occurring was that I tried to use `plyr::rename` and kept changing the name of two columns!

## Final Solution

In order to get past my duplicate name issue, I eventually renamed the 'name' columns individually by each object, prior to `cbind()`:

{% highlight R linenos %}
#Separate breakdown list and original data frame into different objects
df &lt;- ex_df$report$data
breakdown_list &lt;- df$breakdown
df$breakdown &lt;- NULL

#Loop over df and accumulate results
parsed_df &lt;- data.frame()
for(i in 1:nrow(df)){
  right_df &lt;-  breakdown_list[[i]]
  right_df &lt;- rename(right_df, replace=c(&quot;name&quot; = report_raw$report$elements$id[2]))
  temp &lt;- cbind(df[i,],right_df, row.names = NULL)
  parsed_df &lt;- rbind(parsed_df, temp)
}
parsed_df &lt;- rename(parsed_df, replace=c(&quot;counts&quot; = report_raw$report$metrics$id))
{% endhighlight %}

In the end, I found an answer to my solution, but it seems like every time I use R the more oddities I'm able to encounter/generate. At this point, I'm starting to question whether I really understand the underpinnings of how R works. It might be time to stop trying to be a language polyglot so much and focus on really learning a few of these tools in-depth.</content>
      </item>
      
    
      
      <item>
        <title>Using Julia As A &quot;Glue&quot; Language</title>
        
          <description>&lt;p&gt;While much of the focus in the Julia community has been on the performance aspects of Julia relative to other scientific computing languages, Julia is also perfectly suited to ‘glue’ together multiple data sources/languages. In this blog post, I will cover how to create an interactive plot using &lt;a title=&quot;Gadfly.jl documentation&quot; href=&quot;http://dcjones.github.io/Gadfly.jl/&quot; target=&quot;_blank&quot;&gt;Gadfly.jl&lt;/a&gt;, by first preparing the data using Hadoop and &lt;a title=&quot;Teradata Aster&quot; href=&quot;http://www.asterdata.com/&quot; target=&quot;_blank&quot;&gt;Teradata Aster&lt;/a&gt; via &lt;a title=&quot;Julia ODBC&quot; href=&quot;https://github.com/quinnj/ODBC.jl&quot; target=&quot;_blank&quot;&gt;ODBC.jl&lt;/a&gt;.&lt;/p&gt;

</description>
        
        <pubDate>Tue, 24 Jun 2014 04:57:31 -0400</pubDate>
        <link>
        http://randyzwitch.com/julia-odbc-jl/</link>
        <guid isPermaLink="true">http://randyzwitch.com/julia-odbc-jl/</guid>
        <content type="html" xml:base="/julia-odbc-jl/">While much of the focus in the Julia community has been on the performance aspects of Julia relative to other scientific computing languages, Julia is also perfectly suited to 'glue' together multiple data sources/languages. In this blog post, I will cover how to create an interactive plot using &lt;a title=&quot;Gadfly.jl documentation&quot; href=&quot;http://dcjones.github.io/Gadfly.jl/&quot; target=&quot;_blank&quot;&gt;Gadfly.jl&lt;/a&gt;, by first preparing the data using Hadoop and &lt;a title=&quot;Teradata Aster&quot; href=&quot;http://www.asterdata.com/&quot; target=&quot;_blank&quot;&gt;Teradata Aster&lt;/a&gt; via &lt;a title=&quot;Julia ODBC&quot; href=&quot;https://github.com/quinnj/ODBC.jl&quot; target=&quot;_blank&quot;&gt;ODBC.jl&lt;/a&gt;.

The example problem I am going to solve is calculating and visualizing the number of airplanes by hour in the air at any given time in the U.S. for the year 1987. Because of the structure and storage of the underlying data, I will need to write some custom Hive code, upload the data to Teradata Aster via a command-line utility, re-calculate the number of flights per hour using a built-in Aster function, then using Julia to visualize the data.

## Step 1: Getting Data From Hadoop

In a prior set of &lt;a title=&quot;Getting Started Using Hadoop, Part 3: Loading Data&quot; href=&quot;http://randyzwitch.com/uploading-data-hadoop-amazon-ec2-cloudera-part-3/&quot; target=&quot;_blank&quot;&gt;blog posts&lt;/a&gt;, I talked about loading the &lt;a title=&quot;Airline dataset&quot; href=&quot;http://stat-computing.org/dataexpo/2009/&quot; target=&quot;_blank&quot;&gt;airline dataset&lt;/a&gt; into Hadoop, then &lt;a title=&quot;Getting Started With Hadoop, Final: Analysis Using Hive &amp; Pig&quot; href=&quot;http://randyzwitch.com/getting-started-hadoop-hive-pig/&quot; target=&quot;_blank&quot;&gt;analyzing the dataset using Hive or Pig&lt;/a&gt;. Using ODBC.jl, we can use Hive via Julia to submit our queries. The hardest part of setting up this process is making sure that you have the appropriate Hive drivers for your Hadoop cluster and credentials (which isn't covered here). Once you have your DSN set up, running Hive queries is as easy as the following:

{% highlight julia linenos %}
using ODBC

#Connect to Hadoop cluster via Hive (pre-defined Windows DSN in ODBC Manager)
hiveconn = ODBC.connect(&quot;Production hiveserver2&quot;; usr=&quot;your-user-name&quot;, pwd=&quot;your-password-here&quot;)

#Clean data, return results directly to file
#Data returned with have origin of flight, flight takeoff, flight landing and elapsed time
hive_query_string =
&quot;select
origin,
from_unixtime(flight_takeoff_datetime_origin) as flight_takeoff_datetime_origin,
from_unixtime(flight_takeoff_datetime_origin + (actualelapsedtime * 60)) as flight_landing_datetime_origin,
actualelapsedtime
from
(select
origin,
unix_timestamp(CONCAT(year,\&quot;-\&quot;, month, \&quot;-\&quot;, dayofmonth, \&quot; \&quot;, SUBSTR(LPAD(deptime, 4, 0), 1, 2), \&quot;:\&quot;, SUBSTR(LPAD(deptime, 4, 0), 3, 4), \&quot;:\&quot;, \&quot;00\&quot;))  as flight_takeoff_datetime_origin,
actualelapsedtime
from vw_airline
where year = 1987 and actualelapsedtime &gt; 0) inner_query;&quot;

#Run query, save results directly to file
query(hive_query_string, hiveconn;output=&quot;C:\\airline_times.csv&quot;,delim=',')
{% endhighlight %}

In this code, I've written my query as a Julia string, to keep my code easily modifiable. Then, I pass the Julia string object to the `query()` function, along with my ODBC `connection` object. This query runs on Hadoop through Hive, then streams the result directly to my local hard drive, making this a very RAM efficient (though I/O inefficient!) operation.

## Step 2: Shelling Out To Load Data To Aster

Once I created the file with my Hadoop results in it, I now have a decision point: I can either A) do the rest of the analysis in Julia or B) use a different tool for my calculations. Because this is a toy example, I'm going to use Teradata Aster to do my calculations, which provides a convenient function called `burst()` to regularize timestamps into fixed intervals. But before I can use Aster to 'burst' my data, I first need to upload it to the database.

While I could loop over the data within Julia and insert each record one at a time, Teradata provides a command-line utility to upload data in parallel. Running command-line scripts from within Julia is as easy as using the `run()` command, with each command surrounded in backticks:

{% highlight julia linenos %}
#Connect to Aster (pre-defined Windows DSN in ODBC Manager)
asterconn = ODBC.connect(&quot;aster01&quot;; usr=&quot;your-user-name&quot;, pwd=&quot;your-password&quot;)

#Create table to hold airline results
create_airline_table_statement =
&quot;create table ebi_temp.airline
(origin varchar,
flight_takeoff_datetime_origin timestamp,
flight_landing_datetime_origin timestamp,
actualelapsedtime int,
partition key (origin))&quot;

#Execute query
query(create_airline_table_statement, asterconn)

#Create airport table
#Data downloaded from http://openflights.org/data.html
create_airport_table_statement =
&quot;create table ebi_temp.airport
(airport_id int,
name varchar,
city varchar,
country varchar,
IATAFAA varchar,
ICAO varchar,
latitude float,
longitude float,
altitude int,
timezone float,
dst varchar,
partition key (country))&quot;

#Execute query
query(create_airport_table_statement, asterconn)

#Upload data via run() command
#ncluster_loader utility already on Windows PATH
run(`ncluster_loader -h 192.168.1.1 -U your-user-name -w your-password -d aster01 -c --skip-rows=1 --el-enabled --el-table e_dist_error_2 --el-schema temp temp.airline C:\\airline_times.csv`)

run(`ncluster_loader -h 192.168.1.1 -U your-user-name -w your-password -d aster01 -c --el-enabled --el-table e_dist_error_2 --el-schema temp temp.airport C:\\airports.dat`)
{% endhighlight %}

While I could've run this at the command-line, having all of this within an IJulia Notebook keeps all my work together, should I need to re-run this in the future.

## Step 3: Using Aster For Calculations

With my data now loaded in Aster, I can normalize the timestamps to UTC, then 'burst' the data into regular time intervals. Again, all of this can be done via ODBC from within Julia:

{% highlight julia linenos %}
#Normalize timestamps from local time to UTC time
aster_view_string = &quot;
create view temp.vw_airline_times_utc as
select
row_number() over(order by flight_takeoff_datetime_origin) as unique_flight_number,
origin,
flight_takeoff_datetime_origin,
flight_landing_datetime_origin,
flight_takeoff_datetime_origin - (INTERVAL '1 hour' * timezone) as flight_takeoff_datetime_utc,
flight_landing_datetime_origin - (INTERVAL '1 hour' * timezone) as flight_landing_datetime_utc,
timezone
from temp.airline
left join temp.airport on (airline.origin = airport.iatafaa);&quot;

#Execute query
query(aster_view_string, asterconn)

#Teradata Aster SQL-H functionality, accessed via ODBC query
burst_query_string =
&quot;create table temp.airline_burst_hour distribute by hash (origin) as
SELECT
*,
\&quot;INTERVAL_START\&quot;::date as calendar_date,
extract(HOUR from \&quot;INTERVAL_START\&quot;) as hour_utc
FROM BURST(
     ON (select
        unique_flight_number,
        origin,
        flight_takeoff_datetime_utc,
        flight_landing_datetime_utc
        FROM temp.vw_airline_times_utc
)
     START_COLUMN('flight_takeoff_datetime_utc')
     END_COLUMN('flight_landing_datetime_utc')
     BURST_INTERVAL('3600')
);&quot;

#Execute query
query(burst_query_string, asterconn)
{% endhighlight %}

Since it might not be clear what I'm doing here, the `burst()` function in Aster takes a row of data with a start and end timestamp, and (potentially) returns multiple rows which normalize the time between the timestamps. If you're familiar with pandas in Python, it's a similar functionality to `resample` on a series of timestamps.

## Step 4: Download Smaller Data Into Julia, Visualize

Now that the data has been processed from Hadoop to Aster through a series of queries, we now have a much smaller dataset that can be loaded into RAM and processed by Julia:

{% highlight julia linenos %}
#Calculate the number of flights per hour per day
flights_query = &quot;
select
calendar_date,
hour_utc,
sum(1) as num_flights
from temp.airline_burst_hour
group by 1,2
order by 1,2;&quot;

#Bring results into Julia DataFrame
flights_per_day = query(flights_query, asterconn)

using Gadfly

#Create boxplot, with one box plot per hour
set_default_plot_size(20cm, 12cm)
p = plot(flights_per_day , x=&quot;hour_utc&quot;, y=&quot;num_flights&quot;,
            Guide.xlabel(&quot;Hour UTC&quot;),
            Guide.ylabel(&quot;Flights In Air&quot;),
            Guide.title(&quot;Number of Flights In Air To/From U.S. By Hour - 1987&quot;),
            Scale.y_continuous(minvalue=0, maxvalue=4000),
            Geom.boxplot)
{% endhighlight %}

The Gadfly code above produces the following plot:

![gadfly](/assets/img/airline_plot.png)

Since this chart is in UTC, it might not be obvious what the interpretation is of the trend. Because the airline dataset represents flights either leaving or returning to the United States, there are many fewer planes in the air overnight and the early morning hours (UTC 7-10, 2-5am Eastern). During the hours when the airports are open, there appears to be a limit of roughly 2500 planes per hour in the sky.

## Why Not Do All Of This In Julia?

At this point, you might be tempted to wonder why go through all of this effort? Couldn't this all be done in Julia?

Yes, you probably could do all of this work in Julia with a sufficiently large amount of RAM. As a proof-of-concept, I hope I've shown that there is much more to Julia than micro-benchmarking Julia's speed relative to other scientific programming languages. You'll notice that in none of my code have I used any type annotations, as none would really make sense (nor would they improve performance).  And although this is a toy example purposely using multiple systems, I much more frequently use Julia in this manner at work than doing linear algebra or machine learning.

So next time you're tempted to use Python or R or shell scripting or whatever, consider Julia as well. Julia is just as at-home as a scripting language as a scientific computing language.</content>
      </item>
      
    
      
      <item>
        <title>Five Hard-Won Lessons Using Hive</title>
        
          <description>&lt;p&gt;&lt;em&gt;EDIT, 9/8/2016: Hive has come a long way in the two years since I’ve written this. While some of the code snippets might still work, it’s likely the case that this information is so out-of-date to be nothing more than a reflection of working with Hadoop in 2014.&lt;/em&gt;&lt;/p&gt;

</description>
        
        <pubDate>Thu, 12 Jun 2014 09:01:18 -0400</pubDate>
        <link>
        http://randyzwitch.com/hive-five-hard-won-lessons/</link>
        <guid isPermaLink="true">http://randyzwitch.com/hive-five-hard-won-lessons/</guid>
        <content type="html" xml:base="/hive-five-hard-won-lessons/">_EDIT, 9/8/2016: Hive has come a long way in the two years since I've written this. While some of the code snippets might still work, it's likely the case that this information is so out-of-date to be nothing more than a reflection of working with Hadoop in 2014._

I've been spending a ton of time lately on the data _engineering_ side of 'data science', so I've been writing a lot of Hive queries. Hive is a great tool for querying large amounts of data, without having to know very much about the underpinnings of Hadoop. Unfortunately, there are a lot of things about Hive (version 0.12 and before) that aren't quite the same as SQL and have caused me a bunch of frustration; here they are, in no particular order.

## 1. Set Hive Temp directory To Same As Final Output Directory

When doing a &quot;Create Table As&quot; (CTAS) statement in Hive, &lt;a title=&quot;Hive scratch directory&quot; href=&quot;http://doc.mapr.com/display/MapR/Hive#Hive-HiveScratchDirectory&quot; target=&quot;_blank&quot;&gt;Hive allocates temp space for the Map and Reduce portions of the job&lt;/a&gt;. If you're not lucky, the temp space for the job will be somewhere different than where your table actually ends up being saved, resulting in TWO I/O operations instead of just one. This can lead to a painful delay in when your Hive job says it is finished vs. when the table becomes available (one time, I saw a 30 hour delay writing 5TB of data).

If your Hive jobs seem to hang after the Job Tracker says they are complete, try this setting at the beginning of your session:

{% highlight shell %}
set hive.optimize.insert.dest.volume=true;
{% endhighlight %}

## 2. Column Aliasing In Group By/Order By

Not sure why this isn't a default, but if you want to be able to reference your column names by position (i.e. `group by 1,2`) instead of by name (i.e. `group by name, age`), then run this at the beginning of your session:

{% highlight shell %}
set hive.groupby.orderby.position.alias=true;
{% endhighlight %}

## 3. Be Aware Of Predicate Push-Down Rules

In Hive, you can get great performance gains if you A) partition your table by commonly used columns/business concepts (i.e. Day, State, Market, etc.) and B) you use the partitions in a `WHERE` clause. These are known as [partition-based queries](https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Select#LanguageManualSelect-PartitionBasedQueries). Otherwise, if you don't use a partition in your `WHERE` clause, you will get a full table scan.

Unfortunately, when doing an OUTER JOIN, Hive will sometimes ignore the fact that your `WHERE` clause is on a partition and do a full table scan anyway. In order to get Hive to [push your predicate down](https://cwiki.apache.org/confluence/display/Hive/OuterJoinBehavior#OuterJoinBehavior-PredicatePushdownRules) and avoid a full table scan, put your predicate on the `JOIN` instead of the `WHERE` clause:

{% highlight sql linenos %}
--#### Assume sales Hive table partitioned by day_id ####--

--Full Table Scan
select
employees.id,
b.sales
from employees
left join sales on (employees.id = sales.employee_id)
where day_id between '2014-03-01' and '2014-05-31';

--Partitioned-based query
select
employees.id,
b.sales
from employees
left join sales on (employees.id = sales.employee_id and sales.day_id between '2014-03-01' and '2014-05-31');
{% endhighlight %}

If you don't want to think about the different rules, you can generally put your limiting clauses inside your `JOIN` clause instead of on your `WHERE` clause. It _should_ just be a matter of preference (until your query performance indicates it isn't!)

## 4. Calculate And Append Percentiles Using CROSS JOIN

Suppose you want to calculate the top 10% of your customers by sales. If you try to do the following, Hive will complain about needing a `GROUP BY`, because `percentile_approx()` is a summary function:

{% highlight sql linenos %}
--Hive expects that you want to calculate your percentiles by account_number and sales
--This code will generate an error about a missing GROUP BY statement
select
account_number,
sales,
CASE WHEN sales &gt; percentile_approx(sales, .9) THEN 1 ELSE 0 END as top10pct_sales
{% endhighlight %}

To get around the the need for a `GROUP BY`, we can use a `CROSS JOIN`. A `CROSS JOIN` is another name for a Cartesian Join, meaning all of the rows from the first table will be joined to ALL of the rows of the second table. Because the subquery only returns one row, the `CROSS JOIN` provides the desired affect of joining the percentile values back to the original table while keeping the same number of rows from the original table. Generally, you don't want to do a `CROSS JOIN` (because relational data generally is joined on a key), but this is a good use case.

## 5.  Calculating a Histogram

Creating a histogram using Hive should be as simple as calling the [`histogram_numeric()`](https://cwiki.apache.org/confluence/display/Hive/StatisticsAndDataMining#StatisticsAndDataMining-histogram_numeric():Estimatingfrequencydistributions) function. However, the syntax and results of this function are just plain weird. To create a histogram, you can run the following:

{% highlight sql linenos %}
select
histogram_numeric(salary, 20) as salary_hist
from
sample_08;

--Results
[{&quot;x&quot;:23507.68627450983,&quot;y&quot;:255.0},{&quot;x&quot;:31881.7647058824,&quot;y&quot;:340.0},{&quot;x&quot;:39824.11498257844,&quot;y&quot;:287.0},{&quot;x&quot;:47615.58011049725,&quot;y&quot;:181.0},{&quot;x&quot;:55667.01219512195,&quot;y&quot;:164.0},{&quot;x&quot;:59952.499999999985,&quot;y&quot;:8.0},{&quot;x&quot;:66034.67153284674,&quot;y&quot;:137.0},{&quot;x&quot;:75642.31707317074,&quot;y&quot;:82.0},{&quot;x&quot;:82496.13636363638,&quot;y&quot;:44.0},{&quot;x&quot;:91431.66666666667,&quot;y&quot;:60.0},{&quot;x&quot;:100665.71428571428,&quot;y&quot;:21.0},{&quot;x&quot;:107326.66666666667,&quot;y&quot;:15.0},{&quot;x&quot;:121248.74999999999,&quot;y&quot;:16.0},{&quot;x&quot;:142070.0,&quot;y&quot;:2.0},{&quot;x&quot;:153896.6666666667,&quot;y&quot;:6.0},{&quot;x&quot;:162310.0,&quot;y&quot;:6.0},{&quot;x&quot;:169810.0,&quot;y&quot;:2.0},{&quot;x&quot;:176740.0,&quot;y&quot;:2.0},{&quot;x&quot;:193925.0,&quot;y&quot;:8.0},{&quot;x&quot;:206770.0,&quot;y&quot;:2.0}]
{% endhighlight %}

The results of this query comes back as a list, which is very un-SQL like! To get the data as a table, we can use `LATERAL VIEW` and `EXPLODE`:

{% highlight sql linenos %}
SELECT
   CAST(hist.x as int) as bin_center,
   CAST(hist.y as bigint) as bin_height
FROM (select
      histogram_numeric(salary, 20) as salary_hist
      from
      sample_08) a
LATERAL VIEW explode(salary_hist) exploded_table as hist;

--Results
	bin_center	bin_height
0	23507	255
1	31881	340
2	39824	287
3	47615	181
4	55667	164
5	59952	8
6	66034	137
7	75642	82
8	82496	44
9	91431	60
10	100665	21
11	107326	15
12	121248	16
13	142070	2
14	153896	6
15	162310	6
16	169810	2
17	176740	2
18	193925	8
19	206770	2
{% endhighlight %}

However, now that we have a table of data, it's still not clear how to create a histogram, as the _center of variable-width bins_ is what is returned by Hive. The [Hive documentation for `histogram_numeric()`](https://cwiki.apache.org/confluence/display/Hive/StatisticsAndDataMining#StatisticsAndDataMining-histogram_numeric():Estimatingfrequencydistributions) references Gnuplot, Excel, Mathematica and MATLAB, which I can only assume can deal with plotting the centers?  Eventually I'll figure out how to deal with this using R or Python, but for now, I just use the table as a quick gauge of what the data looks like.</content>
      </item>
      
    
      
      <item>
        <title>Building JSON in R: Three Methods</title>
        
          <description>&lt;p&gt;When I set out to build &lt;a href=&quot;https://github.com/randyzwitch/RSiteCatalyst&quot;&gt;RSiteCatalyst&lt;/a&gt;, I had a few major goals: learn R, build &lt;a href=&quot;http://cran.r-project.org/web/packages/RSiteCatalyst/index.html&quot;&gt;CRAN&lt;/a&gt;-worthy package and learn the &lt;a href=&quot;https://marketing.adobe.com/developer/en_US/documentation&quot;&gt;Adobe Analytics API&lt;/a&gt;. As I reflect back on how the package has evolved over the past two years and what I’ve learned, I think my greatest learning was around how to deal with JSON (and strings in general).  &lt;/p&gt;

</description>
        
        <pubDate>Tue, 13 May 2014 09:27:39 -0400</pubDate>
        <link>
        http://randyzwitch.com/r-json-jsonlite-sprintf-paste/</link>
        <guid isPermaLink="true">http://randyzwitch.com/r-json-jsonlite-sprintf-paste/</guid>
        <content type="html" xml:base="/r-json-jsonlite-sprintf-paste/">When I set out to build [RSiteCatalyst](https://github.com/randyzwitch/RSiteCatalyst), I had a few major goals: learn R, build [CRAN](http://cran.r-project.org/web/packages/RSiteCatalyst/index.html)-worthy package and learn the [Adobe Analytics API](https://marketing.adobe.com/developer/en_US/documentation). As I reflect back on how the package has evolved over the past two years and what I've learned, I think my greatest learning was around how to deal with JSON (and strings in general).  

JSON is ubiquitous as a data-transfer mechanism over the web, and R does a decent job providing the functionality to not only read JSON but also to create JSON. There are at least three methods I know of to build JSON strings, and this post will cover the pros and cons of each method.

### Method 1: Building JSON using paste

As a beginning R user, I didn't have the awareness of how many great user-contributed packages are out there. So throughout the RSiteCatalyst source code you can see [gems](https://github.com/randyzwitch/RSiteCatalyst/blob/master/R/QueueOvertime.R#L75-78) like:

{% highlight r linenos %}
#&quot;metrics&quot; would be a user input into a function arguments
metrics &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;)

#Loop over the metrics list, appending proper curly braces
metrics_conv &lt;- lapply(metrics, function(x) paste('{&quot;id&quot;:', '&quot;', x, '&quot;', '}', sep=&quot;&quot;))

#Collapse the list into a proper comma separated string
metrics_final &lt;- paste(metrics_conv, collapse=&quot;, &quot;)
{% endhighlight %}

The code above loops over a character vector (using `lapply` instead of a for loop like a good R user!), appending curly braces, then flattening the list down to a string. While this code works, it's a quite brittle way to build JSON. You end up needing to worry about matching quotation marks, remembering if you need curly braces, brackets or singletons...overall, it's a maintenance nightmare to build strings this way.

Of course, if you have a _really simple_ JSON string you need to build, `paste()` doesn't have to be off-limits, but for a majority of the cases I've seen, it's probably not a good idea.

### Method 2: Building JSON using sprintf

Somewhere in the middle of building version 1 of RSiteCatalyst, I started learning Python. For those of you who aren't familiar, Python has a [string interpolation operator](https://docs.python.org/2/library/stdtypes.html#string-formatting) `%`, which allows you to do things like the following:

{% highlight python linenos %}
In [1]: print &quot;Here's a string subtitution for my name: %s&quot; %(&quot;Randy&quot;)

Out[1]: &quot;Here's a string subtitution for my name: Randy&quot;
{% endhighlight %}

Thinking that this was the most useful thing I'd ever seen in programming, I naturally searched to see if R had the same functionality. Of course, I quickly learned that all C-based languages have [`printf/sprintf`](http://en.wikipedia.org/wiki/Printf_format_string), and R is no exception. So I started [building JSON using sprintf](https://github.com/randyzwitch/RSiteCatalyst/blob/master/R/QueueTrended.R#L115-119) in the following manner:

{% highlight r linenos %}
elements_list = sprintf('{&quot;id&quot;:&quot;%s&quot;,
                          &quot;top&quot;: &quot;%s&quot;,
                          &quot;startingWith&quot;:&quot;%s&quot;,
                          &quot;search&quot;:{&quot;type&quot;:&quot;%s&quot;, &quot;keywords&quot;:[%s]}
                          }', element, top, startingWith, searchType, searchKW2)
{% endhighlight %}

In this example, we're now passing R objects into the `sprintf()` function, with `%s` tokens everywhere we need to substitute text. This is certainly an improvement over `paste()`, especially given that Adobe provides example JSON via their [API explorer](https://marketing.adobe.com/developer/en_US/get-started/api-explorer). So I copied the example strings, replaced their examples with my tokens and voilà! Better JSON string building.

### Method 3: Building JSON using a package (jsonlite, rjson or RJSONIO)

While `sprintf()` allowed for much easier JSON, there is still a frequent code smell in RSiteCatalyst, as evidenced by the [following](https://github.com/randyzwitch/RSiteCatalyst/blob/master/R/GetTrafficVars.R#L31-39):

{% highlight r linenos %}
#Converts report_suites to JSON
if(length(report_suites)&gt;1){
  report_suites &lt;- toJSON(report_suites)
} else {
  report_suites &lt;- toJSON(list(report_suites))
}

#API request
json &lt;- postRequest(&quot;ReportSuite.GetTrafficVars&quot;,paste('{&quot;rsid_list&quot;:', report_suites , '}'))
{% endhighlight %}

At some point, I realized that using the `toJSON()` function from [rjson](http://cran.r-project.org/web/packages/rjson/index.html) would take care of the formatting R objects to strings, yet I didn't make the leap to understanding that I could build the _whole string_ using R objects translated by `toJSON()`! So I have more hard-to-maintain code where I'm checking the class/length of objects and formatting them. The efficient way to do this using rjson would be:

{% highlight r linenos %}
#Efficient method
library(rjson)
report_suites &lt;- list(rsid_list=c(&quot;A&quot;, &quot;B&quot;, &quot;C&quot;))
request.body &lt;- toJSON(report_suites)

#API request
json &lt;- postRequest(&quot;ReportSuite.GetTrafficVars&quot;, request.body)
{% endhighlight %}

With the code above, we're building JSON in a very R-looking manner; just R objects and functions, and in return getting the output we want. While it's slightly less obvious what is being created by `request.body`, there's literally zero bracket-matching, quoting issues or anything else to worry about in building our JSON. That's not to say that there isn't a learning curve to using a JSON package, but I'd rather figure out whether I need a character vector or list than burn my eyes out looking for mismatched quotes and brackets!

### Collaborating Makes You A Better Programmer

Like any pursuit, you can get pretty far on your own through hard work and self-study. However, I wouldn't be nearly where I am without collaborating with others (especially learning about how to build JSON properly in R!). A majority of the RSiteCatalyst code for the upcoming version 1.4 was re-written by &lt;a href=&quot;https://github.com/WillemPaling&quot; title=&quot;Willem Paling GitHub&quot; target=&quot;_blank&quot;&gt;Willem Paling&lt;/a&gt;, where he added consistency to keyword arguments, switched to &lt;a href=&quot;http://cran.r-project.org/web/packages/jsonlite/index.html&quot; title=&quot;jsonlite CRAN&quot; target=&quot;_blank&quot;&gt;jsonlite&lt;/a&gt; for better JSON parsing to Data Frames, and most importantly for the topic of this post, cleaned up the method of building all the required JSON strings!

Edit 5/13: For a more thorough example of building complex JSON using jsonlite, check out &lt;a href=&quot;https://github.com/randyzwitch/RSiteCatalyst/blob/version_1_4/R/QueueRanked.R#L67-114&quot; title=&quot;Complex R jsonlite example&quot; target=&quot;_blank&quot;&gt;this example&lt;/a&gt; from the v1.4 branch of RSiteCatalyst. The linked example R code populates the required arguments from this &lt;a href=&quot;https://gist.github.com/randyzwitch/762343d5e8d8501af522&quot; title=&quot;Example JSON call from Adobe API Explorer&quot; target=&quot;_blank&quot;&gt;JSON outline&lt;/a&gt; provide by Adobe.</content>
      </item>
      
    
      
      <item>
        <title>Using SQL Workbench with Apache Hive</title>
        
          <description>&lt;p&gt;If you’ve spent any non-trivial amount of time working with Hadoop and Hive at the command line, you’ve likely wished that you could interact with Hadoop like you would any other database. If you’re lucky, your Hadoop administrator has already installed the &lt;a href=&quot;http://gethue.com/&quot;&gt;Apache Hue&lt;/a&gt; front-end to your cluster, which allows for &lt;a href=&quot;http://randyzwitch.com/uploading-data-hadoop-amazon-ec2-cloudera-part-3/&quot;&gt;interacting with Hadoop via an easy-to-use browser interface&lt;/a&gt;. However, if you don’t have Hue, Hive also supports access via JDBC; the downside is, setup is not as easy as including a single JDBC driver.&lt;/p&gt;

</description>
        
        <pubDate>Fri, 25 Apr 2014 10:05:57 -0400</pubDate>
        <link>
        http://randyzwitch.com/sql-workbench-apache-hadoop-hive/</link>
        <guid isPermaLink="true">http://randyzwitch.com/sql-workbench-apache-hadoop-hive/</guid>
        <content type="html" xml:base="/sql-workbench-apache-hadoop-hive/">If you've spent any non-trivial amount of time working with Hadoop and Hive at the command line, you've likely wished that you could interact with Hadoop like you would any other database. If you're lucky, your Hadoop administrator has already installed the [Apache Hue](http://gethue.com/) front-end to your cluster, which allows for [interacting with Hadoop via an easy-to-use browser interface](http://randyzwitch.com/uploading-data-hadoop-amazon-ec2-cloudera-part-3/). However, if you don't have Hue, Hive also supports access via JDBC; the downside is, setup is not as easy as including a single JDBC driver.

While there are paid database administration tools such as [Aqua Data Studio](http://www.aquafold.com/dbspecific/apache_hive_client.html) that support Hive, I'm an open source kind of guy, so this tutorial will show you how to use [SQL Workbench](http://www.sql-workbench.net/) to access Hive via JDBC. This tutorial assumes that you are proficient enough to get SQL Workbench installed on whatever computing platform you are using (Windows, OSX, or Linux).

### Download Hadoop jars

The hardest part of using Hive via JDBC is getting all of the required jars. At work I am using a [MapR distribution of Hadoop](http://www.mapr.com/), and each Hadoop vendor platform provides drivers for their version of Hadoop. For MapR, all of the required Java .jar files are located at `/opt/mapr/hive/hive-0.1X/lib` (where X represents the Hive version number you are using).

![mapr-hive-jars](/wp-content/uploads/2014/04/mapr-hive-jars.png)

 &lt;p class=&quot;wp-caption-text&quot;&gt;
    Download all the .jar files in one shot, just in case you need them in the future
 &lt;/p&gt;

Since it's not always clear which .jar files are required (especially for other projects/setups you might be doing), I just downloaded the entire set of files and placed them in a directory called `hadoop_jars`. If you're not using MapR, you'll need to find and download your vendor-specific version of the following .jar files:

  * hive-exec.jar
  * hive-jdbc.jar
  * hive-metastore.jar
  * hive-service.jar

Additionally, you will need the following general Hadoop jars (Note: for clarity/long-term applicability of this blog post, I have removed the version number from all of the jars):

  * hive-cli.jar
  * libfb303.jar
  * slf4j-api.jar
  * commons-logging.jar
  * hadoop-common.jar
  * httpcore.jar
  * httpclient.jar

Whew. Once you have the Hive JDBC driver and the 10 other .jar files, we can begin the installation process.

### Setting up Hive JDBC driver

Setting up the JDBC driver is simply a matter of providing SQL Workbench with the location of all 11 of the required .jar files. After clicking `File -&gt; Manage Drivers`, you'll want to click on the white page icon to create a New Driver. Use the Folder icon to add the .jars:

![sqlworkbench-hive-driver-setup](/wp-content/uploads/2014/04/sqlworkbench-hive-driver-setup.png)

For the `Classname` box, if you are using a relatively new version of Hive, you'll be using Hive2 server. In that case, the `Classname` for the Hive driver is `org.apache.hive.jdbc.HiveDriver` (this should pop up on-screen, you just need to select the value). You are not required to put any value for the Sample URL. Hit `OK` and the driver window will close.

### Connection Window

With the Hive driver defined, all that's left is to define the connection string. Assuming your Hadoop administrator didn't change the default `port` from `10000`, your connection string should look as follows:

![sqlworkbench-hive-connectionstring](/wp-content/uploads/2014/04/sqlworkbench-hive-connectionstring.png)

As stated above, I'm assuming you are using Hive2 Server; if so, your connection string will be `jdbc:hive2://your-hadoop-cluster-location:10000`. After that, type in your Username and Password and you should be all set.

### Using Hive with SQL Workbench

Assuming you have achieved success with the instructions above, you're now ready to use Hive like any other database. You will be able to submit your Hive code via the Query Window, view your schemas/tables (via the 'Database Explorer' functionality which opens in a separate tab) and generally use Hive like any other relational database.

Of course, it's good to remember that Hive isn't actually a relational database! From my experience, using Hive via SQL Workbench works pretty well, but the underlying processing is still in Hadoop. So you're not going to get the clean cancelling of queries like you would with an RDBMS , there can be a significant lag to getting answers back (due to the Hive overhead), you can blow up your computer streaming back results larger than available RAM...but it beats working at the command line.</content>
      </item>
      
    
      
      <item>
        <title>Real-time Reporting with the Adobe Analytics API</title>
        
          <description>&lt;p&gt;Starting with &lt;a href=&quot;http://randyzwitch.com/rsitecatalyst-version-1-3-release-notes/&quot;&gt;version 1.3.1 of RSiteCatalyst&lt;/a&gt;, you can now access the &lt;a href=&quot;https://developer.omniture.com/en_US/documentation/sitecatalyst-reporting/c-real-time#concept_AD1D9EC2BC9C4897B9DE3C99D0066B8E&quot;&gt;real-time reporting capabilities of the Adobe Analytics API&lt;/a&gt; through a familiar R interface. Here’s how to get started…&lt;/p&gt;

</description>
        
        <pubDate>Mon, 10 Mar 2014 07:28:21 -0400</pubDate>
        <link>
        http://randyzwitch.com/real-time-reporting-adobe-analytics-api/</link>
        <guid isPermaLink="true">http://randyzwitch.com/real-time-reporting-adobe-analytics-api/</guid>
        <content type="html" xml:base="/real-time-reporting-adobe-analytics-api/">Starting with [version 1.3.1 of RSiteCatalyst](http://randyzwitch.com/rsitecatalyst-version-1-3-release-notes/), you can now access the [real-time reporting capabilities of the Adobe Analytics API](https://developer.omniture.com/en_US/documentation/sitecatalyst-reporting/c-real-time#concept_AD1D9EC2BC9C4897B9DE3C99D0066B8E) through a familiar R interface. Here's how to get started...

## GetRealTimeConfiguration

Before using the real-time reporting capabilities of Adobe Analytics, you first need to indicate which metrics and elements you are interested in seeing in real-time. To see which reports are already set up for real-time access on a given report suite, you can use the `GetRealTimeConfiguration()` function:

{% highlight r linenos %}
#Get Real-Time reports that already set up
realtime_reports &lt;- GetRealTimeConfiguration(&quot;&lt;reportsuite&gt;&quot;)
{% endhighlight %}

It's likely the case that the first time you set this up, you'll already see a real-time report for 'Instances-Page-Site Section-Referring Domain'. You can leave this report in place, or switch the parameters using `SaveRealTimeConfiguration()`.

## SaveRealTimeConfiguration

If you want to add/modify which real-time reports are available in a report suite, you can use the `SaveRealTimeConfiguration()` function:

{% highlight r linenos %}
SaveRealTimeConfiguration(&quot;&lt;report suite&gt;&quot;,
  metric1 = &quot;instances&quot;,
  elements1 = c(&quot;page&quot;, &quot;referringdomain&quot;, &quot;sitesection&quot;),

  metric2 = &quot;revenue&quot;,
  elements2 = c(&quot;referringdomain&quot;, &quot;sitesection&quot;)

  metric3 = &quot;orders&quot;,
  elements3 = c(&quot;products&quot;)
)
{% endhighlight %}

Up to three real-time reports are available to be stored at any given time. Note that you can mix-and-match what reports you want to modify, you don't have to submit all three reports at a given time. Finally, keep in mind that it can take up to 15 minutes for the API to incorporate your real-time report changes, so if you don't get your data right away don't keep re-submitting the function call!

## GetRealTimeReport

Once you have your real-time reports set up in the API, you can use the `GetRealTimeReport()` function in order to access your reports. There are numerous parameters for customization; selected examples are below.

### Minimum Example - Overtime Report

The simplest function call for a real-time report is to create an `Overtime` report (monitoring a metric over a specific time period):

{% highlight r linenos %}
rt &lt;- GetRealTimeReport(&quot;&lt;report suite&gt;&quot;, &quot;instances&quot;)
{% endhighlight %}

The result of this call will be a DataFrame having 15 rows of one minute granularity for your metric. This is a great way to monitor real-time orders &amp; revenue during a flash sale, see how users are accessing a landing page for an email marketing campaign or any other metric where you want up-to-the-minute status updates.

### Granularity, Offset, Periods

If you want to have a time period other than the last 15 minutes, or one minute granularity is too volatile for the metric you are monitoring, you can add additional arguments to modify the returned DataFrame:

{% highlight r linenos %}
rt2 &lt;- GetRealTimeReport(&quot;&lt;reportsuite&gt;&quot;,
                  &quot;instances&quot;,
                  periodMinutes = &quot;5&quot;,
                  periodCount = &quot;12&quot;,
                  periodOffset = &quot;10&quot;
{% endhighlight %}

For this function call, we will receive instances for the last hour (12 periods) of five minute granularity, with a 10 minute offset (meaning, `now - 10 minutes ago` is the first time period reported).

### Single Elements

Beyond just monitoring a metric over time, you can specify an element such as `page` to receive your metrics by:

{% highlight r linenos %}
GetRealTimeReport(&quot;&lt;reportsuite&gt;&quot;,
                  &quot;instances&quot;,
                  &quot;page&quot;,
                  periodMinutes = &quot;9&quot;,
                  periodCount = &quot;3&quot;)
{% endhighlight %}

This function call will return Instances by Page, for the last 27 minutes (3 rows/periods per page, 9 minute granularity...just because!). Additionally, there are other arguments such as `algorithm`, `algorithmArgument`, `firstRankPeriod` and `floorSensitivity` that allow for creating reports similar to what is provided in the Real-Time tab in the Adobe Analytics interface.

Currently, even through the Adobe Analytics API supports real-time reports with three breakdowns, only one element breakdown is supported by RSiteCatalyst; it is planned to extend these functions in RSiteCatalyst to full support the real-time capabilities in the near future.

## From DataFrame to Something 'Shiny'

If we're talking real-time reports, we're probably talking about dashboarding. If we're talking about R and dashboarding, then naturally, &lt;a title=&quot;R ggvis&quot; href=&quot;http://ggvis.rstudio.com/&quot; target=&quot;_blank&quot;&gt;ggvis&lt;/a&gt;/&lt;a title=&quot;Shiny Web Applications&quot; href=&quot;http://www.rstudio.com/shiny/&quot; target=&quot;_blank&quot;&gt;Shiny&lt;/a&gt; comes to mind. While providing a full ggvis/Shiny example is beyond the scope of this blog post, it's my hope to provide a working example in a future blog post. Stay tuned!</content>
      </item>
      
    
      
      <item>
        <title>RSiteCatalyst Version 1.3 Release Notes</title>
        
          <description>&lt;p&gt;Version 1.3 of the RSiteCatalyst package to access the Adobe Analytics API is now available on CRAN! Changes include:&lt;/p&gt;

</description>
        
        <pubDate>Tue, 04 Feb 2014 04:44:19 -0500</pubDate>
        <link>
        http://randyzwitch.com/rsitecatalyst-version-1-3-release-notes/</link>
        <guid isPermaLink="true">http://randyzwitch.com/rsitecatalyst-version-1-3-release-notes/</guid>
        <content type="html" xml:base="/rsitecatalyst-version-1-3-release-notes/">Version 1.3 of the RSiteCatalyst package to access the Adobe Analytics API is now available on CRAN! Changes include:

  * Search via regex functionality in `QueueRanked/QueueTrended` functions
  * Support for Realtime API reports: `Overtime` and one-element `Ranked` report
  * Allow for variable API request timing in `Queue*`` functions
  * Fixed `validate` flag in JSON request to work correctly
  * Deprecated `GetAdminConsoleLog` (appears to be removed from the API)

### Searching via Regex functionality

RSiteCatalyst now supports the search functionality of the API, similar in nature to using the Advanced Filter/Search feature within Reports &amp; Analytics. Here are some examples for the `QueueRanked` function:

{% highlight r linenos %}
#Top 100 Pages where the pagename starts with &quot;Categories&quot;
#Uses searchKW argument
queue_ranked_pages_search &lt;- QueueRanked(&quot;&lt;reportsuite&gt;&quot;,
                                         &quot;2013-01-01&quot;,
                                         &quot;2014-01-28&quot;,
                                         c(&quot;pageviews&quot;, &quot;visits&quot;),
                                         &quot;page&quot;,
                                         top = &quot;100&quot;,
                                         searchKW = &quot;^Categories&quot;  
                                          )

#Top 100 Pages where the pagename starts with &quot;Categories&quot; OR contains &quot;Home Page&quot;
#Uses searchKW and searchType arguments
queue_ranked_pages_search_or &lt;- QueueRanked(&quot;&lt;reportsuite&gt;&quot;,
                                            &quot;2013-01-01&quot;,
                                            &quot;2014-01-28&quot;,
                                            c(&quot;pageviews&quot;, &quot;visits&quot;),
                                            &quot;page&quot;,
                                            top = &quot;100&quot;,
                                            searchKW = c(&quot;^Categories&quot;, &quot;Home Page&quot;),
                                            searchType = &quot;OR&quot;
                                            )
{% endhighlight %}

QueueTrended function calls work in a similar manner, returning elements broken down by time rather than a single record per element name.

### Realtime Reporting API

Accessing the [Adobe Analytics Realtime API](https://developer.omniture.com/en_US/documentation/sitecatalyst-reporting/c-real-time#concept_AD1D9EC2BC9C4897B9DE3C99D0066B8E) now has limited support in RSiteCatalyst. Note that this is different than just using the `currentData` parameter within the `Queue*` functions, as the realtime API methods provide data within a minute of that data being generated on-site. Currently, RSiteCatalyst only supports the most common types of reports: `Overtime` (no eVar or prop breakdown) and one-element breakdown.

Because of the extensive new functionality for the `GetRealTimeConfiguration()`, `SaveRealTimeConfiguration()` and `GetRealTimeReport()` functions, code examples will be provided as a separate blog post.

### Variable request timing for Queue function calls

This feature is to fix the issue of having an API request run so long that RSiteCatalyst gave up on retrieving an answer. Usually, API requests come back in a few seconds, but in selected cases a call could run so long as to exhaust the number of attempts (previously, 10 minutes). You can use the `maxTries` and `waitTime` arguments to specify how many times you'd like RSiteCatalyst to retrieve the report and the wait time between calls:

{% highlight r linenos %}
#Change timing of function call
#Wait 30 seconds between attempts to retrieve the report, try 5 times
queue_overtime_visits_pv_day_social_anomaly2 &lt;- QueueOvertime(&quot;&lt;reportsuite&gt;&quot;,
                                                              &quot;2013-01-01&quot;,
                                                              &quot;2014-01-28&quot;,
                                                              c(&quot;visits&quot;, &quot;pageviews&quot;),
                                                              &quot;day&quot;,
                                                              &quot;Visit_Social&quot;,
                                                              anomalyDetection = &quot;1&quot;,
                                                              currentData = &quot;1&quot;,
                                                              maxTries = 5,
                                                              waitTime = 30)
{% endhighlight %}

If you don't specify either of these arguments, RSiteCatalyst will default to trying every five seconds to retrieve the report, up to 120 tries.

### New Contributor: Willem Paling

I'm pleased to announce that I've got a new contributor for RSiteCatalyst, &lt;a title=&quot;WillemPaling on Twitter&quot; href=&quot;https://twitter.com/WillemPaling&quot; target=&quot;_blank&quot;&gt;Willem Paling&lt;/a&gt;! Willem did a near-complete re-write of the underlying code to access the API, and rather than have multiple packages out in the wild, we've decided to merge our works. So look forward to better-written R code and more complete access to the Adobe Analytics API's in future releases...

### Support

If you run into any problems with RSiteCatalyst, please &lt;a title=&quot;RSiteCatalyst GitHub issues&quot; href=&quot;https://github.com/randyzwitch/RSiteCatalyst/issues&quot; target=&quot;_blank&quot;&gt;file an issue on GitHub&lt;/a&gt; so it can be tracked properly. Note that I’m not an Adobe employee, so I can only provide so much support, as in most cases I can’t validate your settings to ensure you are set up correctly (nor do I have any inside information about how the system works :) )

_Edit 2/20/2014: I mistakenly forgot to add the new real-time functions to the R NAMESPACE file, and as such, you won't be able to use them if you are using version 1.3. Upgrade to 1.3.1 to access the real-time functionality._</content>
      </item>
      
    
      
      <item>
        <title>Getting Started With Hadoop, Final: Analysis Using Hive &amp; Pig</title>
        
          <description>&lt;p&gt;We’ve finally made it to the final post in this tutorial! In my prior posts about getting started with &lt;a title=&quot;Hadoop posts&quot; href=&quot;http://randyzwitch.com/tag/hadoop/&quot; target=&quot;_blank&quot;&gt;Hadoop&lt;/a&gt;, we’ve covered the entire lifecycle from how to set up a small cluster using Amazon EC2 and Cloudera through how to load data using Hue. With our data loaded in HDFS, we can finally move on to the actual analysis portion of the &lt;a title=&quot;Airline dataset&quot; href=&quot;http://stat-computing.org/dataexpo/2009/the-data.html&quot; target=&quot;_blank&quot;&gt;airline dataset&lt;/a&gt; using Hive and Pig.&lt;/p&gt;

</description>
        
        <pubDate>Sun, 12 Jan 2014 15:25:32 -0500</pubDate>
        <link>
        http://randyzwitch.com/getting-started-hadoop-hive-pig/</link>
        <guid isPermaLink="true">http://randyzwitch.com/getting-started-hadoop-hive-pig/</guid>
        <content type="html" xml:base="/getting-started-hadoop-hive-pig/">We've finally made it to the final post in this tutorial! In my prior posts about getting started with &lt;a title=&quot;Hadoop posts&quot; href=&quot;http://randyzwitch.com/tag/hadoop/&quot; target=&quot;_blank&quot;&gt;Hadoop&lt;/a&gt;, we've covered the entire lifecycle from how to set up a small cluster using Amazon EC2 and Cloudera through how to load data using Hue. With our data loaded in HDFS, we can finally move on to the actual analysis portion of the &lt;a title=&quot;Airline dataset&quot; href=&quot;http://stat-computing.org/dataexpo/2009/the-data.html&quot; target=&quot;_blank&quot;&gt;airline dataset&lt;/a&gt; using Hive and Pig.

## Basic Descriptive Statistics Using Hive

In &lt;a title=&quot;Getting Started with Hadoop Part 4&quot; href=&quot;http://randyzwitch.com/hadoop-creating-tables-hive/&quot; target=&quot;_blank&quot;&gt;part 4&lt;/a&gt; of this tutorial, we used a Hive script to create a view named &quot;vw_airline&quot; to hold all of our airline data. Running a simple query is as easy as running the following in the Hive window in Hue. Note that this is ANSI-standard SQL code, even though we are submitting it using Hive:

![simple-hive-query&quot;](/wp-content/uploads/2013/11/simple-hive-query.png)

A simple query like this is a great way to get a feel for the table, including determining whether or not the files were loaded correctly. Once the results are displayed, you can create simple visualizations like bar charts, line plots, scatterplots and pie charts. The results of the following query are shown below. Knowing this dataset, I can tell that the files were loaded incorrectly; the dips at Years 1994 and 2004 are too few records and will need to be reloaded.

![hive-visualization-results](/wp-content/uploads/2013/11/hive-visualization-results.png)

1994 and 2004 have too few rows, which was validated using `wc -l 1994.csv` at the command line (outside of Hadoop)

Besides just simple counts, Hive supports nearly all standard SQL syntax relative to functions such as `SUM`, `COUNT`, `MIN`, `MAX`, etc., table `joins`, `user-defined functions (UDF)``, window functions...pretty much everything that you are used to from other SQL tools.  AFAIK, the only thing that Hive doesn't support is nested sub-queries, but that's on the &lt;a title=&quot;Hortonworks Stinger Initiative&quot; href=&quot;http://hortonworks.com/labs/stinger/&quot; target=&quot;_blank&quot;&gt;Stinger initiative for improving Hive&lt;/a&gt;. However, depending on the nested subquery being performed, you might be able to accomplish the same thing using a &lt;a title=&quot;Hive LEFT SEMI JOIN&quot; href=&quot;https://cwiki.apache.org/confluence/display/Hive/LanguageManual+Joins#LanguageManualJoins-Examples&quot; target=&quot;_blank&quot;&gt;LEFT SEMI JOIN&lt;/a&gt;.

## Using Pig for Analytics

It's important to realize that Hadoop isn't just another RDBMS where you run SQL. Using Pig, you can &lt;a title=&quot;Pig syntax basics&quot; href=&quot;http://pig.apache.org/docs/r0.12.0/start.html#data-work-with&quot; target=&quot;_blank&quot;&gt;write scripts for calculation&lt;/a&gt; in a similar manner to using other high-level languages such as Python or R.

For example, suppose we wanted to calculate the average distance for each route. A Pig script to calculate this might look like the following:

{% highlight pig linenos %}
--Load data from view to use
air = LOAD 'default.vw_airline' USING org.apache.hcatalog.pig.HCatLoader();

--Use FOREACH to limit data to origin, dest, distance
--Concatentate origin and destination together, separated by a pipe
--CONCAT appears to only allow two arguments, which is why the function is called twice (to allow 3 arguments)
origindest = FOREACH air generate CONCAT(origin, CONCAT('|' , dest)) as route, distance;

--Group origindest dataset by route
groupedroutes = GROUP origindest BY (route);

--Calculate average distance by route
avg_distance = FOREACH groupedroutes GENERATE group, AVG(origindest.distance);

--Show results in Pig shell
dump avg_distance;

--Write out results to text file, separated by tab (default)
store avg_distance into '/user/hue/avg_distance';
{% endhighlight %}

While it is possible to calculate average distance using Hive and a `GROUP BY` statement, one of the benefits to using Pig is having control over every step of the data flow. So while Hive queries tend to answer a single question at a time, Pig allows an analyst to chain together any number of steps in a data flow. In the example above, we could pass the average distance for each route to another transformation, join it back to the original dataset or do anything else our analyst minds can imagine!

## Summary

Over these five blog posts, I've outlined how to get started with Hadoop and 'Big Data' using Amazon and Cloudera/Hortonworks. Hopefully I've been able to demystify the &lt;a title=&quot;Hadoop concepts&quot; href=&quot;http://randyzwitch.com/big-data-hadoop-amazon-ec2-cloudera-part-1/&quot; target=&quot;_blank&quot;&gt;concepts and terminology behind Hadoop&lt;/a&gt;, shown that &lt;a title=&quot;Hadoop on Amazon EC2 using Cloudera&quot; href=&quot;http://randyzwitch.com/big-data-hadoop-amazon-ec2-cloudera-part-2/&quot; target=&quot;_blank&quot;&gt;setting up a Hadoop using Cloudera on Amazon EC2&lt;/a&gt; isn't unsurmountable, and &lt;a title=&quot;Loading data into Hadoop HDFS&quot; href=&quot;http://randyzwitch.com/uploading-data-hadoop-amazon-ec2-cloudera-part-3/&quot; target=&quot;_blank&quot;&gt;loading data&lt;/a&gt; and analyzing it using Hive and Pig isn't dramatically different than using SQL on other database systems you may have encountered in the past.

While there's a lot of hype around 'Big Data', data sizes aren't going to be getting any smaller in the future. So spend the $20 in AWS charges and build a Hadoop cluster! There's no better way to learn than by doing...</content>
      </item>
      
    
      
      <item>
        <title>Quickly Create Dummy Variables in a Data Frame</title>
        
          <description>&lt;p&gt;On Quora, a question was asked about how to fix the error of the &lt;a title=&quot;Error in Random Forest 32 levels categorical variable&quot; href=&quot;https://www.quora.com/Random-Forests/How-can-I-fix-the-error-in-the-package-randomForest&quot; target=&quot;_blank&quot;&gt;randomForest package in R not being able to handle more than 32 levels in a categorical variable&lt;/a&gt;. Seeing as how I’ve seen this question asked on Kaggle forums, StackOverflow and elsewhere, here’s the answer: code your own &lt;em&gt;dummy variables&lt;/em&gt; instead of relying on Factors!&lt;/p&gt;

</description>
        
        <pubDate>Thu, 02 Jan 2014 08:58:51 -0500</pubDate>
        <link>
        http://randyzwitch.com/creating-dummy-variables-data-frame-r/</link>
        <guid isPermaLink="true">http://randyzwitch.com/creating-dummy-variables-data-frame-r/</guid>
        <content type="html" xml:base="/creating-dummy-variables-data-frame-r/">On Quora, a question was asked about how to fix the error of the &lt;a title=&quot;Error in Random Forest 32 levels categorical variable&quot; href=&quot;https://www.quora.com/Random-Forests/How-can-I-fix-the-error-in-the-package-randomForest&quot; target=&quot;_blank&quot;&gt;randomForest package in R not being able to handle more than 32 levels in a categorical variable&lt;/a&gt;. Seeing as how I've seen this question asked on Kaggle forums, StackOverflow and elsewhere, here's the answer: code your own _dummy variables_ instead of relying on Factors!

## Code snippet

{% highlight R linenos %}
#Generate example dataframe with character column
example &lt;- as.data.frame(c(&quot;A&quot;, &quot;A&quot;, &quot;B&quot;, &quot;F&quot;, &quot;C&quot;, &quot;G&quot;, &quot;C&quot;, &quot;D&quot;, &quot;E&quot;, &quot;F&quot;))
names(example) &lt;- &quot;strcol&quot;

#For every unique value in the string column, create a new 1/0 column
#This is what Factors do &quot;under-the-hood&quot; automatically when passed to function requiring numeric data
for(level in unique(example$strcol)){
  example[paste(&quot;dummy&quot;, level, sep = &quot;_&quot;)] &lt;- ifelse(example$strcol == level, 1, 0)
}
view raw
{% endhighlight %}

As the code above shows, it's trivial to generate your own 1/0 columns of data instead of relying on Factors. There are two things to keep in mind when creating your own dummy variables:

  1. The problem you are trying to solve
  2. How much RAM you have available

While it may make sense to generate dummy variables for Customer State (~50 for the United States), if you were to use the code above on City Name, you'd likely either run out of RAM or find out that there are too many levels to be useful. Of course, with any qualitative statement such as &quot;too many levels to be useful&quot;, oftentimes the only way to definitively know is to try it! Just make sure you save your work before running this code, just in case you run out of RAM. Or, use someone else's computer for testing 😉

Edit 1/2/14: John Myles White brought up a good point via Twitter about RAM usage:

&lt;blockquote class=&quot;twitter-tweet&quot; data-conversation=&quot;none&quot; data-cards=&quot;hidden&quot; data-partner=&quot;tweetdeck&quot;&gt;
  &lt;p&gt;
    &lt;a href=&quot;https://twitter.com/randyzwitch&quot;&gt;@randyzwitch&lt;/a&gt; If you're running out of RAM with dummy variables, you probably want to use a sparse matrix instead of a data.frame.
  &lt;/p&gt;

  &lt;p&gt;
    — John Myles White (@johnmyleswhite) &lt;a href=&quot;https://twitter.com/johnmyleswhite/statuses/418821463563829248&quot;&gt;January 2, 2014&lt;/a&gt;
  &lt;/p&gt;
&lt;/blockquote&gt;</content>
      </item>
      
    
      
      <item>
        <title>Adobe Analytics Implementation Documentation in 60 Seconds</title>
        
          <description>&lt;p&gt;When I was working as a digital analytics consultant, no question quite had the ability to cause belly laughs AND angst as, “Can you send me an updated copy of your implementation documentation?” I saw companies that were spending six-or-seven-figures annually on their analytics infrastructure, multi-millions in salary for employees and yet the only way to understand what data they were collecting was to inspect their JavaScript code.&lt;/p&gt;

</description>
        
        <pubDate>Mon, 09 Dec 2013 04:46:30 -0500</pubDate>
        <link>
        http://randyzwitch.com/adobe-analytics-implementation-documentation/</link>
        <guid isPermaLink="true">http://randyzwitch.com/adobe-analytics-implementation-documentation/</guid>
        <content type="html" xml:base="/adobe-analytics-implementation-documentation/">When I was working as a digital analytics consultant, no question quite had the ability to cause belly laughs AND angst as, &quot;Can you send me an updated copy of your implementation documentation?&quot; I saw companies that were spending six-or-seven-figures annually on their analytics infrastructure, multi-millions in salary for employees and yet the only way to understand what data they were collecting was to inspect their JavaScript code.

Luckily for Adobe Analytics customers, the API provides a means of generating the framework for a properly-documented implementation. Here's how to do it using &lt;a title=&quot;RSiteCatalyst CRAN&quot; href=&quot;http://cran.r-project.org/web/packages/RSiteCatalyst/index.html&quot; target=&quot;_blank&quot;&gt;RSiteCatalyst&lt;/a&gt;.

## Generating Adobe Analytics documentation file

The code below outlines the commands needed to generate an Excel file (&lt;a title=&quot;Example Excel file Adobe Analytics Documentation&quot; href=&quot;http://randyzwitch.com/wp-content/uploads/2013/12/adobe_analytics_implementation_doc.xlsx&quot; target=&quot;_blank&quot;&gt;see example&lt;/a&gt;) with six tabs containing the basic structure of an Adobe Analytics. This report contains all of the report suites you have access to, the elements that reports can be broken down by, traffic variables (props), conversion variables (eVars) and segments available for reporting.

Additionally, within each tab metadata is provided that contains the various settings for variables, so you'll be able to document the expiration settings for eVars, participation, list variables, segment types and so on. 

{% highlight r linenos %}
library(&quot;RSiteCatalyst&quot;)
library(&quot;WriteXLS&quot;)

#Validate that underlying Perl modules for WriteXLS are installed correctly
#Will return &quot;Perl found. All required Perl modules were found&quot; if installed correctly
testPerl()

#### 1. Pull data for all report suites to create one comprehensive report ####

#Authenticate with Adobe Analytics API
SCAuth(&quot;user:company&quot;, &quot;sharedsecret&quot;)

#Get Report Suites
report_suites &lt;- GetReportSuites()

#Get Available Elements
elements &lt;- GetElements(report_suites$rsid)

#Get eVars
evars &lt;- GetEvars(report_suites$rsid)

#Get Segments
segments &lt;- GetSegments(report_suites$rsid)

#Get Success Events
events &lt;- GetSuccessEvents(report_suites$rsid)

#Get Traffic Vars
props &lt;- GetProps(report_suites$rsid)

#### 2. Generate a single Excel file

#Create list of report suite objects, written as strings
objlist &lt;- c(&quot;report_suites&quot;, &quot;elements&quot;, &quot;evars&quot;, &quot;segments&quot;, &quot;events&quot;, &quot;props&quot;)

#Write out Excel file with auto-width columns, a bolded header row and filters turned on
WriteXLS(objlist, &quot;/Users/randyzwitch/Desktop/adobe_analytics_implementation_doc.xlsx&quot;,
         AdjWidth = TRUE, BoldHeaderRow = TRUE, AutoFilter = TRUE)
{% endhighlight %}

The only &quot;gotchas&quot; to keep in mind when using the script above is that the user running this will only receive data for report suites they have access to (which is determined by Admin panel setting within Adobe Analytics) and that you need to have the &lt;a title=&quot;WriteXLS&quot; href=&quot;http://cran.r-project.org/web/packages/WriteXLS/index.html&quot; target=&quot;_blank&quot;&gt;WriteXLS&lt;/a&gt; package installed to write to Excel. The WriteXLS package uses Perl as the underlying code, so you'll need to validate that the package is installed correctly, which is done using the `testPerl()` function in the package.

## This is pretty bare-bones, no?

After you run this code, you'll have an Excel file that has all of the underlying characteristics of your Adobe Analytics implementation. It's important to realize that this is only the _starting_ point; a great set of documentation will contain other pieces of information such as where/when the value is set (on entry, every page, when certain events occur, etc.), a layman's explanation about what the data element means and other _business information_ so your stakeholders can be confident they are using the data correctly. Additionally, you might consider creating a single Excel file for every report suite in your implementation. It's trivial to modify the code above to subset each data frame used above for a single value of rsid, then write to separate Excel files. Regardless of how your structure your documentation, DOCUMENT YOUR IMPLEMENTATION! The employees that come after you (and your future self!) will thank you.

_EDIT 2/4/2016: Thanks to reader &lt;a href=&quot;https://twitter.com/CSitty&quot; target=&quot;_blank&quot;&gt;@CSitty&lt;/a&gt; for pointing out the R code became a little stale. The documentation generating code should now work again for RSiteCatalyst versions &gt;= 1.4 and WriteXLS &gt;= 4.0 (basically, any current version as of the time of this update)._</content>
      </item>
      
    
      
      <item>
        <title>Using Amazon EC2 with IPython Notebook</title>
        
          <description>&lt;p&gt;Last week, I wrote a guest blog post at &lt;a title=&quot;Guest post at Bad Hessian&quot; href=&quot;http://badhessian.org/2013/11/cluster-computing-for-027hr-using-amazon-ec2-and-ipython-notebook/&quot; target=&quot;_blank&quot;&gt;Bad Hessian&lt;/a&gt; about how to use IPython Notebook along with Amazon EC2 as your data science &amp;amp; analytics platform. I won’t reproduce the whole article here, but if you are interested in step-by-step instruction on how to setup an Amazon EC2 instance to use IPython Notebook, see the SlideShare presentation below which outlines the steps needed to setup a remote IPython Notebook environment (or, &lt;a title=&quot;amazon-ec2-ipython-installation PDF&quot; href=&quot;http://randyzwitch.com/wp-content/uploads/2013/11/cluster-computing-ipython-ec2.pdf&quot; target=&quot;_blank&quot;&gt;PDF download&lt;/a&gt;).&lt;/p&gt;

</description>
        
        <pubDate>Thu, 21 Nov 2013 15:13:11 -0500</pubDate>
        <link>
        http://randyzwitch.com/ipython-notebook-amazon-ec2/</link>
        <guid isPermaLink="true">http://randyzwitch.com/ipython-notebook-amazon-ec2/</guid>
        <content type="html" xml:base="/ipython-notebook-amazon-ec2/">Last week, I wrote a guest blog post at &lt;a title=&quot;Guest post at Bad Hessian&quot; href=&quot;http://badhessian.org/2013/11/cluster-computing-for-027hr-using-amazon-ec2-and-ipython-notebook/&quot; target=&quot;_blank&quot;&gt;Bad Hessian&lt;/a&gt; about how to use IPython Notebook along with Amazon EC2 as your data science &amp; analytics platform. I won't reproduce the whole article here, but if you are interested in step-by-step instruction on how to setup an Amazon EC2 instance to use IPython Notebook, see the SlideShare presentation below which outlines the steps needed to setup a remote IPython Notebook environment (or, &lt;a title=&quot;amazon-ec2-ipython-installation PDF&quot; href=&quot;http://randyzwitch.com/wp-content/uploads/2013/11/cluster-computing-ipython-ec2.pdf&quot; target=&quot;_blank&quot;&gt;PDF download&lt;/a&gt;).

&lt;iframe src=&quot;http://www.slideshare.net/slideshow/embed_code/28501345&quot; width=&quot;476&quot; height=&quot;400&quot; frameborder=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;

If you already have experience setting up EC2 images and just need the IPython Notebook settings, here are the commands that are needed to set up your IPython public notebook server.

{% highlight python linenos %}
#### Start IPython, generate SHA1 password to use for IPython Notebook server

$ ipython
Python 2.7.5 |Anaconda 1.8.0 (x86_64)| (default, Oct 24 2013, 07:02:20)
Type &quot;copyright&quot;, &quot;credits&quot; or &quot;license&quot; for more information.

IPython 1.1.0 -- An enhanced Interactive Python.
?         -&gt; Introduction and overview of IPython's features.
%quickref -&gt; Quick reference.
help      -&gt; Python's own help system.
object?   -&gt; Details about 'object', use 'object??' for extra details.

In [1]: from IPython.lib import passwd

In [2]: passwd()
Enter password:
Verify password:
Out[2]: 'sha1:207eb1f4671f:92af695...'

#### Create nbserver profile

$ ipython profile create nbserver
[ProfileCreate] Generating default config file: u'/.ipython/profile_nbserver/ipython_config.py'
[ProfileCreate] Generating default config file: u'/.ipython/profile_nbserver/ipython_qtconsole_config.py'
[ProfileCreate] Generating default config file: u'/.ipython/profile_nbserver/ipython_notebook_config.py'
[ProfileCreate] Generating default config file: u'/.ipython/profile_nbserver/ipython_nbconvert_config.py'

#### Create self-signed SSL certificate

$ openssl req -x509 -nodes -days 365 -newkey rsa:1024 -keyout mycert.pem -out mycert.pem

#### Modify ipython_notebook_config.py configuration file
#### Add these lines to the top of the file; no other changes necessary
#### Obviously, you'll want to add your path to the .pem key and your password

# Configuration file for ipython-notebook.

c = get_config()

# Kernel config
c.IPKernelApp.pylab = 'inline'  # if you want plotting support always

# Notebook config
c.NotebookApp.certfile = u'/home/ubuntu/certificates/mycert.pem'
c.NotebookApp.ip = '*'
c.NotebookApp.open_browser = False
c.NotebookApp.password = u'sha1:207eb1f4671f:92af695...'
# It is a good idea to put it on a known, fixed port
c.NotebookApp.port = 8888

#### Start IPython Notebook on the remote server

$ ipython notebook --profile=nbserver
{% endhighlight %}

Happy IPython Notebooking!</content>
      </item>
      
    
      
      <item>
        <title>Adding Line Numbers in IPython/Jupyter Notebooks</title>
        
          <description>&lt;p&gt;Lately, I’ve been using Jupyter Notebooks for all of my Python and Julia coding. The ability to develop and submit small snippets of code and create plots inline is just so useful that it has broken the stranglehold of using an IDE while I’m coding. However, the one thing that was missing for a smooth transition was line numbers in the cells; luckily, this can be achieved in two ways.&lt;/p&gt;

</description>
        
        <pubDate>Tue, 19 Nov 2013 03:48:18 -0500</pubDate>
        <link>
        http://randyzwitch.com/line-numbers-ipython-notebook/</link>
        <guid isPermaLink="true">http://randyzwitch.com/line-numbers-ipython-notebook/</guid>
        <content type="html" xml:base="/line-numbers-ipython-notebook/">Lately, I've been using Jupyter Notebooks for all of my Python and Julia coding. The ability to develop and submit small snippets of code and create plots inline is just so useful that it has broken the stranglehold of using an IDE while I'm coding. However, the one thing that was missing for a smooth transition was line numbers in the cells; luckily, this can be achieved in two ways.

## Keyboard Shortcut

The easiest way to add line numbers to a Jupyter Notebook is to use the keyboard shortcut, which is **Ctrl-m** to enter Command Mode, then type** L**. Just highlight the cell you are interested in adding line numbers to, then hit the keyboard shortcut to toggle the line numbers.

![ipython-notebook-line-numbers](/wp-content/uploads/2013/11/ipython-notebook-line-numbers.png)

## Add Line Numbers to All Cells at Startup

&lt;del&gt;While the keyboard shortcut is great for toggling line numbers on/off, I prefer having line numbers always on. Luckily, the IPython Dev folks on Twitter were kind enough to explain how to do this:&lt;/del&gt;

&lt;blockquote class=&quot;twitter-tweet&quot; lang=&quot;en&quot;&gt;
  &lt;p style=&quot;text-align: center;&quot;&gt;
    &lt;del&gt;&lt;a href=&quot;https://twitter.com/randyzwitch&quot;&gt;@randyzwitch&lt;/a&gt; add `IPython.Cell.options_default.cm_config.lineNumbers = true;` to your custom.js&lt;/del&gt;
  &lt;/p&gt;

  &lt;p style=&quot;text-align: center;&quot;&gt;
    &lt;del&gt;— IPython Dev (@IPythonDev) &lt;a href=&quot;https://twitter.com/IPythonDev/statuses/394906726828236800&quot;&gt;October 28, 2013&lt;/a&gt;&lt;/del&gt;
  &lt;/p&gt;
&lt;/blockquote&gt;

&lt;del&gt;I use OSX with the default 'profile_default' profile, so the path for my custom.js file for IPython is:&lt;/del&gt;

&lt;pre&gt;&lt;del&gt;/Users/randyzwitch/.ipython/profile_default/static/custom/&lt;/del&gt;&lt;/pre&gt;

&lt;del&gt;Similarly, you can do the same for IJulia:&lt;/del&gt;

&lt;pre&gt;&lt;del&gt;/Users/randyzwitch/.ipython/profile_julia/static/custom&lt;/del&gt;&lt;/pre&gt;

&lt;del&gt;If you are using a different operating system than OSX, or you are using OSX and you don't see a custom.js file in these locations, a quick search for custom.js will get you to the right file location. Once you open up the custom.js file, you can place the line of JavaScript anywhere in the file, as long as it's not inside any of any pre-existing functions in the file.&lt;/del&gt;

&lt;del&gt;Once you place the line of JavaScript in your file, you'll need to restart IPython/IJulia completely for the change to take effect. After that, you'll have line numbers in each cell, each Notebook!&lt;/del&gt;

_Edit 11/4/2015: Thanks to reader Nat Dunn, I've been made aware that the above method no longer works, which isn't a surprise given the amount of changes between IPython Notebook to the entire Jupyter project in the past 2 years._

_For the (currently) correct method of &lt;a href=&quot;https://www.webucator.com/blog/2015/11/show-line-numbers-by-default-in-ipython-notebook/&quot; target=&quot;_blank&quot;&gt;adding line numbers to Jupyter Notebook by default&lt;/a&gt;, please see &lt;a href=&quot;https://www.webucator.com/blog/2015/11/show-line-numbers-by-default-in-ipython-notebook/&quot; target=&quot;_blank&quot;&gt;Nat's post&lt;/a&gt; with the correct instructions on modifying the custom.js file._</content>
      </item>
      
    
      
      <item>
        <title>RSiteCatalyst Version 1.2 Release Notes</title>
        
          <description>&lt;p&gt;Version 1.2 of the &lt;a title=&quot;RSiteCatalyst CRAN&quot; href=&quot;http://cran.r-project.org/web/packages/RSiteCatalyst/index.html&quot; target=&quot;_blank&quot;&gt;RSiteCatalyst&lt;/a&gt; package to access the Adobe Analytics API is now available on CRAN! Changes include:&lt;/p&gt;

</description>
        
        <pubDate>Tue, 05 Nov 2013 03:29:16 -0500</pubDate>
        <link>
        http://randyzwitch.com/rsitecatalyst-version-1-2-release-notes/</link>
        <guid isPermaLink="true">http://randyzwitch.com/rsitecatalyst-version-1-2-release-notes/</guid>
        <content type="html" xml:base="/rsitecatalyst-version-1-2-release-notes/">Version 1.2 of the &lt;a title=&quot;RSiteCatalyst CRAN&quot; href=&quot;http://cran.r-project.org/web/packages/RSiteCatalyst/index.html&quot; target=&quot;_blank&quot;&gt;RSiteCatalyst&lt;/a&gt; package to access the Adobe Analytics API is now available on CRAN! Changes include:

  * Removed RCurl package dependency
  * Changed argument order for `GetAdminConsoleLog` to avoid error when date not passed
  * Return proper numeric type for metric columns
  * Fixed bug in `GetEVars` function
  * Added `validate:true` flag to API to improve error reporting
  * Removed remaining references to Omniture

For the most part, the only noticeable change for most users will be that you no longer need to call `as.numeric()` on a DataFrame after getting the results of an API call, as all functions now return the proper numeric type.

### Changes from Development Version

For any of you out there that may have installed the 1.2 development version directly from GitHub, the only difference between the 1.2 development version and the stable, CRAN version of the package is that support for the Adobe Analytics Real Time API has been removed. This functionality will continue to be developed on the &lt;a title=&quot;RSiteCatalyst version 1.3&quot; href=&quot;https://github.com/randyzwitch/RSiteCatalyst/tree/version_1_3&quot; target=&quot;_blank&quot;&gt;1.3 development branch&lt;/a&gt; on GitHub.

### Testing

For this release, I've made a more concerted effort to test RSiteCatalyst on various platforms outside of OSX (where I do my development). RSiteCatalyst works in the following environments:

  * OSX Lion and prior
  * Ubuntu 12.04 LTS
  * Windows 7 64-bit SP1
  * Windows 8.1 64-bit
  * R 2.15.2 and newer
  * R and RStudio

If your environment is not listed above, it is still likely the case that RSiteCatalyst will work in your environment, as there is no operating-system-specific code in the package. If you are finding issues, validate that you have all package dependencies installed, your Adobe account has Web Service Access privileges (set in Admin panel), you have permission access to the report suites you are trying to access (also an Admin panel setting) and that your company doesn't have any firewall settings that would prevent API access.

### Support

If you run into any problems with RSiteCatalyst, please &lt;a title=&quot;RSiteCatalyst GitHub issues&quot; href=&quot;https://github.com/randyzwitch/RSiteCatalyst/issues&quot; target=&quot;_blank&quot;&gt;file an issue on GitHub&lt;/a&gt; so it can be tracked properly. Note that I'm not an Adobe employee, so I can only provide so much support, as in most cases I can't validate your settings to ensure you are set up correctly (nor do I have any inside information about how the system works 🙂 )</content>
      </item>
      
    
      
      <item>
        <title>Clustering Search Keywords Using K-Means Clustering</title>
        
          <description>&lt;p&gt;One of the key tenets to doing impactful digital analysis is understanding what your visitors are trying to accomplish. One of the easiest methods to do this is by analyzing the words your visitors use to arrive on site (search keywords) and what words they are using while on the site (on-site search). &lt;/p&gt;

</description>
        
        <pubDate>Tue, 17 Sep 2013 10:41:01 -0400</pubDate>
        <link>
        http://randyzwitch.com/rsitecatalyst-k-means-clustering/</link>
        <guid isPermaLink="true">http://randyzwitch.com/rsitecatalyst-k-means-clustering/</guid>
        <content type="html" xml:base="/rsitecatalyst-k-means-clustering/">One of the key tenets to doing impactful digital analysis is understanding what your visitors are trying to accomplish. One of the easiest methods to do this is by analyzing the words your visitors use to arrive on site (search keywords) and what words they are using while on the site (on-site search). 

Although Google has made it much more difficult to analyze search keywords over the past several years (due to their passing of &lt;a title=&quot;(not provided): Using R and the Google Analytics API&quot; href=&quot;http://randyzwitch.com/r-google-analytics-api/&quot; target=&quot;_blank&quot;&gt;&quot;(not provided)&quot;&lt;/a&gt; instead of the actual keywords), we can create customer intent segments based on the keywords that are still being passed using unsupervised clustering methods such as k-means clustering.

## Concept: K-Means Clustering/Unsupervised Learning

&lt;a title=&quot;k-means clustering&quot; href=&quot;http://en.wikipedia.org/wiki/K-means_clustering&quot; target=&quot;_blank&quot;&gt;K-means clustering&lt;/a&gt; is one of many techniques within &lt;a title=&quot;Unsupervised learning Wikipedia&quot; href=&quot;http://en.wikipedia.org/wiki/Unsupervised_learning&quot; target=&quot;_blank&quot;&gt;unsupervised learning&lt;/a&gt; that can be used for text analysis. _Unsupervised_ refers to the fact that we're trying to understand the structure of our underlying data, rather than trying to optimize for a specific, pre-labeled criterion (such as creating a predictive model for conversion). Unsupervised learning is a great technique for exploratory analysis in that the analyst enforces few assumptions on the data, so previously unexamined relationships can be determined _then_ analyzed; contrast that with pre-defined relationships specified by the analyst (such as _visitors from mobile_ or _visitors from social_), then evaluating how various metrics differ across these pre-defined groups.

Without getting too technical, k-means clustering is a method of partitioning data into 'k' subsets, where each data element is assigned to the closest cluster based on the distance of the data element from the center of the cluster. In order to use k-means clustering with text data, we need to do some text-to-numeric transformation of our text data. Luckily, R provides several packages to simplify the process.

## Converting Text to Numeric Data: Document-Term Matrix

Since I use Adobe Analytics on this blog, I'm going to use the &lt;a title=&quot;RSiteCatalyst&quot; href=&quot;http://randyzwitch.com/rsitecatalyst/&quot; target=&quot;_blank&quot;&gt;RSiteCatalyst package&lt;/a&gt; to get my natural search keywords into a dataframe. Once the keywords are in a dataframe, we can use the &lt;a title=&quot;RTextTools&quot; href=&quot;http://www.rtexttools.com/&quot; target=&quot;_blank&quot;&gt;RTextTools&lt;/a&gt; package to create a document-term matrix, where each row is our search term and each column is a 1/0 representation of whether a single word is contained within natural search term. 

{% highlight r linenos %}
#### 0. Setup
library(&quot;RSiteCatalyst&quot;)
library(&quot;RTextTools&quot;) #Loads many packages useful for text mining

#### 1. RSiteCatalyst code - Get Natural Search Keywords &amp; Metrics

#Set credentials
SCAuth(&lt;username:company&gt;, &lt;shared secret&gt;)

#Get list of search engine terms
searchkeywords &lt;- QueueRanked(&lt;report_suite&gt;, &quot;2013-02-01&quot;,&quot;2013-09-16&quot;,
                  c(&quot;entries&quot;, &quot;visits&quot;, &quot;pageviews&quot;, &quot;instances&quot;, &quot;bounces&quot;),
                  &quot;searchenginenaturalkeyword&quot;, top=&quot;100000&quot;, startingWith = &quot;1&quot;)

#### 2. Process keywords into format suitable for text mining

#Create document-term matrix, passing data cleaning options
#Stem the words to avoid multiples of similar words
#Need to set wordLength to minimum of 1 because &quot;r&quot; a likely term
dtm &lt;- create_matrix(searchkeywords$'Natural Search Keyword',
                     stemWords=TRUE,
                     removeStopwords=FALSE,
                     minWordLength=1,
                     removePunctuation= TRUE)
{% endhighlight %}

Within the `create_matrix` function, I'm using four keyword arguments to process the data:

1. `stemWords` reduces a word down to its root, which is a standardization method to avoid having multiple versions of words referring to the same concept (e.g. argue, arguing, argued reduces to 'argu')
2. `removeStopwords` eliminates common English words such as &quot;they&quot;, &quot;he&quot; , &quot;always&quot;
3. `minWordLength` sets the minimum number of characters that constitutes a 'word', which I set to 1 because of the high likelihood of 'r' being a keyword
4. `removePunctuation` removes periods, commas, etc.

## Popular Words

If you are unfamiliar with the terms that might be contained in your dataset, you can use the `findFreqTerms` to see which terms occur with a minimum frequency. Here are the terms that occur at least 20 times on this blog:

{% highlight r linenos %}
&gt; #Inspect most popular words, minimum frequency of 20
&gt; findFreqTerms(dtm, lowfreq=20)
  [1] &quot;15&quot;           &quot;2008&quot;         &quot;2009&quot;         &quot;2011&quot;         &quot;a&quot;            &quot;ad&quot;           &quot;add&quot;          &quot;adsens&quot;      
  [9] &quot;air&quot;          &quot;analyt&quot;       &quot;and&quot;          &quot;appl&quot;         &quot;at&quot;           &quot;back&quot;         &quot;bezel&quot;        &quot;black&quot;       
 [17] &quot;book&quot;         &quot;bookmark&quot;     &quot;break&quot;        &quot;broke&quot;        &quot;broken&quot;       &quot;bubbl&quot;        &quot;by&quot;           &quot;can&quot;         
 [25] &quot;case&quot;         &quot;chang&quot;        &quot;child&quot;        &quot;code&quot;         &quot;comment&quot;      &quot;comput&quot;       &quot;cost&quot;         &quot;cover&quot;       
 [33] &quot;crack&quot;        &quot;css&quot;          &quot;custom&quot;       &quot;data&quot;         &quot;delet&quot;        &quot;disabl&quot;       &quot;display&quot;      &quot;do&quot;          
 [41] &quot;doe&quot;          &quot;drop&quot;         &quot;edit&quot;         &quot;eleven&quot;       &quot;em209&quot;        &quot;entri&quot;        &quot;fix&quot;          &quot;footer&quot;      
 [49] &quot;footerphp&quot;    &quot;for&quot;          &quot;free&quot;         &quot;from&quot;         &quot;get&quot;          &quot;glue&quot;         &quot;googl&quot;        &quot;hadoop&quot;      
 [57] &quot;header&quot;       &quot;hing&quot;         &quot;how&quot;          &quot;i&quot;            &quot;if&quot;           &quot;imag&quot;         &quot;in&quot;           &quot;is&quot;          
 [65] &quot;it&quot;           &quot;laptop&quot;       &quot;late&quot;         &quot;lcd&quot;          &quot;lid&quot;          &quot;link&quot;         &quot;logo&quot;         &quot;loos&quot;        
 [73] &quot;mac&quot;          &quot;macbook&quot;      &quot;make&quot;         &quot;mobil&quot;        &quot;modifi&quot;       &quot;much&quot;         &quot;my&quot;           &quot;navig&quot;       
 [81] &quot;of&quot;           &quot;off&quot;          &quot;omnitur&quot;      &quot;on&quot;           &quot;page&quot;         &quot;permalink&quot;    &quot;php&quot;          &quot;post&quot;        
 [89] &quot;power&quot;        &quot;pro&quot;          &quot;problem&quot;      &quot;program&quot;      &quot;proud&quot;        &quot;r&quot;            &quot;remov&quot;        &quot;repair&quot;      
 [97] &quot;replac&quot;       &quot;report&quot;       &quot;sas&quot;          &quot;screen&quot;       &quot;separ&quot;        &quot;site&quot;         &quot;sitecatalyst&quot; &quot;store&quot;       
[105] &quot;tag&quot;          &quot;text&quot;         &quot;the&quot;          &quot;theme&quot;        &quot;this&quot;         &quot;tighten&quot;      &quot;to&quot;           &quot;top&quot;         
[113] &quot;turn&quot;         &quot;twenti&quot;       &quot;twentyeleven&quot; &quot;uncategor&quot;    &quot;unibodi&quot;      &quot;use&quot;          &quot;variabl&quot;      &quot;version&quot;     
[121] &quot;view&quot;         &quot;vs&quot;           &quot;warranti&quot;     &quot;was&quot;          &quot;what&quot;         &quot;will&quot;         &quot;with&quot;         &quot;wordpress&quot;   
[129] &quot;wp&quot;           &quot;you&quot;    
{% endhighlight %}

## Guessing at 'k': A First Run at Clustering

Once we have our data set up, we can very quickly run the k-means algorithm within R. The one downside to using k-means clustering as a technique is that the user must choose 'k', the number of clusters expected from the dataset. In absence of any heuristics about what 'k' to use, I can guess that there are five topics on this blog:
1. Data Science
2. Digital Analytics  
3. R
4. Julia
5. WordPress

Running the following code, we can see if the algorithm agrees:

{% highlight r linenos %}
#I think there are 5 main topics: Data Science, Web Analytics, R, Julia, Wordpress
kmeans5&lt;- kmeans(dtm, 5)

#Merge cluster assignment back to keywords
kw_with_cluster &lt;- as.data.frame(cbind(searchkeywords$'Natural Search Keyword', kmeans5$cluster))
names(kw_with_cluster) &lt;- c(&quot;keyword&quot;, &quot;kmeans5&quot;)

#Make df for each cluster result, quickly &quot;eyeball&quot; results
cluster1 &lt;- subset(kw_with_cluster, subset=kmeans5 == 1)
cluster2 &lt;- subset(kw_with_cluster, subset=kmeans5 == 2)
cluster3 &lt;- subset(kw_with_cluster, subset=kmeans5 == 3)
cluster4 &lt;- subset(kw_with_cluster, subset=kmeans5 == 4)
cluster5 &lt;- subset(kw_with_cluster, subset=kmeans5 == 5)
{% endhighlight %}

Opening the dataframes to observe the results, it seems that the algorithm disagrees:

  * Cluster 1: &quot;Free-for-All&quot; cluster: not well separated (41.1% of terms)
  * Cluster 2: &quot;wordpress&quot; and &quot;remove&quot; (4.9% of terms)
  * Cluster 3: &quot;powered by wordpress&quot; (4.3% of terms)
  * Cluster 4: &quot;twenty eleven&quot; (13.5% of terms)
  * Cluster 5: &quot;macbook&quot; (36.2% of terms)

Of the clusters, the strongest cluster in terms of performance is cluster 5, which is pretty homogenous in terms of being about 'macbook' terms. Clusters 2-4 are all about WordPress, albeit different topics surrounding blogging. And cluster 1 is a large hodge-podge of terms that seem unrelated. Clearly, five clusters isn't the proper value for 'k'.   

## Selecting 'k' Using 'Elbow Method'

Instead of randomly choosing values of 'k', then looking at each cluster result until we find one we like, we can take a more automated approach to picking 'k'. For every `kmeans` object returned by R, there is a metric `tot.withinss` that provides the total of the squared distance metric for each cluster.

{% highlight r linenos %}
#accumulator for cost results
cost_df &lt;- data.frame()

#run kmeans for all clusters up to 100
for(i in 1:100){
  #Run kmeans for each level of i, allowing up to 100 iterations for convergence
  kmeans&lt;- kmeans(x=dtm, centers=i, iter.max=100)

  #Combine cluster number and cost together, write to df
  cost_df&lt;- rbind(cost_df, cbind(i, kmeans$tot.withinss))

}
names(cost_df) &lt;- c(&quot;cluster&quot;, &quot;cost&quot;)
{% endhighlight %}

The `cost_df` dataframe accumulates the results for each run, which can then be plotted using ggplot2 (&lt;a title=&quot;ggplot2 k-means elbow method gist&quot; href=&quot;https://gist.github.com/randyzwitch/6597905&quot; target=&quot;_blank&quot;&gt;ggplot2 Gist here&lt;/a&gt;):

![elbow-plot](/wp-content/uploads/2013/09/elbow-plot.png)

The plot above is a technique known informally as the 'elbow method', where we are looking for breakpoints in our cost plot to understand where we should stop adding clusters. We can see that the slope of the cost function gets flatter at 10 clusters, then flatter again around 20 clusters. This means that as we add clusters above 10 (or 20), each additional cluster becomes less effective at reducing the distance from the each data center (i.e. reduces the variance less). So while we haven't determined an absolute, single 'best' value of 'k', we have narrowed down a range of values for 'k' to evaluate.

Ultimately, the best value of 'k' will be determined as a combination of a heuristic method like the 'Elbow Method', along with analyst judgement after looking at the results. Once you've determined your optimal cluster definitions, it's trivial to calculate metrics such as Bounce Rate, Pageviews per Visit, Conversion Rate or Average Order Value to see how well the clusters actually describe different behaviors on-site.

## Summary

K-means clustering is one of many unsupervised learning techniques that can be used to understand the underlying structure of a dataset. When used with text data, k-means clustering can provide a great way to organize the thousands-to-millions of words being used by your customers to describe their visits. Once you understand what your customers are trying to do, you can tailor your on-site experiences to match these needs, as well as adjusting your reporting/dashboards to monitor the various customer groups.

_EDIT: For those who want to play around with the code but don't use Adobe Analytics, here is the &lt;a title=&quot;search keyword file&quot; href=&quot;http://randyzwitch.com/wp-content/uploads/2013/09/searchkeywords_0913.csv&quot; target=&quot;_blank&quot;&gt;file of search keywords&lt;/a&gt; I used. Once you read in the .csv file into a dataframe and name it searchkeywords, you should be able to replicate everything in this blog post._</content>
      </item>
      
    
      
      <item>
        <title>Fun With Just-In-Time Compiling: Julia, Python, R and pqR</title>
        
          <description>&lt;p&gt;Recently I’ve been spending a lot of time trying to learn &lt;a title=&quot;Julia language&quot; href=&quot;http://julialang.org/&quot; target=&quot;_blank&quot;&gt;Julia&lt;/a&gt; by doing the problems at &lt;a title=&quot;Project Euler&quot; href=&quot;http://projecteuler.net/&quot; target=&quot;_blank&quot;&gt;Project Euler&lt;/a&gt;. What’s great about these problems is that it gets me out of my normal design patterns, since I don’t generally think about prime numbers, factorials and other number theory problems during my normal workday. These problems have also given me the opportunity to really think about how computers work, since Julia allows the programmer to pass type declarations to the just-in-time compiler (JIT).&lt;/p&gt;

</description>
        
        <pubDate>Mon, 02 Sep 2013 15:57:45 -0400</pubDate>
        <link>
        http://randyzwitch.com/python-pypy-julia-r-pqr-jit-just-in-time-compiler/</link>
        <guid isPermaLink="true">http://randyzwitch.com/python-pypy-julia-r-pqr-jit-just-in-time-compiler/</guid>
        <content type="html" xml:base="/python-pypy-julia-r-pqr-jit-just-in-time-compiler/">Recently I've been spending a lot of time trying to learn &lt;a title=&quot;Julia language&quot; href=&quot;http://julialang.org/&quot; target=&quot;_blank&quot;&gt;Julia&lt;/a&gt; by doing the problems at &lt;a title=&quot;Project Euler&quot; href=&quot;http://projecteuler.net/&quot; target=&quot;_blank&quot;&gt;Project Euler&lt;/a&gt;. What's great about these problems is that it gets me out of my normal design patterns, since I don't generally think about prime numbers, factorials and other number theory problems during my normal workday. These problems have also given me the opportunity to really think about how computers work, since Julia allows the programmer to pass type declarations to the just-in-time compiler (JIT).

As I've been working on optimizing my Julia code, I decided to figure out how fast this problem can be solved using any of the languages/techniques I know. So I decided to benchmark one of the Project Euler problems using &lt;a title=&quot;Julia language&quot; href=&quot;http://julialang.org/&quot; target=&quot;_blank&quot;&gt;Julia&lt;/a&gt;, &lt;a title=&quot;Python language&quot; href=&quot;http://python.org/&quot; target=&quot;_blank&quot;&gt;Python&lt;/a&gt;, &lt;a title=&quot;Numba&quot; href=&quot;http://numba.pydata.org/&quot; target=&quot;_blank&quot;&gt;Python with Numba&lt;/a&gt;, &lt;a title=&quot;Pypy&quot; href=&quot;http://pypy.org/&quot; target=&quot;_blank&quot;&gt;PyPy&lt;/a&gt;, &lt;a title=&quot;R&quot; href=&quot;http://cran.us.r-project.org/&quot; target=&quot;_blank&quot;&gt;R&lt;/a&gt;, R using the &lt;a title=&quot;R compiler&quot; href=&quot;http://stat.ethz.ch/R-manual/R-devel/library/compiler/html/compile.html&quot; target=&quot;_blank&quot;&gt;compiler&lt;/a&gt; package, &lt;a title=&quot;pqR&quot; href=&quot;http://radfordneal.wordpress.com/2013/06/22/announcing-pqr-a-faster-version-of-r/&quot; target=&quot;_blank&quot;&gt;pqR&lt;/a&gt; and pqR using the compiler package. Here's what I found...

## Problem

The problem I'm using for the benchmark is calculating the smallest number that is divisible by all of the numbers in a factorial. For example, for the numbers in `5!`, 60 is the smallest number that is divisible by 2, 3, 4 and 5. Here's the Julia code:

{% highlight julia linenos %}
function smallestdivisall(n::Int64)
    for i = 1:factorial(n)
        for j = 1:n
            if i % j !=0
                break
            elseif j == n
                return i
            end
        end
    end
end
{% endhighlight %}

All code versions follow this same pattern: the outside loop will run from `1` up to `n!`, since by definition the last value in the loop will be divisible by all of the numbers in the factorial. The inner loops go through and do a modulo calculation, checking to see if there is a remainder after division. If there is a remainder, break out of the loop and move to the next number. Once the state occurs where there is no remainder on the modulo calculation and the inner loop value of j equals the last number in the factorial (i.e. it is divisible by all of the factorial numbers), we have found the minimum number.

## Benchmarking - Overall

Here are the results of the eight permutations of languages/techniques (see &lt;a title=&quot;GitHub Gist for JIT test&quot; href=&quot;https://gist.github.com/randyzwitch/6341926&quot; target=&quot;_blank&quot;&gt;this&lt;/a&gt; GitHub Gist for the actual code used, &lt;a title=&quot;compiler results&quot; href=&quot;http://randyzwitch.com/wp-content/uploads/2013/09/jit.csv&quot; target=&quot;_blank&quot;&gt;this link&lt;/a&gt; for results file, and &lt;a title=&quot;ggplot2 code&quot; href=&quot;https://gist.github.com/randyzwitch/6414244&quot; target=&quot;_blank&quot;&gt;this&lt;/a&gt; GitHub Gist for the ggplot2 code):

![jit-comparison](/wp-content/uploads/2013/08/jit-comparison.png)

Across the range of tests from `5!` to `20!`, Julia is the fastest to find the minimum number. Python with Numba is second and PyPy is third. pqR fares better than R in general, but using the compiler package can narrow the gap.

To make more useful comparisons, in the next section I'll compare each language to its &quot;compiled&quot; function state.

## Benchmarking - Individual

### Python

![JITpython](/wp-content/uploads/2013/09/JITpython-e1378131849775.png)

Amongst the native Python code options, I saw a 16x speedup by using PyPy instead of Python 2.7.6 (10.62s vs. 172.06s at `20!`). Using Numba with Python instead of PyPy nets an _incremental_ ~40% speedup using the &lt;a title=&quot;autojit example&quot; href=&quot;http://numba.pydata.org/&quot; target=&quot;_blank&quot;&gt;`@autojit`&lt;/a&gt; decorator (7.63s vs. 10.63 at 20!).

So in the case of Python, using two lines of code with the Numba JIT compiler you can get substantial improvements in performance without needing to do any code re-writes. This is a great benefit given that you can stay in native Python, since PyPy doesn't support all existing packages within the Python ecosystem.

### R/pqR

![JITr](/wp-content/uploads/2013/09/JITr-e1378132951124.png)

It's understood in the R community that &lt;a title=&quot;Why are R loops slow?&quot; href=&quot;http://stackoverflow.com/questions/7142767/why-are-loops-slow-in-r&quot; target=&quot;_blank&quot;&gt;loops are not a strong point&lt;/a&gt; of the language. In the case of this problem, I decided to use loops because 1) it keeps the code pattern similar across languages and 2) I hoped I'd see the max benefit from the compiler package by not trying any funky R optimizations up front.

As expected, pqR is generally faster than R and using the compiler package is faster than not using the compiler. I saw ~30% improvement using pqR relative to R and ~20% _incremental_ improvement using the compiler package with pqR. Using the compiler package within R showed ~35% improvement.

So unlike the case with Python, where you could just use Python with Numba and stay within the same language/environment, if you can use pqR _and_ the compiler package, you can get a performance benefit from using both.

## Summary

For a comparison like I've done above, it's easy to get carried away and extrapolate the results from one simple test to all programming problems ever. &quot;_Julia is the best language for all cases ever!!!11111eleventy!_&quot; would be easy to proclaim, but all problems aren't looping problems using simple division. Once you get into writing longer programs, other tasks such string manipulation and accessing APIs, using a technique from a package only available in one ecosystem but not another, etc., which tool is &quot;best&quot; for solving a problem becomes a much more difficult decision. The only way to know how much improvement you can see from different techniques &amp; tools is to profile your program(s) and experiment.

The main thing that I took away from this exercise is that no matter which tool you are comfortable with to do analysis, there are potentially large performance improvements that can be made _just_ by using a JIT without needing to dramatically re-write your code. For those of us who don't know C (and/or are too lazy to re-write our code several times to wring out a little extra performance), that's a great thing.</content>
      </item>
      
    
      
      <item>
        <title>RSiteCatalyst Version 1.1 Release Notes</title>
        
          <description>&lt;p&gt;&lt;a title=&quot;RSiteCatalyst on CRAN&quot; href=&quot;http://cran.r-project.org/web/packages/RSiteCatalyst/index.html&quot; target=&quot;_blank&quot;&gt;RSiteCatalyst &lt;/a&gt;version 1.1 is now available on CRAN. Changes from version 1 include:&lt;/p&gt;

</description>
        
        <pubDate>Sun, 25 Aug 2013 09:54:32 -0400</pubDate>
        <link>
        http://randyzwitch.com/rsitecatalyst-version-1-1-release-notes/</link>
        <guid isPermaLink="true">http://randyzwitch.com/rsitecatalyst-version-1-1-release-notes/</guid>
        <content type="html" xml:base="/rsitecatalyst-version-1-1-release-notes/">&lt;a title=&quot;RSiteCatalyst on CRAN&quot; href=&quot;http://cran.r-project.org/web/packages/RSiteCatalyst/index.html&quot; target=&quot;_blank&quot;&gt;RSiteCatalyst &lt;/a&gt;version 1.1 is now available on CRAN. Changes from version 1 include:

  * Support for Correlations/Subrelations in the `QueueRanked` function
  * Support for Current Data in all `Queue*` functions
  * Support Anomaly Detection for `QueueOvertime` and `QueueTrended` functions (&lt;a title=&quot;Anomaly Detection Adobe Analytics&quot; href=&quot;http://randyzwitch.com/anomaly-detection-adobe-analytics-api/&quot; target=&quot;_blank&quot;&gt;example usage with ggplot2 graph&lt;/a&gt;)
  * Decrease in wait time for API calls (from 5 seconds to 2 seconds) and extending total number of API tries before report failure (from 100 seconds to 10 minutes)

For those of you Adobe Analytics (Omniture) users who haven't yet tried to use the Adobe Analytics API, I've created an &lt;a title=&quot;RSiteCatalyst main page&quot; href=&quot;http://randyzwitch.com/rsitecatalyst/&quot; target=&quot;_blank&quot;&gt;introduction video&lt;/a&gt; to get started. There will also continue to be examples of using this package on this blog on the &lt;a title=&quot;RSiteCatalyst usage examples&quot; href=&quot;http://randyzwitch.com/tag/rsitecatalyst/&quot; target=&quot;_blank&quot;&gt;RSiteCatalyst&lt;/a&gt; tag. Enjoy!</content>
      </item>
      
    
      
      <item>
        <title>Getting Started Using Hadoop, Part 4: Creating Tables With Hive</title>
        
          <description>&lt;p&gt;In the previous three tutorials (&lt;a title=&quot;Hadoop for beginners&quot; href=&quot;http://randyzwitch.com/big-data-hadoop-amazon-ec2-cloudera-part-1/&quot; target=&quot;_blank&quot;&gt;1&lt;/a&gt;, &lt;a title=&quot;Building Hadoop cluster on Amazon EC2&quot; href=&quot;http://randyzwitch.com/big-data-hadoop-amazon-ec2-cloudera-part-2/&quot; target=&quot;_blank&quot;&gt;2&lt;/a&gt;, &lt;a title=&quot;Loading data Hadoop Hue&quot; href=&quot;http://randyzwitch.com/uploading-data-hadoop-amazon-ec2-cloudera-part-3/&quot; target=&quot;_blank&quot;&gt;3&lt;/a&gt;), we’ve covered the background of Hadoop, how to build a proof-of-concept Hadoop cluster using Amazon EC2 and how to upload a .zip file to the cluster using Hue. In Part 4, we’ll use the data uploaded from the .zip file to create a master table of all files, as well as creating a view.&lt;/p&gt;

</description>
        
        <pubDate>Thu, 22 Aug 2013 14:03:19 -0400</pubDate>
        <link>
        http://randyzwitch.com/hadoop-creating-tables-hive/</link>
        <guid isPermaLink="true">http://randyzwitch.com/hadoop-creating-tables-hive/</guid>
        <content type="html" xml:base="/hadoop-creating-tables-hive/">In the previous three tutorials (&lt;a title=&quot;Hadoop for beginners&quot; href=&quot;http://randyzwitch.com/big-data-hadoop-amazon-ec2-cloudera-part-1/&quot; target=&quot;_blank&quot;&gt;1&lt;/a&gt;, &lt;a title=&quot;Building Hadoop cluster on Amazon EC2&quot; href=&quot;http://randyzwitch.com/big-data-hadoop-amazon-ec2-cloudera-part-2/&quot; target=&quot;_blank&quot;&gt;2&lt;/a&gt;, &lt;a title=&quot;Loading data Hadoop Hue&quot; href=&quot;http://randyzwitch.com/uploading-data-hadoop-amazon-ec2-cloudera-part-3/&quot; target=&quot;_blank&quot;&gt;3&lt;/a&gt;), we've covered the background of Hadoop, how to build a proof-of-concept Hadoop cluster using Amazon EC2 and how to upload a .zip file to the cluster using Hue. In Part 4, we'll use the data uploaded from the .zip file to create a master table of all files, as well as creating a view.

## Creating Tables Using Hive

Like SQL for 'regular' relational databases, Hive is the tool we can use within Hadoop to create tables from data loaded into HDFS. Because Hadoop was built with large, messy data in mind, there are some amazingly convenient features for creating and loading data, such as being able to load all files in a directory (assuming they have the same format).  Here's the Hive statement we can use to load the &lt;a title=&quot;airline dataset&quot; href=&quot;http://stat-computing.org/dataexpo/2009/the-data.html&quot; target=&quot;_blank&quot;&gt;airline dataset&lt;/a&gt;:

{% highlight sql linenos %}
-- Create table from yearly airline csv files
CREATE EXTERNAL TABLE airline (
  `Year` int,
  `Month` int,
  `DayofMonth` int,
  `DayOfWeek` int,
  `DepTime`  int,
  `CRSDepTime` int,
  `ArrTime` int,
  `CRSArrTime` int,
  `UniqueCarrier` string,
  `FlightNum` int,
  `TailNum` string,
  `ActualElapsedTime` int,
  `CRSElapsedTime` int,
  `AirTime` int,
  `ArrDelay` int,
  `DepDelay` int,
  `Origin` string,
  `Dest` string,
  `Distance` int,
  `TaxiIn` int,
  `TaxiOut` int,
  `Cancelled` int,
  `CancellationCode` string,
  `Diverted` string,
  `CarrierDelay` int,
  `WeatherDelay` int,
  `NASDelay` int,
  `SecurityDelay` int,
  `LateAircraftDelay` int)
ROW FORMAT DELIMITED
FIELDS TERMINATED BY ','
ESCAPED BY '\\'
STORED AS TEXTFILE
LOCATION '/user/hue/airline/airline';
{% endhighlight %}

The above statement starts by outlining the structure of the table, which is mostly integers with a few string columns. The next four lines of code specifies what type of data we have, which are delimited files where the fields are terminated (separated) by commas and where the delimiter is escaped using a backslash. Finally, we type the location of our files, which is the location of the directory where we uploaded the .zip file in &lt;a title=&quot;Part 3&quot; href=&quot;http://randyzwitch.com/uploading-data-hadoop-amazon-ec2-cloudera-part-3/&quot; target=&quot;_blank&quot;&gt;part 3 of this tutorial&lt;/a&gt;. Note that we specify an &quot;external table&quot;, which means that if we drop the 'airline' table, we will still retain our raw data. Had we not specified the `external` keyword, Hive would've moved our raw data files into Hive, and had we decided to drop the 'airline' table, all our data would be deleted. Specifying `external` also lets us build multiple tables on the same underlying dataset if we so choose.

## Creating a View Using Hive

One thing that's slightly awkward above Hive is that you can't specify that there is a header row in your files. As such, once the above code loads, we have 22 rows in our 'airline' table where the data is invalid. Another thing that's awkward about Hive is that there is no row-level operations, so you can't delete data! However, we can very easily fix our problem using a view:

{% highlight sql linenos %}
-- Create view to &quot;remove&quot; 22 bad records from our table
create view vw_airline as
select * from airline
where uniquecarrier &lt;&gt; &quot;UniqueCarrier&quot;;
{% endhighlight %}

Now that we have our view defined, we no longer have to explicitly exclude the rows in every future query we run. Just like in SQL, views are &quot;free&quot; from a performance standpoint, as they don't require any additional data storage space (they just represent stored code references).

## Time for Analysis?!

If you've made it this far, you've waited a long time to do some actual analysis! The next and final part of this tutorial will do some interesting things using Hive and/or Pig to analyze the data. The origin of this dataset was a data mining contest to predict why a flight would arrive late to its destination and we'll do examples towards that end.</content>
      </item>
      
    
      
      <item>
        <title>Anomaly Detection Using The Adobe Analytics API</title>
        
          <description>&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2013/08/anomaly-detection-adobe-analytics.jpg&quot; alt=&quot;anomaly-detection-adobe-analytics&quot; /&gt;&lt;/p&gt;

</description>
        
        <pubDate>Thu, 15 Aug 2013 06:57:56 -0400</pubDate>
        <link>
        http://randyzwitch.com/anomaly-detection-adobe-analytics-api/</link>
        <guid isPermaLink="true">http://randyzwitch.com/anomaly-detection-adobe-analytics-api/</guid>
        <content type="html" xml:base="/anomaly-detection-adobe-analytics-api/">![anomaly-detection-adobe-analytics](/wp-content/uploads/2013/08/anomaly-detection-adobe-analytics.jpg)

As digital marketers &amp; analysts, we're often asked to quantify when a metric goes beyond just random variation and becomes an actual &quot;unexpected&quot; result. In cases such as _A/B..N_ testing, it's easy to calculate a t-test to quantify the difference between two testing populations, but &lt;a title=&quot;Why t-test not appropriate for time-series&quot; href=&quot;http://www.indiana.edu/~statmath/stat/all/ttest/ttest1.html&quot; target=&quot;_blank&quot;&gt;for time-series metrics, using a t-test is likely not appropriate&lt;/a&gt;.

To determine whether a time-series has become &quot;out-of-control&quot;, we can use Exponential Smoothing to forecast the Expected Value, as well as calculate Upper Control Limits (UCL) and Lower Control Limits (LCL). To the extent a data point exceeds the UCL or falls below the LCL, we can say that statistically a time-series is no longer within the expected range. There are numerous ways to &lt;a title=&quot;R time-series&quot; href=&quot;http://cran.r-project.org/web/views/TimeSeries.html&quot; target=&quot;_blank&quot;&gt;create time-series models using R&lt;/a&gt;, but for the purposes of this blog post I'm going to focus on Exponential Smoothing, which is how the &lt;a title=&quot;Anomaly Detection Adobe Analytics API&quot; href=&quot;https://developer.omniture.com/en_US/documentation/sitecatalyst-reporting/c-anomaly#concept_E51D14B9899A4974BD946A77D7368BC5&quot; target=&quot;_blank&quot;&gt;anomaly detection&lt;/a&gt; feature is implemented within the Adobe Analytics API.

### Holt-Winters &amp; Exponential Smoothing

There are three techniques that the Adobe Analytics API uses to build time-series models:

  * Holt-Winters Additive (Triple Exponential Smoothing)
  * Holt-Winters Multiplicative (Triple Exponential Smoothing)
  * Holt Trend Corrected (Double Exponential Smoothing)

The formulas behind each of the techniques are easily found elsewhere, but the main point behind the three techniques is that time-series data can have a &lt;span style=&quot;text-decoration: underline;&quot;&gt;long-term trend&lt;/span&gt; (Double Exponential Smoothing) and/or a &lt;span style=&quot;text-decoration: underline;&quot;&gt;seasonal trend&lt;/span&gt; (Triple Exponential Smoothing). To the extent that a time-series  has a seasonal component, the seasonal component can be _additive_ (a fixed amount of increase across the series, such as the number of degrees increase in temperature in Summer) or _multiplicative_ (a multiplier relative to the level of the series, such as a 10% increase in sales during holiday periods).

The Adobe Analytics API simplifies the choice of which technique to use by calculating a forecast using all three methods, then choosing the method that has the best fit as calculated by the model having the minimum (squared) error. It's important to note that while this is probably an okay model selection method for detecting anomalies, this method does not guarantee that the model chosen is the actual &quot;best&quot; forecast model to fit the data.

### RSiteCatalyst API call

Using the RSiteCatalyst R package version 1.1, it's trivial to access the anomaly detection feature:

{% highlight R linenos %}
#Run until version &gt; 1.0 on CRAN
library(devtools)
install_github(&quot;RSiteCatalyst&quot;, &quot;randyzwitch&quot;, ref = &quot;master&quot;)

#Run if version &gt;= 1.1 on CRAN
library(&quot;RSiteCatalyst&quot;)

#API Authentication
SCAuth(&lt;username:company&gt;, &lt;shared_secret&gt;)

#API function call
pageviews_w_forecast &lt;- QueueOvertime(reportSuiteID=&lt;report suite&gt;, dateFrom = &quot;2013-06-01&quot;, dateTo=&quot;2013-08-13&quot;, metrics = &quot;pageviews&quot;, dateGranularity=&quot;day&quot;, anomalyDetection=&quot;1&quot;)
{% endhighlight %}

Once the function call is run, you will receive a DataFrame of 'Day' granularity with the actual metric and three additional columns for the forecasted value, UCL and LCL.  Graphing these data using ggplot2 (&lt;a title=&quot;ggplot2 Anomaly Detection graph&quot; href=&quot;https://gist.github.com/randyzwitch/6241051&quot; target=&quot;_blank&quot;&gt;Graph Code Here - GitHub Gist&lt;/a&gt;), we can now see on which days an anomalous result occurred:

![Huge spike in traffic July 23 - 24](/wp-content/uploads/2013/08/anomaly-detection-adobe-analytics1.png)

The red dots in the graph above indicate days where page views either exceeded the UCL or fell below the LCL. On July 23 - 24 timeframe, traffic to this blog spiked dramatically due to a &lt;a title=&quot;A Beginner’s Look at Julia&quot; href=&quot;http://randyzwitch.com/julia-language-beginners/&quot; target=&quot;_blank&quot;&gt;blog post about the Julia programming language&lt;/a&gt;, and continued to stay above the norm for about a week afterwards.

### Anomaly Detection Limitations

There two limitations to keep in mind when using the Anomaly Detection feature of the Adobe Analytics API:

  * Anomaly Detection is currently only available for 'Day' granularity
  * Forecasts are built on 35 days of past history

In neither case do I view these limitations as dealbreakers. The first limitation is just an engineering decision, which I'm sure could be expanded if enough people used this functionality.

For the time period of 35 days to build the forecasts, this is an area where there is a balance between calculation time vs. capturing a long-term and/or seasonal trend in the data. Using 35 days as your time period, you get five weeks of day-of-week seasonality, as well as 35 points to calculate a 'long-term' trend. If the time period is of concern in terms of what constitutes a 'good forecast', then there are plenty of other techniques that can be explored using R (or any other statistical software for that matter).

### Elevating the discussion

I have to give a hearty 'Well Done!' to the Adobe Analytics folks for elevating the discussion in terms of digital analytics. By using statistical techniques like Exponential Smoothing, analysts can move away from qualitative statements like &quot;Does it look like _something_ is wrong with our data?&quot; to  actually quantifying _when_ KPIs are &quot;too far&quot; away from the norm and should be explored further.</content>
      </item>
      
    
      
      <item>
        <title>Tabular Data I/O in Julia</title>
        
          <description>&lt;p&gt;Importing tabular data into Julia can be done in (at least) three ways: reading a delimited file into an array, reading a delimited file into a DataFrame and accessing databases using ODBC.&lt;/p&gt;

</description>
        
        <pubDate>Tue, 06 Aug 2013 06:05:38 -0400</pubDate>
        <link>
        http://randyzwitch.com/julia-import-data/</link>
        <guid isPermaLink="true">http://randyzwitch.com/julia-import-data/</guid>
        <content type="html" xml:base="/julia-import-data/">Importing tabular data into Julia can be done in (at least) three ways: reading a delimited file into an array, reading a delimited file into a DataFrame and accessing databases using ODBC.

### Reading a file into an array using readdlm

The most basic way to read data into Julia is through the use of the `readdlm` function, which will create an array:

{% highlight julia linenos %}
readdlm(source, delim::Char, T::Type; options...)
{% endhighlight %}

If you are reading in a fairly normal delimited file, you can get away with just using the first two arguments, `source` and `delim`:

{% highlight julia linenos %}
julia&gt; airline_array = readdlm(&quot;/Users/randyzwitch/airline/1987.csv&quot;, ',');

julia&gt; size(airline_array)
(1311827,29)

julia&gt; typeof(airline_array)
Array{Any,2}
{% endhighlight %}

It's important to note that by only specifying the first two arguments, you leave it up to Julia to determine the type of array to return. In the code example above, an array of type `Any` is returned, as the .csv file I read in was not of homogenous type such as `Int64` or &lt;del&gt;ASCII&lt;/del&gt;`String`. If you know for certain which type of array you want, you specify the data type using the `type` argument:

{% highlight julia linenos %}
julia&gt; airline_array = readdlm(&quot;/Users/randyzwitch/airline/1987.csv&quot;, ',' , String);

julia&gt; size(airline_array)
(1311827,29)

julia&gt; typeof(airline_array)
Array{String,2}
{% endhighlight %}

It's probably the case that unless you are looking to do linear algebra or other specific mathy type work, you'll likely find that reading your data into a DataFrame will be more comfortable to work with (especially if you are coming from an R, Python/pandas or even spreadsheet tradition).

To write an array out to a file, you can use the `writedlm` function (defaults to comma-separated):

{% highlight julia linenos %}
writedlm(filename, array, delim::Char)
{% endhighlight %}

### Reading a file into a DataFrame using readtable

As I covered in my prior blog post about Julia, you can also &lt;a title=&quot;Julia for Beginners&quot; href=&quot;http://randyzwitch.com/julia-language-beginners/&quot; target=&quot;_blank&quot;&gt;read in delimited files into Julia using the DataFrames package&lt;/a&gt;, which returns a DataFrame instead of an array. Besides just being able to read in delimited files, the DataFrames package also supports reading in gzippped files on the fly:

{% highlight julia linenos %}
julia&gt; using DataFrames

julia&gt; airline_df = readtable(&quot;/Users/randyzwitch/airline/1987.csv.gz&quot;);

julia&gt; size(airline_df)
(1311826,29)

julia&gt; typeof(airline_df)
DataFrame  (use methods(DataFrame) to see constructors)
{% endhighlight %}

From what I understand, in the future you will be able to read files directly from Amazon S3 into a DataFrame (this is already supported in the &lt;a title=&quot;Julia Amazon S3&quot; href=&quot;https://github.com/amitmurthy/AWS.jl&quot; target=&quot;_blank&quot;&gt;AWS package&lt;/a&gt;), but for now, the DataFrames package works only on local files. Writing a DataFrame to file can be done with the `writetable` function:

{% highlight julia linenos %}
writetable(filename::String, df::DataFrame)
{% endhighlight %}

By default, the &lt;a title=&quot;Julia DataFrames&quot; href=&quot;http://juliastats.github.io/DataFrames.jl/io.html&quot; target=&quot;_blank&quot;&gt;`writetable` function&lt;/a&gt; will use the delimiter specified by the filename extension and default to printing the column names as a header.

### Accessing Databases using ODBC

The third major way of importing tabular data into Julia is through the use of ODBC access to various databases such as MySQL and PostgreSQL.

#### Using a DSN

The &lt;a title=&quot;Julia ODBC package&quot; href=&quot;https://github.com/karbarcca/ODBC.jl&quot; target=&quot;_blank&quot;&gt;Julia ODBC package&lt;/a&gt; provides functionality to connect to a database using a Data Source Name (DSN). Assuming you store all the credentials in your DSN (server name, username, password, etc.), connecting to a database is as easy as:

{% highlight julia linenos %}
julia&gt; using ODBC

julia&gt; ODBC.connect(&quot;MySQL&quot;)
Connection 1 to MySQL successful.
{% endhighlight %}

Of course, if you don't want to store your password in your DSN (especially in the case where there are multiple users for a computer), you can pass the `usr` and `pwd` arguments to the `ODBC.connect` function:

{% highlight julia linenos %}
ODBC.connect(dsn; usr=&quot;&quot;, pwd=&quot;&quot;)
{% endhighlight %}

#### Using a connection string

Alternatively, you can build your own connection strings within a Julia session using the `advancedconnect` function:

{% highlight julia linenos %}
#Amazon Redshift/Postgres connection string
Julia&gt; red = advancedconnect(&quot;Driver={psqlODBC};ServerName=reporting.XXXXX.us-east-1.redshift.amazonaws.com;Username=XXXX;Password=XXXX;Database=XXXX;Port=XXXX&quot;);
Connection 1 to Driver={psqlODBC};ServerName=reporting.XXXXX.us-east-1.redshift.amazonaws.com;Username=XXXX;Password=XXXX;Database=XXXX;Port=XXXX successful.

#MySQL connection string
julia&gt; my = advancedconnect(&quot;Driver={MySQL};user=root;server=localhost;database=airline;&quot;)
Connection 1 to Driver={MySQL};user=root;server=localhost;database=airline; successful.
{% endhighlight %}

Regardless of which way you connect, you can query data using the `query` function. If you want your output as a DataFrame, you can assign the result of the function to an object. If you want to save the results to a file, you specify the `file` argument:

{% highlight julia linenos %}
julia&gt; using ODBC

julia&gt; ODBC.connect(&quot;MySQL&quot;)
Connection 1 to MySQL successful.

#Save query results into a DataFrame called 'results'
julia&gt; results = query(&quot;Select * from a1987;&quot;);

julia&gt; typeof(results)
DataFrame  (use methods(DataFrame) to see constructors)

#Save query results to a file, tab-delimited (default)
julia&gt; query(&quot;Select * from a1987;&quot;, file=&quot;output.tab&quot;, delim = '\t');
{% endhighlight %}

### Summary

Overall, importing data into Julia is no easier/more difficult than any other language. The biggest thing I've noticed thus far is that Julia is a bit less efficient than Python/pandas or R in terms of the amount of RAM needed to store data. In my experience, this is really only an issue once you are working with 1GB+ files (of course, depending on the resources available to you on your machine).

_Edit 3/25/2016: A much more up-to-date method of &lt;a href=&quot;https://cbrownley.wordpress.com/2015/05/29/reading_writing_csv_with_r_python_julia/&quot; target=&quot;_blank&quot;&gt;reading CSV data into Julia&lt;/a&gt; can be found at this &lt;a href=&quot;https://cbrownley.wordpress.com/2015/05/29/reading_writing_csv_with_r_python_julia/&quot; target=&quot;_blank&quot;&gt;blog post by Clinton Brownley&lt;/a&gt;._</content>
      </item>
      
    
      
      <item>
        <title>Hadoop Streaming with Amazon Elastic MapReduce, Python and mrjob</title>
        
          <description>&lt;p&gt;In a previous rant about &lt;a title=&quot;Data Science &amp;amp; Innovation&quot; href=&quot;http://randyzwitch.com/data-science-innovation/&quot; target=&quot;_blank&quot;&gt;data science &amp;amp; innovation&lt;/a&gt;, I made reference to a problem I’m having at work where I wanted to classify roughly a quarter-billion URLs by predicted website content (without having to actually visit the website). A few colleagues have asked how you go about even starting to solve a problem like that, and the answer is &lt;em&gt;massively parallel processing&lt;/em&gt;.&lt;/p&gt;

</description>
        
        <pubDate>Wed, 31 Jul 2013 08:34:58 -0400</pubDate>
        <link>
        http://randyzwitch.com/amazon-elastic-map-reduce-mrjob-python/</link>
        <guid isPermaLink="true">http://randyzwitch.com/amazon-elastic-map-reduce-mrjob-python/</guid>
        <content type="html" xml:base="/amazon-elastic-map-reduce-mrjob-python/">In a previous rant about &lt;a title=&quot;Data Science &amp; Innovation&quot; href=&quot;http://randyzwitch.com/data-science-innovation/&quot; target=&quot;_blank&quot;&gt;data science &amp; innovation&lt;/a&gt;, I made reference to a problem I'm having at work where I wanted to classify roughly a quarter-billion URLs by predicted website content (without having to actually visit the website). A few colleagues have asked how you go about even starting to solve a problem like that, and the answer is _massively parallel processing_.

## Attacking the problem using a local machine

In order to classify the URLs, the first thing that's needed is a customized dictionary of words relative to our company's subject matter. When you have a corpus of words that are already defined (such as a digitized book), finding the population of words is relatively simple: split the text based on spaces &amp; punctuation and you're more or less done. However, with a URL, you have one continuous string with no word boundaries. One way to try and find the boundaries would be the following in Python:

{% highlight python linenos %}
import collections
import nltk

#Dictionary from Unix
internal_dict = open(&quot;/usr/share/dict/words&quot;)
#Stopwords corpus from NLTK
stopwords = nltk.corpus.stopwords.words('english')

#Build english_dictionary of prospect words
english_dictionary = []
for line in internal_dict:
    if line not in stopwords and len(line) &gt; 4:  #make sure only &quot;big&quot;, useful words included
        english_dictionary.append(line.rstrip('\n'))

#How many words are in the complete dictionary?        
len(english_dictionary)

#Import urls
urls = [line for line in open(&quot;/path/to/urls/file.csv&quot;)]

#Build counter dictionary
wordcount = collections.Counter()
for word in english_dictionary:    #Loop over all possible English words
  for url in urls:     #Loop over all urls in list
    if word in url:
      wordcount[word] += 1 #Once word found, add to dictionary counter
{% endhighlight %}

The problem with approaching the word searching problem in this manner is you are limited to the power of your local machine. In my case with a relatively new MacBook Pro, I can process 1,000 lines in 19 seconds as a single-threaded process. At 250,000,000 URLs, that's 4.75 million seconds...197,916 minutes...3,298 hours...137 days...**4.58 months! ** Of course, 4.58 months is for the data I have &lt;span style=&quot;text-decoration: underline;&quot;&gt;now&lt;/span&gt;, which is accumulating every second of every day. Clearly, to find just the custom dictionary of words, I'll need to employ MANY more computers/tasks.

## Amazon ElasticMapreduce = Lots of Horsepower

One thing you might notice about the Python code above is that the two loops have no real reason to be run serially; each comparison of URL and dictionary word can be run independently of each other (often referred to as &quot;&lt;a title=&quot;Embarassingly parallel&quot; href=&quot;http://english.stackexchange.com/questions/83677/what-is-embarrassing-about-an-embarrassingly-parallel-problem&quot; target=&quot;_blank&quot;&gt;embarrassingly parallel&lt;/a&gt;&quot;). This type of programming pattern is one that is well suited to running on a Hadoop cluster. With Amazon ElasticMapReduce (EMR), we can provision tens, hundreds, even thousands of computer instances to process this URL-dictionary word comparison, and thus getting our answer much faster. The one downside of using Amazon EMR to access Hadoop is that EMR expects to get a Java ``.jar` file containing your MapReduce code. Luckily, there is a Python package called &lt;a title=&quot;MRjob Python package&quot; href=&quot;http://pythonhosted.org/mrjob/&quot; target=&quot;_blank&quot;&gt;MRJob&lt;/a&gt; that does the Python-to-Java translation automatically, so that users don't have to switch languages to get massively parallel processing.

## Writing MapReduce code

The Python code above, keeping a tally of words &amp; number of occurrences IS a version of the MapReduce coding paradigm. Going through the looping process to do the comparison is the &quot;Map&quot; portion of the code and the sum of the word values is the &quot;Reduce&quot; step. However, in order to use EMR, we need to modify the above code to remove the outer URL loop:

{% highlight python linenos %}
from mrjob.job import MRJob

class MRWordCounter(MRJob):    

  def mapper(self, english_dict, line):
  english_dict = ['aal', 'aalii', 'aam', 'aani'...'zythum', 'zyzomys', 'zyzzogeton']

  for word in english_dict:
            if word in line:
                yield word, 1

  def reducer(self, word, occurrences):
        yield word, sum(occurrences)

if __name__ == '__main__':
    MRWordCounter.run()
{% endhighlight %}

The reason why we remove the outer loop that loops over the lines of the URL file is because that is implicit to the EMR/Hadoop style of processing. We will specify a file that we want to process in our Python script, then EMR will distribute the URLs file across all the Hadoop nodes. Essentially, our 250,000,000 million lines of URLs will become 1,000 tasks of length 250,000 urls (assuming 125 nodes of 8 tasks each).

### Calling EMR from the Python command line

Once we have our Python MRJob code written, we can submit our code to EMR from the command line. Here's what an example code looks like:

{% highlight shell linenos %}
python ~/Desktop/mapreduce.py -r emr s3://&lt;s3bucket&gt;/url_unload/0000_part_01 --output-dir=s3://&lt;s3bucket&gt;/url_output --num-ec2-instances=81
{% endhighlight %}

There are many more options that are possible for the MRJob package, so I highly suggest that you read the &lt;a title=&quot;MRJobs EMR options&quot; href=&quot;http://pythonhosted.org/mrjob/guides/emr-quickstart.html&quot; target=&quot;_blank&quot;&gt;documentation for EMR options&lt;/a&gt;. One thing to also note is that MRJob uses a configuration file to host various options for EMR called &quot;runners&quot;.  Yelp (the maker of the MRJob package) has posted an &lt;a title=&quot;MRJob .conf file&quot; href=&quot;https://github.com/Yelp/mrjob/blob/master/mrjob.conf.example&quot; target=&quot;_blank&quot;&gt;example of the mrjob.conf file&lt;/a&gt; with the most common options to use. In this file, you can specify your Amazon API keys, the type of instances you want to use (I use c1.xlarge spot instances for the most part), where your SSH keys are located and so on.

## Results

In terms of performance, I have 8 files of 5GB's each of URLs (~17.5 million lines per file) that I'm running through the MRJob code above. The first file was run with 19 c1.xlarge instances, creating on average 133 mappers and 65 reducers and taking 917 minutes (_3.14 seconds/1000 lines_).  The second file was run with 80 c1.xlarge instances, creating 560 mappers and 160 reducers and taking 218 minutes (_0.75 seconds/1000 lines_). So using four times as many instances leads to one-fourth of the run-time.

For the most part, you can expect linear performance in terms of adding nodes to your EMR cluster. I know at some point, Hadoop will decide that it no longer needs to add any more mappers/reducers, but I haven't had the desire to find out exactly how many I'd need to add to get to that point! 🙂</content>
      </item>
      
    
      
      <item>
        <title>A Beginner's Look at Julia</title>
        
          <description>&lt;p&gt;Over the past month or so, I’ve been playing with a new scientific programming language called ‘&lt;a title=&quot;Julia language&quot; href=&quot;http://julialang.org/&quot; target=&quot;_blank&quot;&gt;Julia&lt;/a&gt;’, which aims to be a high-level language with performance approaching that of C. With that goal in mind, Julia could be a replacement for the ‘multi-language’ problem of needing to move between R, Python, MATLAB, C, Fortran, Scala, etc. within a single scientific programming project.  Here are some observations that might be helpful for others looking to get started with Julia.&lt;/p&gt;

</description>
        
        <pubDate>Tue, 23 Jul 2013 08:16:34 -0400</pubDate>
        <link>
        http://randyzwitch.com/julia-language-beginners/</link>
        <guid isPermaLink="true">http://randyzwitch.com/julia-language-beginners/</guid>
        <content type="html" xml:base="/julia-language-beginners/">Over the past month or so, I've been playing with a new scientific programming language called '&lt;a title=&quot;Julia language&quot; href=&quot;http://julialang.org/&quot; target=&quot;_blank&quot;&gt;Julia&lt;/a&gt;', which aims to be a high-level language with performance approaching that of C. With that goal in mind, Julia could be a replacement for the 'multi-language' problem of needing to move between R, Python, MATLAB, C, Fortran, Scala, etc. within a single scientific programming project.  Here are some observations that might be helpful for others looking to get started with Julia.

### Get used to 'Git' and 'make'

While there are &lt;a title=&quot;Julia language downloads&quot; href=&quot;http://julialang.org/downloads/&quot; target=&quot;_blank&quot;&gt;pre-built binaries&lt;/a&gt; for Julia, due to the rapid pace of development, it's best to build Julia from source. To be able to keep up with the literally dozen code changes per day, you can clone the &lt;a title=&quot;Julia GitHub repo&quot; href=&quot;https://github.com/JuliaLang/julia&quot; target=&quot;_blank&quot;&gt;Julia GitHub repository&lt;/a&gt; to your local machine. If you use one of the &lt;a title=&quot;GitHub GUI downloads&quot; href=&quot;http://git-scm.com/downloads/guis&quot; target=&quot;_blank&quot;&gt;GitHub GUI's&lt;/a&gt;, this is as easy as hitting the 'Sync Branch' button to receive all of the newest code updates.

To install Julia, you need to compile the code. The instructions for each supported operating system are listed on the &lt;a title=&quot;Julia GitHub repo&quot; href=&quot;https://github.com/JuliaLang/julia&quot; target=&quot;_blank&quot;&gt;Julia GitHub page&lt;/a&gt;. For Mac users, use Terminal to navigate to the directory where you cloned Julia, then run the following command, where 'n' refers to the number of concurrent processes you want the compiler to use:

{% highlight julia linenos %}make -j n {% endhighlight %}

I use 8 concurrent processes on a 2013 MacBook Pro and it works pretty well. Certainly much faster than a single process. Note that the first time you run the `make` command, the build process will take much longer than successive builds, as Julia downloads all the required libraries needed. After the first build, you can just run the `make` command with a single process, as the code updates don't take very long to build.

Package management is also done via GitHub. To add &lt;a title=&quot;Julia packages&quot; href=&quot;http://docs.julialang.org/en/latest/packages/packagelist/&quot; target=&quot;_blank&quot;&gt;Julia packages&lt;/a&gt; to your install, you use the `Pkg.add()` function, with the package name in double-quotes.

### Julia code feels very familiar

#### Text file import

Although the &lt;a title=&quot;Julia documentation&quot; href=&quot;http://docs.julialang.org/en/latest/manual/introduction/&quot; target=&quot;_blank&quot;&gt;Julia documentation&lt;/a&gt; makes numerous references to MATLAB in terms of code similarity, Julia feels very familiar to me as an R and Python user. Take reading a .csv file into a dataframe and finding the dimensions of the resulting object

{% highlight julia linenos %}
#R: Read in 1987.csv from airline dataset into a dataframe
#No import statement needed to create a dataframe in R
airline1987 &lt;- read.csv(&quot;~/airline/1987.csv&quot;)
dim(airline1987)
[1] 1311826      29

#Python: use pandas to create a dataframe
import pandas as pd
airline1987 = pd.read_csv(&quot;/Users/randyzwitch/airline/1987.csv&quot;)
airline1987.shape
Out[7]: (1311826, 29)

#Julia: use DataFrames to create a dataframe
using DataFrames
airline1987 = readtable(&quot;/Users/randyzwitch/airline/1987.csv&quot;)
size(airline1987)
(1311826,29)
{% endhighlight %}

In each language, the basic syntax is to call a 'read' function, specify the .csv filename, then the defaults of the function read in a basic file. I also could've specified other keyword arguments, but for purposes of this example I kept it simple.

#### Looping

Looping in Julia is similar to other languages. Python requires proper spacing for each level of a loop, with a colon for each evaluated expression. And although you generally don't use many loops in R, to do so requires using parenthesis and brackets.

{% highlight julia linenos %}
#Python looping to create a term-frequency dictionary

from collections import Counter

term_freq = Counter()
for word in english_dictionary:
  for url in url_list:
    if word in url_list:
      term_freq[word] += 1

#Julia looping to create a term-frequency dictionary

term_freq=Dict{String, Int64}()
for word in english_dictionary
    for url in url_list
        if search(line, word) != (0:-1)
            term_freq[word]=get(term_freq,word,0)+1
        end
    end
end
{% endhighlight %}

If you're coming from a Python background, you can see that there's not a ton of difference between Python looping into a dictionary vs. Julia. The biggest differences are the use of the `end` control-flow word and that Julia doesn't currently have the convenience &quot;Counter&quot; object type. R doesn't natively have a dictionary type, but you can add a similar concept using the &lt;a title=&quot;CRAN hash package&quot; href=&quot;http://cran.r-project.org/web/packages/hash/&quot; target=&quot;_blank&quot;&gt;hash&lt;/a&gt; package.

#### Vectorization

While not required to achieve high performance, Julia also provides the &lt;a title=&quot;Is looping as a programming construct bad?&quot; href=&quot;http://slendrmeans.wordpress.com/2013/05/11/julia-loops/&quot; target=&quot;_blank&quot;&gt;functional programming construct of vectorization and list comprehensions&lt;/a&gt;. In R, you use the `*apply` family of functions instead of loops in order to &lt;a title=&quot;Functional programming in R&quot; href=&quot;https://github.com/hadley/devtools/wiki/Functional-programming&quot; target=&quot;_blank&quot;&gt;apply a function to multiple elements in a list&lt;/a&gt;. In Python, there are the `map` and `reduce` functions, but there is also the concept of list comprehensions. In Julia, both of the aforementioned functionalities are possible.

{% highlight julia linenos %}
#Cube every number from 1 to 100

#Python map function
cubes = map(lambda(x): x*x*x, range(1,100))

#Python list comprehension
cubes= [x*x*x for x in range(1,100)]

#R sapply function
cubes &lt;- sapply(seq(1,100), function(x) x*x*x)

#Julia map function
cubes = map((x)-&gt; x*x*x, [1:100])

#Julia list comprehension
cubes = [x*x*x for x in [1:100]]
{% endhighlight %}

In each case, the syntax is _just about_ the same to apply a function across a list/array of numbers.

### A small, but intense community

One thing that's important to note about Julia at this stage is that it's very early. If you're going to be messing around with Julia, there's going to be a lot of alone-time experimenting and reading the &lt;a title=&quot;Julia documentation&quot; href=&quot;http://docs.julialang.org/en/latest/&quot; target=&quot;_blank&quot;&gt;Julia documentation&lt;/a&gt;. There are also several other resources including a &lt;a title=&quot;Julia users Google group&quot; href=&quot;https://groups.google.com/forum/?fromgroups=#!forum/julia-users&quot; target=&quot;_blank&quot;&gt;Julia-Users Google group&lt;/a&gt;, &lt;a title=&quot;Julia for R programmers&quot; href=&quot;http://www.stat.wisc.edu/~bates/JuliaForRProgrammers.pdf&quot; target=&quot;_blank&quot;&gt;Julia for R programmers&lt;/a&gt;, individual discussions on GitHub in the 'Issues' section of each Julia package, and a few tutorials floating around (&lt;a title=&quot;Julia tutorials&quot; href=&quot;http://forio.com/julia/tutorials-list&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt; and &lt;a title=&quot;Julia meta tutorial&quot; href=&quot;http://datacommunitydc.org/blog/2013/07/a-julia-meta-tutorial/&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;).

Beyond just the written examples though, I've found that the budding Julia community is very helpful and willing in terms of answering questions. I've been bugging the hell out of &lt;a title=&quot;John Myles White&quot; href=&quot;http://www.johnmyleswhite.com/&quot; target=&quot;_blank&quot;&gt;John Myles White&lt;/a&gt; and he hasn't complained (yet!), and even when code issues are raised through the users group or on GitHub, ultimately everyone has been very respectful and eager to help. So don't be intimidated by the fact that Julia has a very MIT and Ph.D-ness to it...jump right in and migrate some of your favorite code over from other languages.

While I haven't moved to using Julia for my everyday workload, I am getting facility to the point where I'm starting to consider using Julia for selected projects. Once the language matures a bit more, &lt;del&gt;&lt;a title=&quot;Julia Studio&quot; href=&quot;http://forio.com/julia/&quot; target=&quot;_blank&quot;&gt;JuliaStudio&lt;/a&gt; starts to approach &lt;a title=&quot;RStudio&quot; href=&quot;http://www.rstudio.com/&quot; target=&quot;_blank&quot;&gt;RStudio&lt;/a&gt; in terms of functionality&lt;/del&gt;, and I get more familiar with the language in general, I can see Julia taking over for at least one if not all of my scientific programming languages.</content>
      </item>
      
    
      
      <item>
        <title>Getting Started Using Hadoop, Part 3: Loading Data</title>
        
          <description>&lt;p&gt;In part 2 of the “Getting Started Using Hadoop” series, I discussed how to &lt;a title=&quot;Build a Hadoop cluster Amazon EC2&quot; href=&quot;http://randyzwitch.com/big-data-hadoop-amazon-ec2-cloudera-part-2/&quot; target=&quot;_blank&quot;&gt;build a Hadoop cluster on Amazon EC2&lt;/a&gt; using Cloudera CDH. This post will cover how to get your data into the Hadoop Distributed File System (HDFS) using the publicly available “&lt;a title=&quot;Airline dataset&quot; href=&quot;http://stat-computing.org/dataexpo/2009/the-data.html&quot; target=&quot;_blank&quot;&gt;Airline Dataset&lt;/a&gt;”. While there are multiple ways to upload data into HDFS, this post will only cover the easiest method, which is to use the Hue ‘File Browser’ interface.&lt;/p&gt;

</description>
        
        <pubDate>Wed, 22 May 2013 07:39:19 -0400</pubDate>
        <link>
        http://randyzwitch.com/uploading-data-hadoop-amazon-ec2-cloudera-part-3/</link>
        <guid isPermaLink="true">http://randyzwitch.com/uploading-data-hadoop-amazon-ec2-cloudera-part-3/</guid>
        <content type="html" xml:base="/uploading-data-hadoop-amazon-ec2-cloudera-part-3/">In part 2 of the &quot;Getting Started Using Hadoop&quot; series, I discussed how to &lt;a title=&quot;Build a Hadoop cluster Amazon EC2&quot; href=&quot;http://randyzwitch.com/big-data-hadoop-amazon-ec2-cloudera-part-2/&quot; target=&quot;_blank&quot;&gt;build a Hadoop cluster on Amazon EC2&lt;/a&gt; using Cloudera CDH. This post will cover how to get your data into the Hadoop Distributed File System (HDFS) using the publicly available &quot;&lt;a title=&quot;Airline dataset&quot; href=&quot;http://stat-computing.org/dataexpo/2009/the-data.html&quot; target=&quot;_blank&quot;&gt;Airline Dataset&lt;/a&gt;&quot;. While there are multiple ways to upload data into HDFS, this post will only cover the easiest method, which is to use the Hue 'File Browser' interface.

## Loading data into HDFS using Hue

![hadoop-hue-file-browser](/wp-content/uploads/2013/05/hadoop-hue-file-browser-e1367455309802.png)
&lt;p class=&quot;wp-caption-text&quot;&gt;
'File Browser' in Hue (Cloudera)
&lt;/p&gt;

Loading data into Hadoop using Hue is by far the easiest way to get started. Hue provides a GUI that provides a &quot;File Browser&quot; like you normally see in Windows or OSX. The workflow here would be to download each year of Airline data to your local machine, then upload each file using the `Upload -&gt; Files` menu drop-down.

While downloading files from one site on the Internet, then uploading files to somewhere else on the Internet is somewhat wasteful of time and bandwidth, as a tutorial to _get started_ with Hadoop this isn't the worst thing in the world. For those of you who are OSX users and comfortable using Bash from the command line, here's some code so you don't have to babysit the download process:

{% highlight shell linenos %}
$ for i in {1987..2008}
&gt; do
&gt; curl http://stat-computing.org/dataexpo/2009/$i.csv.bz2 &gt; $i.csv.bz2
&gt; bunzip2 $i.csv.bz2
&gt; done
{% endhighlight %}

Because you are going to be uploading a bunch of text files to your Hadoop cluster, I'd recommend zipping the files prior to upload. It doesn't matter if you use .zip or .gz files with one key distinction: if you use **.zip** files, you will upload using the &lt;span style=&quot;text-decoration: underline;&quot;&gt;&quot;Zip Files&quot;&lt;/span&gt; button in the File Browser; if you choose **.gz**, then you must use the &lt;span style=&quot;text-decoration: underline;&quot;&gt;&quot;Files&quot;&lt;/span&gt; line in the File Browser. Not only will zipping the files make the upload faster, but it will also make sure you only need to do the process once (as opposed to hitting the upload button on each file). Using the .zip file upload process, you should something like the following...a new folder with all of the files extracted automatically:

![hue-file-browser-unzipped](/wp-content/uploads/2013/05/hue-file-browser-unzipped.png)
&lt;p class=&quot;wp-caption-text&quot;&gt;
.zip file automatically extracted into folder with files (Hortonworks)
&lt;/p&gt;

### Next Steps

With the airline .csv files loaded for each year, we can use Pig or Hive to load the tables into a master dataset &amp; schema. That will be the topic of the next tutorial.</content>
      </item>
      
    
      
      <item>
        <title>Innovation Will Never Be At The Push Of A Button</title>
        
          <description>&lt;blockquote class=&quot;twitter-tweet&quot; data-conversation=&quot;none&quot;&gt;
  &lt;p&gt;
    @&lt;a href=&quot;https://twitter.com/randyzwitch&quot;&gt;randyzwitch&lt;/a&gt; @&lt;a href=&quot;https://twitter.com/benjamingaines&quot;&gt;benjamingaines&lt;/a&gt; @&lt;a href=&quot;https://twitter.com/usujason&quot;&gt;usujason&lt;/a&gt; I am envisioning the data science equivalent of an autonomous vehicle pileup.
  &lt;/p&gt;

&lt;/blockquote&gt;
</description>
        
        <pubDate>Fri, 17 May 2013 06:28:47 -0400</pubDate>
        <link>
        http://randyzwitch.com/data-science-innovation/</link>
        <guid isPermaLink="true">http://randyzwitch.com/data-science-innovation/</guid>
        <content type="html" xml:base="/data-science-innovation/">&lt;blockquote class=&quot;twitter-tweet&quot; data-conversation=&quot;none&quot;&gt;
  &lt;p&gt;
    @&lt;a href=&quot;https://twitter.com/randyzwitch&quot;&gt;randyzwitch&lt;/a&gt; @&lt;a href=&quot;https://twitter.com/benjamingaines&quot;&gt;benjamingaines&lt;/a&gt; @&lt;a href=&quot;https://twitter.com/usujason&quot;&gt;usujason&lt;/a&gt; I am envisioning the data science equivalent of an autonomous vehicle pileup.
  &lt;/p&gt;

  &lt;p&gt;
    — Todd Belcher (@toddmetrics) &lt;a href=&quot;https://twitter.com/toddmetrics/status/335030724375756800&quot;&gt;May 16, 2013&lt;/a&gt;
  &lt;/p&gt;
&lt;/blockquote&gt;

Recently, I've been getting my blood pressure up reading (marketing) articles about &quot;big data&quot; and &quot;data science&quot;.  What saddens me about the whole discussion is that there is the underlying premise that what is stopping companies from &quot;harnessing the power of big data&quot; is just the lack of an easy-to-use, push-button tool. Respectfully, if you believe this, you should bow out of the conversation altogether.

### Math is hard and stuff.

The first article that really bothered me is titled &quot;[Do Predictive Modelers Need to Know Math?](http://smartdatacollective.com/deanabbott/115886/do-predictive-modelers-need-know-math)&quot; This is a provocative title from a veteran in the data mining/data science industry, and his conclusion is basically '_Yes, but not everyone on the team needs to be able to hand-solve equations._' I think that's a fair point within the context of needing to understand the mathematical concepts behind algorithms, but not needing to be bogged down by notation.

Extending that idea a little further, how far away from the math should a business be comfortable with an employee pushing the button on a machine learning algorithm? Should the CEO be building predictive models? The Intern? A Call Center Rep? For me, I think the answer falls back on the allegory of the [highly-specialized tradesperson](http://www.snopes.com/business/genius/where.asp):

&gt; Driver: &quot;How can you charge $100 for five minutes work? All you did was put a bolt on and turned the wrench a few times!
&gt;
&gt; Mechanic: &quot;I didn't charge you for the parts, I charged you for knowing where to put the wrench...&quot;

The value a data scientist brings to a business is not that he can push the buttons in a GUI like [rattle for R](http://rattle.togaware.com/), [Weka](http://www.cs.waikato.ac.nz/ml/weka/), [KnowledgeSeeker](http://www.angoss.com/predictive-analytics-software/products/data-analysis-software), or [SAS Enterprise Miner](http://www.sas.com/technologies/analytics/datamining/miner/). What your data scientist brings to the table is knowing the underlying assumptions that go into a model, how the algorithm works, which algorithm is appropriate for the business problem being solved and _when to know the model/algorithm has failed_.

Of all of the things listed, the experience of knowing when the model can/has failed is what you're paying the money for. That knowledge doesn't come from just pushing the GUI buttons a bunch of times. And if you're making million-dollar decisions based on an algorithm, it's worth paying the salary for a person who really understands the model.

### Hire a mathematician, get a programmer free

The next article that bothered me is a [&quot;Business Analytics: Do we need data scientists?&quot;](http://www.zdnet.com/debate/business-analytics-do-we-need-data-scientists/10119786/rebuttal/#skip-intro) debate over &quot;Do we need data scientists at all?&quot; The _No_ argument boils down to an idea that only things that can be made easy and are sufficiently developed are useful/valuable. Thus, because a general analyst can't use Excel, but rather might need to write a SQL query or write a program to put together a dataset, the problem-domain is too difficult. The _No_ debater also refers to data scientists as being &quot;adversarial&quot;, &quot;pretentious&quot;, project &quot;snobbery&quot;, etc.

But here's the thing...the problem-domain isn't particularly difficult if you hire someone with above-average math proficiency. Any decent graduate program in mathematics, statistics, computer science, economics, finance, psychology and others will be using data through programming. Now, the languages may vary between Java, R, Python, Matlab, C++, SAS, Octave, Eviews or others, but the language doesn't matter, they'll learn whatever language your company is using once you hire them. They also will learn the systems you are using to store your data, whether it's a standard relational database, a NoSQL database, or a parallel processing platform like Hadoop.

How can I be certain that the math person you hire will be able to learn all that's necessary for data science? Because the type of person who likes math &amp; programming is probably a 'system builder' type of person. The type of person who played with Legos growing up. The type of person built their own desktop computer back in the day. The type of person who thinks _How It's Made_ is much more interesting TV than mindless reality shows. The type of person who WANTS to know how a database is storing data, what new open-source technology is out there, wants to find out how many nodes they can connect together before their program won't finish any faster.

As far as the adversarial/pretentious/snobby comment, all I can say is I've never witnessed that. Everyone I know in the data science community are the nicest people, willing to share code, collaborate on ideas and talk until they lose their voice about how to solve an interesting problem.

### Data Science is about innovative research, not reporting

I've read four academic papers this week. I'm not in graduate school.

As some of you might know, I started a new position at a startup which provides real-time intelligence for the lead generation industry. As such, I've got access to billions of records of unstructured data and equally as much structured data. And as a startup, there are several warts that need to be fixed with respect to data storage. So for any given day, I might go from accessing a MySQL database, Amazon Redshift (columnar RDBMS), Amazon DynamoDB (NoSQL) and plain ol' .csv files via Excel or massive .csv files on Amazon S3. To access this data, I've used a combination of R, Python, SQL Workbench, and MySQL Workbench using OSX, Ubuntu desktop and a 'headless' Ubuntu image on Amazon EC2.

Why am I giving you about all this jibber-jabber about research papers and tools? Because the idea of building a one-size-fits-all tool to solve the problem I'm working on just doesn't make sense. And for that matter, I'm not even sure the problem I'm working on is worth solving. But that's the thing...I don't KNOW it's not worth solving, so I need to find out. I've got a quarter-billion URLs that I think I can extract information from, just to give our clients ONE more data element to use to optimize their marketing strategies.  There may be an already existing algorithm I can use, or maybe I'll try this research paper on [&quot;word breaking&quot;](http://research.microsoft.com/apps/mobile/publication.aspx?id=144355) I found from Microsoft Research. Once I find out the answer, if it's valuable, then I need to be able to implement my algorithm into our real-time API, because it's likely whatever language I end up using isn't going to be what our API is written in.

So if these aren't the type of problems you're working on, then maybe there is an all-in-one tool out there for you to use (and that's okay). But these are the types of edge-case problems that I think about when I think about &quot;data science&quot;, and as such, it will always be custom and ad-hoc. There are many awesome open-source tools I will use to help me along the way, but it will never make sense to build an easy-to-use tool for a problem a few dozen companies may ever need to know the answer to.

### Use the data you have to do something extraordinary

&lt;blockquote class=&quot;twitter-tweet&quot;&gt;
  &lt;p&gt;
    If you don't 'get' something, own it. Don't dump your dumb garbage into the world.
  &lt;/p&gt;

  &lt;p&gt;
    — marc maron (@marcmaron) &lt;a href=&quot;https://twitter.com/marcmaron/status/335167001427320832&quot;&gt;May 16, 2013&lt;/a&gt;
  &lt;/p&gt;
&lt;/blockquote&gt;

I'm already 1100 words into this rant, so I'll finish up with a few admissions. Yes, &quot;data science&quot; is somewhat a ridiculous name for the combination of advanced analytics and data engineering that it represents. And yes, there are plenty of vendors out there pedaling hype about the grandeur of 'Big Data' and why every business MUST jump on board or be left behind.

But rather than focusing on why something is &quot;useless&quot; or &quot;stupid&quot; or &quot;hype&quot;, just ask yourself &quot;Can I solve the business problems I have today using the tools I currently have access to?&quot; If the answer is yes, then great, get to work. If not, maybe you can find someone to help you get where you're going (and that person may or may not call themselves a &quot;Data Scientist&quot;). Either way, let's all move forward and do something extraordinary. It's the least we can do for our customers.</content>
      </item>
      
    
      
      <item>
        <title>Getting Started Using Hadoop, Part 2: Building a Cluster</title>
        
          <description>&lt;p&gt;In &lt;a title=&quot;Getting Started With Hadoop, Part 1&quot; href=&quot;http://randyzwitch.com/big-data-hadoop-amazon-ec2-cloudera-part-1/&quot; target=&quot;_blank&quot;&gt;Part 1 of this series&lt;/a&gt;, I discussed some of the basic concepts around Hadoop, specifically when it’s appropriate to use Hadoop to solve your data engineering problems and the terminology of the Hadoop eco-system. This post will cover how to install your own Hadoop cluster on Amazon EC2 using Cloudera Manager.&lt;/p&gt;

</description>
        
        <pubDate>Thu, 25 Apr 2013 13:33:48 -0400</pubDate>
        <link>
        http://randyzwitch.com/big-data-hadoop-amazon-ec2-cloudera-part-2/</link>
        <guid isPermaLink="true">http://randyzwitch.com/big-data-hadoop-amazon-ec2-cloudera-part-2/</guid>
        <content type="html" xml:base="/big-data-hadoop-amazon-ec2-cloudera-part-2/">In &lt;a title=&quot;Getting Started With Hadoop, Part 1&quot; href=&quot;http://randyzwitch.com/big-data-hadoop-amazon-ec2-cloudera-part-1/&quot; target=&quot;_blank&quot;&gt;Part 1 of this series&lt;/a&gt;, I discussed some of the basic concepts around Hadoop, specifically when it's appropriate to use Hadoop to solve your data engineering problems and the terminology of the Hadoop eco-system. This post will cover how to install your own Hadoop cluster on Amazon EC2 using Cloudera Manager.

Like prior posts talking about &lt;a title=&quot;Amazon EC2 posts&quot; href=&quot;http://randyzwitch.com/tag/amazon-ec2/&quot; target=&quot;_blank&quot;&gt;Amazon EC2&lt;/a&gt;, this post assumes you have some basic facility with Linux, submitting instructions via the command line, etc. Because really, if you're interested in Hadoop, using the command line probably isn't a limiting factor!

## Building a 18-node Hadoop Cluster

The SlideShare presentation below shows the steps to building a 18-node Hadoop cluster, using a single _m1.large_ EC2 instance as the 'Name Node' and 18 _m1.medium_ EC2 instances as the 'Data Nodes'.  I chose 18 nodes because according to &lt;a title=&quot;Cloudera Manager Example&quot; href=&quot;http://blog.cloudera.com/blog/2013/03/how-to-create-a-cdh-cluster-on-amazon-ec2-via-cloudera-manager/&quot; target=&quot;_blank&quot;&gt;Cloudera&lt;/a&gt;, 20 is the maximum that can be activated at one time through the Amazon API, so let's stay under the max to avoid any errors. It's possible to add more instances later through the Cloudera Manager (up to 50 total), if so desired.

Note that going through this tutorial will cost $2.40/hr at current prices ($0.24/hr per _m1.large_ instance and $0.12/hr per _m1.medium_ instance).

&lt;iframe style=&quot;border: 1px solid #CCC; border-width: 1px 1px 0; margin-bottom: 5px;&quot; src=&quot;http://www.slideshare.net/slideshow/embed_code/19982722&quot; height=&quot;421&quot; width=&quot;512&quot; allowfullscreen=&quot;&quot; frameborder=&quot;0&quot; marginwidth=&quot;0&quot; marginheight=&quot;0&quot; scrolling=&quot;no&quot;&gt;&lt;/iframe&gt;

Since the SlideShare presentation is potentially not so friendly on the eyes, I've also created a &lt;a title=&quot;Cloudera Amazon EC2 instructions&quot; href=&quot;http://randyzwitch.com/wp-content/uploads/2013/04/cloudera-amazon-ec2.pdf&quot; target=&quot;_blank&quot;&gt;PDF download&lt;/a&gt; that's full resolution.

## Next Steps

Once you make it through all these steps to set up a Hadoop cluster, you are ready to do some analysis. [Part 3 of this tutorial](http://randyzwitch.com/uploading-data-hadoop-amazon-ec2-cloudera-part-3/ &quot;Upload data into HDFS using Hue&quot;) will cover how to upload data into HDFS using Hue.

_Update, 7/13/13:_ As is the case with any open-source project, there have been several changes to the Cloudera Manager that makes setup easier. When getting started, on the screen where it asks &quot;Which Cloudera do you want to deploy?&quot;, choose 'Cloudera Standard'. Also, once you get to slides 13-14 where you click on the link to get started with Hue, the link now works correctly (you don't need to search for the Amazon DNS any more!)</content>
      </item>
      
    
      
      <item>
        <title>Getting Started Using Hadoop, Part 1: Intro</title>
        
          <description>&lt;p&gt;For the last couple of days I’ve been at the eMetrics conference in San Francisco. There were several panels that discussed big data, both from an engineering standpoint as well as how to adopt newer technologies from a business perspective.&lt;/p&gt;

</description>
        
        <pubDate>Thu, 18 Apr 2013 08:47:15 -0400</pubDate>
        <link>
        http://randyzwitch.com/big-data-hadoop-amazon-ec2-cloudera-part-1/</link>
        <guid isPermaLink="true">http://randyzwitch.com/big-data-hadoop-amazon-ec2-cloudera-part-1/</guid>
        <content type="html" xml:base="/big-data-hadoop-amazon-ec2-cloudera-part-1/">For the last couple of days I've been at the eMetrics conference in San Francisco. There were several panels that discussed big data, both from an engineering standpoint as well as how to adopt newer technologies from a business perspective.

Unfortunately, there wasn't a whole lot of practical information on how to actually get started using 'big data' technologies, of which Hadoop is one.  Luckily, it's fairly easy to create a proof-of-concept Hadoop cluster using Amazon EC2 and Cloudera.

This series will be at least 5 parts, as follows:

  1. Intro to Hadoop ecosystem and concepts
  2. [Setting up Hadoop cluster on Amazon EC2](http://randyzwitch.com/big-data-hadoop-amazon-ec2-cloudera-part-2/ &quot;Setting up Hadoop Cluster on Amazon EC2&quot;) using &lt;a title=&quot;Cloudera Amazon EC2&quot; href=&quot;http://blog.cloudera.com/blog/2013/03/how-to-create-a-cdh-cluster-on-amazon-ec2-via-cloudera-manager/&quot; target=&quot;_blank&quot;&gt;Cloudera&lt;/a&gt;
  3. &lt;a title=&quot;Populating HDFS using Hue&quot; href=&quot;http://randyzwitch.com/uploading-data-hadoop-amazon-ec2-cloudera-part-3/&quot; target=&quot;_blank&quot;&gt;Populating HDFS with airline dataset&lt;/a&gt; files using &lt;a title=&quot;Hadoop Hue&quot; href=&quot;http://cloudera.github.io/hue/&quot; target=&quot;_blank&quot;&gt;Hue&lt;/a&gt;
  4. Use &lt;a title=&quot;Hive joins&quot; href=&quot;https://cwiki.apache.org/Hive/languagemanual-joins.html&quot; target=&quot;_blank&quot;&gt;Hive&lt;/a&gt; and/or &lt;a title=&quot;Apache Pig&quot; href=&quot;http://pig.apache.org/&quot; target=&quot;_blank&quot;&gt;Pig&lt;/a&gt; to &lt;a title=&quot;Creating Tables with Hive&quot; href=&quot;http://randyzwitch.com/hadoop-creating-tables-hive/&quot; target=&quot;_blank&quot;&gt;stack datasets into one master dataset&lt;/a&gt;
  5. &lt;a title=&quot;Analysis using Pig &amp; Hive&quot; href=&quot;http://randyzwitch.com/getting-started-hadoop-hive-pig/&quot; target=&quot;_blank&quot;&gt;Doing analytics on the combined Airline dataset using Pig and/or Hive&lt;/a&gt;

My aim with this series is to _simply_ explain why you might want to consider using Hadoop for your data storage and processing. There's a lot of marketing &amp; vendor &lt;del&gt;bullshit&lt;/del&gt; excitement surrounding the term 'big data', so for this blog series, I'm just going to focus on the most important points for an analyst/marketer to understand. And other than this sentence, there will be no mentions of _MS Excel_ in terms of 'big data', which is &lt;a title=&quot;Use R not Excel&quot; href=&quot;http://blog.revolutionanalytics.com/2013/04/more-reasons-not-to-use-excel-for-modeling.html&quot; target=&quot;_blank&quot;&gt;barely an appropriate tool for analysis&lt;/a&gt; in general, let alone analysis at scale.

### What Is Hadoop &amp; Why Are People Talking About It?

At it's simplest, Hadoop provides a parallel-processing computing framework for data storage and processing. The reason why a parallel-processing framework is important for enterprise-level analysis is due to physical limitations on how quickly a single machine can process information.

As an example, suppose  you want to create a report that looks at 1 trillion daily credit card transactions. It's possible to do your calculations on your local desktop using a tool like SAS. However, the amount of time to process that much data on a desktop with 8GB-16GB of RAM might be 8 hours, 10 hours....24 hours?! So an analyst trying to get an answer can start a SINGLE business question at 8am and _hope_ they get their answer before it's time to leave at the end of the day. Suffice to say, not a particularly efficient way to run a business.

The solution might seem to add more processors and RAM to a desktop, but what happens when you add more users asking questions? Now you need an enterprise-class server such as Oracle or Teradata (and a few million dollars!). And for every terabyte of data you want to store, you'll need a few thousand dollars. And that's just for your nicely structured data...what happens when you want to start storing data such as free-form text that's not so cleanly structured? Eventually, these types of _engineering questions_ lead you towards a solution like Hadoop.

The reason why there is so much discussion around Hadoop as a data platform is that it solves the problems stated above: excessive time to process vast amounts of data and excessive cost of data storage. By using &quot;commodity hardware&quot; along with some fancy engineering, Hadoop provides an extremely cost-effective and flexible way to handle your enterprise data.

### If Hadoop is so Great, Why Doesn't Everyone Use It?

_&quot;Fast, Cheap And Good. Everyone should use Hadoop!&quot; - Every vendor in marketplace_

Just like you (probably) don't use a screwdriver to stir a pot of chicken soup, not every data storage and analysis problem requires the extreme flexibility that Hadoop can provide. From the example above with credit card transactions, a standard relational database might continue to be an acceptable solution if you're just running a basic SQL query to sum across the rows. But once your data starts moving beyond &quot;rows and columns&quot; and into things such as free-form text, images, clickstream data...the more Hadoop makes sense.

While it's a tautology, how you know you need a solution like Hadoop is when you suspect you need a solution like Hadoop! If you already have a highly functioning data mart that answers your business questions, you probably don't need to re-engineer everything _just because_. If you're an Internet startup trying to create the next Facebook, then a standard relational database probably won't cut it.

The best example I heard at eMetrics about the need for Hadoop was from Bob Page (now at Hortonworks, a Hadoop vendor): when Bob was at Ebay, for the longest time they were throwing away data, specifically images from the listings. So prior high storage costs leading to undesirable business outcome (deletion), unstructured data in the form of images...a Hadoop framework made sense to implement. Once implemented, Ebay could look across years of auctions to answer their business questions.

### I'm An Analyst, Not An Engineer...What's The Minimum I Need To Know To Get Started?

_&quot;MapReduce, Pigs, HCatalogs, Elephants, Bees, Zoos...Ooozie (Uzi's)? WTF is everyone talking about?&quot;_

If you've made it this far and you're not an engineer or DBA, you're probably someone who's interested in data science. You may be someone who already uses R, Python, Ruby or Java. Or, you're a masochist. In any case, here are the minimum concepts I think you need to know to get started for later blog posts:

  * &lt;span style=&quot;text-decoration: underline;&quot;&gt;MapReduce:&lt;/span&gt; Not explicitly a Hadoop idea, but the idea that data can be split into chunks by a key (&quot;Map&quot;) and then processed into information by one or more functions/transformations (&quot;Reduce&quot;). In the Hadoop sense, MapReduce is generally a reference to a &quot;job&quot; written in Java that performs a data transformation
  * &lt;span style=&quot;text-decoration: underline;&quot;&gt;HDFS:&lt;/span&gt; Hadoop Distributed File System. Raw data gets imported into HDFS (either structured or unstructured), the distributed around to all of the various nodes to allow for parallel processing
  * &lt;span style=&quot;text-decoration: underline;&quot;&gt;Hive:&lt;/span&gt; SQL-like interface so that analysts don't have to write MapReduce code directly
  * &lt;span style=&quot;text-decoration: underline;&quot;&gt;Pig:&lt;/span&gt; A scripting language used for analysis. Generally, an analyst will use Hive and/or Pig to do their work
  * &lt;span style=&quot;text-decoration: underline;&quot;&gt;HCatalog:&lt;/span&gt; A 'Data Warehouse' layer on top of HDFS, similar to how you define a database table (a series of columns in a table with formats)

### Next Steps

With the above five Hadoop concepts in place, the next few posts will be to set up a proof-of-concept Hadoop cluster on Amazon EC2, processing ~12GB of publicly available data from the '&lt;a title=&quot;Airline dataset&quot; href=&quot;http://stat-computing.org/dataexpo/2009/the-data.html&quot; target=&quot;_blank&quot;&gt;Airline dataset&lt;/a&gt;'. That's not 'big' as 'big data' goes, but it's big enough to be fun to work with.</content>
      </item>
      
    
      
      <item>
        <title>Instructions for Installing &amp;#038; Using R on Amazon EC2</title>
        
          <description>&lt;p&gt;If you’re an R user, you’ve surely heard all the hype around ‘big data’ and how R is commonly used to analyze these volumes of data. One thing that’s often missing from the discussion is HOW to work around issues using big data and R, specifically how to deal with the fact that R stores all its objects in-memory.&lt;/p&gt;

</description>
        
        <pubDate>Mon, 08 Apr 2013 12:36:07 -0400</pubDate>
        <link>
        http://randyzwitch.com/r-amazon-ec2/</link>
        <guid isPermaLink="true">http://randyzwitch.com/r-amazon-ec2/</guid>
        <content type="html" xml:base="/r-amazon-ec2/">If you're an R user, you've surely heard all the hype around 'big data' and how R is commonly used to analyze these volumes of data. One thing that's often missing from the discussion is HOW to work around issues using big data and R, specifically how to deal with the fact that R stores all its objects in-memory.

While you can use packages such as &lt;a title=&quot;ff package&quot; href=&quot;http://cran.r-project.org/web/packages/ff/index.html&quot; target=&quot;_blank&quot;&gt;ff&lt;/a&gt; and &lt;a title=&quot;bigmemory&quot; href=&quot;http://cran.r-project.org/web/packages/bigmemory/index.html&quot; target=&quot;_blank&quot;&gt;bigmemory&lt;/a&gt; to overcome the in-memory limits of your local machine, these additional packages do require some re-engineering of your code. Instead, consider using &lt;a title=&quot;Amazon Web Services&quot; href=&quot;http://aws.amazon.com/ec2/&quot; target=&quot;_blank&quot;&gt;Amazon EC2&lt;/a&gt; to provision the resources you need.  Here are two ways to get started...

### Use a Pre-Made AMI

In the great open-source tradition, there are already R Amazon EC2 AMI images available out there to use. The way I got started was using the pre-built images that &lt;a title=&quot;RStudio AMI Images&quot; href=&quot;http://www.louisaslett.com/RStudio_AMI/&quot; target=&quot;_blank&quot;&gt;Louis Aslett&lt;/a&gt; provides on his website.  Louis also provides great instructions on learning about EC2, so if you've never worked with R in the cloud or a just looking to get up and running fast, his website is a great means to do so.

### Build Your Own Image

Alternatively, suppose you want to build your own customized image. For example, say you wanted to build a proof-of-concept 'big data' environment, so you want &lt;a title=&quot;R download at CRAN&quot; href=&quot;http://cran.r-project.org/&quot; target=&quot;_blank&quot;&gt;R&lt;/a&gt;, &lt;a title=&quot;Python download&quot; href=&quot;http://python.org/&quot; target=&quot;_blank&quot;&gt;Python&lt;/a&gt;, &lt;a title=&quot;MySQL download&quot; href=&quot;http://dev.mysql.com/&quot; target=&quot;_blank&quot;&gt;MySQL&lt;/a&gt; and &lt;a title=&quot;MongoDB&quot; href=&quot;http://www.mongodb.org/&quot; target=&quot;_blank&quot;&gt;MongoDB&lt;/a&gt;.  The commands to accomplish this are listed below. Note that I'm assuming you have a &lt;a title=&quot;AWS FAQ&quot; href=&quot;http://aws.amazon.com/ec2/faqs/&quot; target=&quot;_blank&quot;&gt;basic understanding of working through the Amazon Web Service Console (AWS)&lt;/a&gt;, including being able to get to the 'Classic Wizard' for launching an EC2 instance. You also should have a basic understanding of &lt;a title=&quot;Command Line tutorial&quot; href=&quot;http://cli.learncodethehardway.org/book/&quot; target=&quot;_blank&quot;&gt;working from the command line&lt;/a&gt;.

#### Setting Up Amazon EC2 Instance

  1. Launch an Ubuntu 12.04.1 LTS 64-bit image. You can use a free &quot;t1.micro&quot; image while building, then provision more resources later once you're ready for analysis.
  2. Accept defaults until you get to Key-Pair tab. The Key-Pair is what allows you to login securely to your Amazon EC2 image without a password. Create and download a Key-Pair if you don't already have one or choose an existing Key-Pair if you do.
  3. When you get to the 'Security Groups' tab, create a security group that has the following ports open: `22` (SSH), `80` (HTTP), `443` (HTTPS), `3389` (RDP, optional), and `8787` (RStudio Server).
  4. Work through the rest of the Wizard until your instance is launched.

#### Connecting to Amazon EC2 Instance

  1. There are two ways to connect to your EC2 image, both of which can be found by going to the &quot;Actions&quot; tab in the AWS console, then selecting &quot;Connect&quot; from the drop-down. The rest of this tutorial assumes you connect via a stand-alone SSH client (such as Terminal for Mac OSX)
  2. Connect to your instance by typing the code provided to you, such as: `ssh -i me-aws.pem ubuntu@ec2-50-19-18-120.compute-1.amazonaws.com`
  3. Be sure that before you submit this code, you either modify the line to put the directory in front of your Key-Pair, or &quot;cd&quot; to the directory where the Key-Pair is located
  4. After submitting the connect code, you will get a warning saying that the 'authenticity can't be established, do you want to continue?'  Type `yes` and hit enter to log in.

#### Installing Base R

Once you are logged in, there are about a dozen commands that need to be submitted. Some commands run quickly, others can take 10-15 minutes to run through the entire installation process. Depending on how quickly each command completes, you may or may not need to type &quot;sudo&quot; in front of each command to have proper access rights for installation. Submit each line one at a time.

{% highlight shell linenos %}
#Create a user, home directory and set password
sudo useradd rstudio
sudo mkdir /home/rstudio
sudo passwd rstudio
sudo chmod -R 0777 /home/rstudio

#Update all files from the default state
sudo apt-get update
sudo apt-get upgrade

#Add CRAN mirror to custom sources.list file using vi
sudo vi /etc/apt/sources.list.d/sources.list

#Add following line (or your favorite CRAN mirror)
deb http://lib.stat.cmu.edu/R/CRAN/bin/linux/ubuntu precise/

#Update files to use CRAN mirror
#Don't worry about error message
sudo apt-get update

#Install latest version of R
#Install without verification
sudo apt-get install r-base
{% endhighlight %}

While not strictly required to run R, I also like to run the following commands to install the Curl and XML packages as well, which are useful if you want to use R to connect to any web data/APIs.

{% highlight shell linenos %}
#Install in order to use RCurl &amp; XML
sudo aptitude install libcurl4-openssl-dev
sudo apt-get install libxml2-dev
{% endhighlight %}

With these commands run, you will now be able to run R from the command line just by typing &quot;R&quot; at the prompt. However, it would be a crime to do all this work and not install RStudio Server, which makes working in R so much easier.

#### Installing RStudio Server

Once you've installed the above commands, you can now access RStudio through your local browser. Navigate to the Public DNS of your image on port 8787, similar to:

**http://ec2-50-19-18-120.compute-1.amazonaws.com:8787**

The `login` and `password` will be the values you used in the image creation process (I used `rstudio` as my username above).

#### Installing MySQL, Python, and MongoDB

If you've made it this far, I'm sure you realize that installing additional packages will only take a line or two of code. Even better, Python is installed by default on Linux, so we really only need to install MySQL and MongoDB.

{% highlight shell linenos %}
#Install MySQL
sudo apt-get install mysql-common
sudo apt-get install mysql-server

#Install MongoDB
sudo apt-get install mongodb
{% endhighlight %}

### Summary

While the steps above can be intimidating if you've never used Linux or worked on the command line, but once you get the hang of it, your ability to use R on 'big data' (however you define it) will be much improved. For only a few pennies to up to a few dollars per hour, you can use hardware having 16-64GB of RAM or more.

EDIT, 4/9: The code is wrapping weird on some monitors.  &lt;a title=&quot;Amazon EC2 RStudio commands&quot; href=&quot;http://randyzwitch.com/wp-content/uploads/2013/04/amazon-ec2-rstudio.txt&quot; target=&quot;_blank&quot;&gt;Click here&lt;/a&gt; for the commands in a .txt. file.</content>
      </item>
      
    
      
      <item>
        <title>Automated Re-Install of Packages for R 3.0</title>
        
          <description>&lt;p&gt;With the big release of &lt;a title=&quot;R 3.0 introduction&quot; href=&quot;http://www.r-bloggers.com/r-3-0-0-is-released-whats-new-and-how-to-upgrade/&quot; target=&quot;_blank&quot;&gt;R 3.0&lt;/a&gt; today comes an unfortunate side effect of needing to re-install all of your packages. Luckily, R provides a pretty easy method of getting all of your packages into a list for automated re-install.  Here’s how to do it for OSX users with a default install to the Library:&lt;/p&gt;

</description>
        
        <pubDate>Wed, 03 Apr 2013 06:10:09 -0400</pubDate>
        <link>
        http://randyzwitch.com/automated-re-install-of-packages-for-r-3-0/</link>
        <guid isPermaLink="true">http://randyzwitch.com/automated-re-install-of-packages-for-r-3-0/</guid>
        <content type="html" xml:base="/automated-re-install-of-packages-for-r-3-0/">With the big release of &lt;a title=&quot;R 3.0 introduction&quot; href=&quot;http://www.r-bloggers.com/r-3-0-0-is-released-whats-new-and-how-to-upgrade/&quot; target=&quot;_blank&quot;&gt;R 3.0&lt;/a&gt; today comes an unfortunate side effect of needing to re-install all of your packages. Luckily, R provides a pretty easy method of getting all of your packages into a list for automated re-install.  Here's how to do it for OSX users with a default install to the Library:

For Windows users, the same general process should work, assuming you change the file reference in the _installed.packages_ function to the proper Windows location. The one downside to this method is that only packages that are &lt;a title=&quot;CRAN&quot; href=&quot;http://cran.r-project.org/&quot; target=&quot;_blank&quot;&gt;listed on CRAN&lt;/a&gt; will be reinstalled, so if you installed anything using devtools, you'll need to re-install those packages again. But at the very least, the code snippet above is a quick way to re-install most of your packages. EDIT, 4/4/13: Per Noam below, you can also use a more direct method: `update.packages(ask=FALSE, checkBuilt = TRUE)`</content>
      </item>
      
    
      
      <item>
        <title>The Fun of Error Trapping: R Package Edition</title>
        
          <description>&lt;p&gt;For the last month or so I’ve been working on an R package to make accessing the &lt;a title=&quot;Omniture Reporting API&quot; href=&quot;https://developer.omniture.com/&quot; target=&quot;_blank&quot;&gt;Adobe (Omniture) Digital Marketing Suite Reporting API&lt;/a&gt; easier.  As part of this development effort, I’m at the point where I’m intentionally introducing errors into my function inputs, trying to guess some of the ways users might incorrectly input arguments into each function.  Imagine my surprise when I saw this:&lt;/p&gt;

</description>
        
        <pubDate>Mon, 25 Feb 2013 12:15:46 -0500</pubDate>
        <link>
        http://randyzwitch.com/r-error-message-fun/</link>
        <guid isPermaLink="true">http://randyzwitch.com/r-error-message-fun/</guid>
        <content type="html" xml:base="/r-error-message-fun/">For the last month or so I've been working on an R package to make accessing the &lt;a title=&quot;Omniture Reporting API&quot; href=&quot;https://developer.omniture.com/&quot; target=&quot;_blank&quot;&gt;Adobe (Omniture) Digital Marketing Suite Reporting API&lt;/a&gt; easier.  As part of this development effort, I'm at the point where I'm intentionally introducing errors into my function inputs, trying to guess some of the ways users might incorrectly input arguments into each function.  Imagine my surprise when I saw this:

&gt; &lt;span style=&quot;color: #0000ff;&quot;&gt;&gt; result &lt;- content(json)&lt;/span&gt;  
&gt; &lt;span style=&quot;color: #ff0000;&quot;&gt;Loading required package: XML&lt;/span&gt;  
&gt; &lt;span style=&quot;color: #ff0000;&quot;&gt;Error in parser(content, ...) : could not find function &quot;htmlTreeParse&quot;&lt;/span&gt;  
&gt; &lt;span style=&quot;color: #ff0000;&quot;&gt;In addition: Warning message:&lt;/span&gt;  
&gt; &lt;span style=&quot;color: #ff0000;&quot;&gt;In library(package, lib.loc = lib.loc, character.only = TRUE, logical.return = TRUE,  :&lt;/span&gt;  
&gt; &lt;span style=&quot;color: #ff0000;&quot;&gt;there is no package called ‘XML’&lt;/span&gt;  

The main idea behind the functions I've written is making REST calls to the Omniture API, which done correctly return valid &lt;a title=&quot;JSON documentation&quot; href=&quot;http://www.json.org/&quot; target=&quot;_blank&quot;&gt;JSON&lt;/a&gt;. From there, each JSON string is converted from binary or whatever formatting they come back as using the &lt;a title=&quot;httr R package&quot; href=&quot;http://cran.r-project.org/web/packages/httr/index.html&quot; target=&quot;_blank&quot;&gt;`content`&lt;/a&gt; function from the &lt;a title=&quot;httr R package&quot; href=&quot;http://cran.r-project.org/web/packages/httr/index.html&quot; target=&quot;_blank&quot;&gt;`httr`&lt;/a&gt; package. Without specifying any arguments to the `content` function, the function tries to guess at the proper translation method.

The guessing is all fine and good until you don't pass a valid JSON string!  In this case, the error message is guessing that it might be XML (the returned error is actually HTML), tries to load the XML package...then says it can't load the XML package. A two-for-one error!

Maybe it's just me, but I'm finding this hilarious after a long day of programming. Maybe it's because I'm not longer intimidated by an error like this, and as such, I've gotten over the steep learning curve of R.

_Note:  Hadley, if you read this, I'm not saying your httr package has any sort of bug or anything. Just that I found this particular error amusing._</content>
      </item>
      
    
      
      <item>
        <title>(not provided): Using R and the Google Analytics API</title>
        
          <description>&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2013/01/google-not-provided.png&quot; alt=&quot;(not provided) terms from Google average 35%-60% of all organic search terms&quot; /&gt;&lt;/p&gt;

</description>
        
        <pubDate>Fri, 11 Jan 2013 11:27:33 -0500</pubDate>
        <link>
        http://randyzwitch.com/r-google-analytics-api/</link>
        <guid isPermaLink="true">http://randyzwitch.com/r-google-analytics-api/</guid>
        <content type="html" xml:base="/r-google-analytics-api/">![(not provided) terms from Google average 35%-60% of all organic search terms](/wp-content/uploads/2013/01/google-not-provided.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
(not provided) terms from Google average 35%-60% of all Google organic search terms
&lt;/p&gt;

For power users of Google Analytics, there is a heavy dose of spreadsheet work that accompanies any decent analysis.  But even with Excel in tow, it's often difficult to get the data _just right_ without resorting to formula hacks and manual table formatting.  This is where the Google Analytics API and R can come very much in handy.

## Connecting to the Google Analytics API using R

I'm not going to say that connecting to the Google Analytics API is easy _per se_, but with the [rga package](http://skardhamar.github.com/rga/ &quot;R Google Analytics API package&quot;) written by &quot;skardhamar&quot; on GitHub, it's easier than if you had to develop the connection code yourself!  However, before you can get started making calls to the Google Analytics API, you need to register within the [Google Analytics API console](https://code.google.com/apis/console/ &quot;Google Analytics API console&quot;).  There you can define a new project and then you'll be able to make your API calls via R.

After you have your API access straightened out, the [GitHub page for the rga package](http://skardhamar.github.com/rga/ &quot;RGA package instructions&quot;) has all the details in how to authenticate using the `rga.open` function.  I chose to use the `where` argument so that I can continuously hit the API across many sessions without having to do browser authentication each time.

{% highlight r linenos %}
rga.open(instance = &quot;ga&quot;, where = &quot;~/Documents/R/ga-api&quot;)
{% endhighlight %}

## Analyzing (not provided) as a Google Analytics organic search term

Once connected to the Google Analytics API, now it's time to submit our API calls.  I used two API calls to create the graph at the top of the post, which shows the percentage of all Google organic search terms that are listed as &quot;(not provided)&quot; for the entire history of this blog.  The two API calls were to download the number of total organic search term visits by date from Google and the number of &quot;(not provided)&quot; visits by date, also from Google.  Here's the API call for the &quot;(not provided)&quot; data (replace XXXXXXXX with your profile ID):

{% highlight r linenos %}
visits_notprovided.df &lt;- ga$getData(XXXXXXXX,
start.date = &quot;2011-01-01&quot;,
end.date = &quot;2013-01-10&quot;,
metrics = &quot;ga:visits&quot;,
filters = &quot;ga:keyword==(not provided);ga:source==google;ga:medium==organic&quot;,
dimensions = &quot;ga:date&quot;,
max = 1500,
sort = &quot;ga:date&quot;)
{% endhighlight %}

The result of this API call provides an R data frame containing two columns: date and number of visits where the search term was &quot;(not provided)&quot;.

## Munging the data using R

After pulling the data into R, all that's left is to merge the data frames, do a few calculations, then make the boxplot.  Because the default object returned by the rga package is a data frame, it's trivial to use the `merge` function in R to join the data frames, then use a few calculated columns to create the percentage of visits that are &quot;(not provided)&quot;

## What was that Google, only 10% of searches are supposed to be (not provided)?

By now, it's beating a dead horse that the percentage of &quot;(not provided)&quot; search results from Google FAR exceeds what they said it would.  This blog gets about 5,000 visits a month, and due to the technical nature of the blog many of the users are using Chrome (which does secure search automatically) or from iOS (which also does secure search).  But at minimum, this graph illustrates the power of using the Google Analytics API via R; I can update this graph at my leisure by running my script, and I can create a graphic that's not possible within Excel.

Full code:

{% highlight r linenos %}
#### Connecting to Google Analytics API via R
#### Uses OAuth 2.0
#### https://developers.google.com/analytics/devguides/reporting/core/v3/ for documentation

# Install devtools package &amp; rga - This is only done one time
install.packages(&quot;devtools&quot;)
library(devtools)
install_github(&quot;rga&quot;, &quot;skardhamar&quot;)


# Load rga package - requires bitops, RCurl, rjson
# Load lubridate to handle dates
library(rga)
library(lubridate)

# Authenticating to GA API. Go to https://code.google.com/apis/console/ and create
# an API application.  Don't need to worry about the client id and shared secret for
# this R code, it is not needed

# If file listed in &quot;where&quot; location doesn't exist, browser window will open.
# Allow access, copy code into R console where prompted
# Once file located in &quot;where&quot; directory created, you will have continous access to
# API without needing to do browser authentication
rga.open(instance = &quot;ga&quot;, where = &quot;~/Documents/R/ga-api&quot;)


# Get (not provided) Search results.  Replace XXXXXXXX with your profile ID from GA
visits_notprovided.df &lt;- ga$getData(XXXXXXXX,
                                  start.date = &quot;2011-01-01&quot;,
                                  end.date = &quot;2013-01-10&quot;,
                                  metrics = &quot;ga:visits&quot;,
                                  filters = &quot;ga:keyword==(not provided);ga:source==google;ga:medium==organic&quot;,
                                  dimensions = &quot;ga:date&quot;,
                                  max = 1500,
                                  sort = &quot;ga:date&quot;)

names(visits_notprovided.df)&lt;- c(&quot;hit_date&quot;, &quot;np_visits&quot;)

# Get sum of all Google Organic Search results.  Replace XXXXXXXX with your profile ID from GA
visits_orgsearch.df &lt;- ga$getData(XXXXXXXX,
                                    start.date = &quot;2011-01-01&quot;,
                                    end.date = &quot;2013-01-10&quot;,
                                    metrics = &quot;ga:visits&quot;,
                                    filters = &quot;ga:source==google;ga:medium==organic&quot;,
                                    dimensions = &quot;ga:date&quot;,
                                    max = 1500,
                                    sort = &quot;ga:date&quot;)

names(visits_orgsearch.df)&lt;- c(&quot;hit_date&quot;, &quot;total_visits&quot;)

# Merge files, create metrics, limit dataset to just days when tags firing
merged.df &lt;- merge(visits_notprovided.df, visits_orgsearch.df, all=TRUE)
merged.df$search_term_provided &lt;- merged.df$total_visits - merged.df$np_visits
merged.df$pct_np &lt;- merged.df$np_visits / merged.df$total_visits
merged.df$yearmo &lt;- year(merged.df$hit_date)*100 + month(merged.df$hit_date)

final_dataset = subset(merged.df, total_visits &gt; 0)


# Visualization - boxplot by month
# Main plot, minus y axis tick labels
boxplot(pct_np~yearmo,data=final_dataset, main=&quot;Google (not provided)\nPercentage of Total Organic Searches&quot;,
        xlab=&quot;Year-Month&quot;, ylab=&quot;Percent (not provided)&quot;, col= &quot;orange&quot;, ylim=c(0,.8), yaxt=&quot;n&quot;)

#Create tick sequence and format axis labels
ticks &lt;- seq(0, .8, .2)
label_ticks &lt;- sprintf(&quot;%1.f%%&quot;, 100*ticks)
axis(2, at=ticks, labels=label_ticks)
{% endhighlight %}</content>
      </item>
      
    
      
      <item>
        <title>Video:  SQL Queries in R using sqldf</title>
        
          <description>&lt;p&gt;This video covers how to run SQL queries using the ‘sqldf’ package within R. This sqldf tutorial was part of a &lt;a href=&quot;http://keystonesolutions.com&quot;&gt;Keystone Solutions&lt;/a&gt; podcast discussion about data science and what skills beginning analysts should be learning to improve their skill set.&lt;/p&gt;

</description>
        
        <pubDate>Mon, 17 Dec 2012 08:22:51 -0500</pubDate>
        <link>
        http://randyzwitch.com/sqldf-package-r/</link>
        <guid isPermaLink="true">http://randyzwitch.com/sqldf-package-r/</guid>
        <content type="html" xml:base="/sqldf-package-r/">This video covers how to run SQL queries using the 'sqldf' package within R. This sqldf tutorial was part of a [Keystone Solutions](http://keystonesolutions.com) podcast discussion about data science and what skills beginning analysts should be learning to improve their skill set.

The example files from this tutorial can be downloaded from this link:

&lt;a title=&quot;SQL R Tutorial data files&quot; href=&quot;http://randyzwitch.com/wp-content/uploads/2013/11/r-sql-demo-files.zip&quot; target=&quot;_blank&quot;&gt;Example Data files&lt;/a&gt;

&lt;iframe src=&quot;http://www.youtube.com/embed/s2oTUsAJfjI&quot; height=&quot;480&quot; width=&quot;640&quot; allowfullscreen=&quot;&quot; frameborder=&quot;0&quot;&gt;&lt;/iframe&gt;</content>
      </item>
      
    
      
      <item>
        <title>Adding a &quot;Back to Top&quot; Link on WordPress</title>
        
          <description>&lt;p&gt;In a previous post, I discussed how to &lt;a href=&quot;http://randyzwitch.com/removing-powered-by-wordpress-scrappy/&quot; title=&quot;Remove Powered By WordPress Scrappy theme&quot;&gt;remove “Powered By WordPress”&lt;/a&gt; from the footer of the Scrappy theme.  You might also want to add a “Back to Top” link in the footer, especially if your blog has a lot of vertical distance from the top to the bottom.  Here’s how to do it…&lt;/p&gt;

</description>
        
        <pubDate>Wed, 21 Nov 2012 12:44:30 -0500</pubDate>
        <link>
        http://randyzwitch.com/back-to-top-wordpress-scrappy/</link>
        <guid isPermaLink="true">http://randyzwitch.com/back-to-top-wordpress-scrappy/</guid>
        <content type="html" xml:base="/back-to-top-wordpress-scrappy/">In a previous post, I discussed how to [remove &quot;Powered By WordPress&quot;](http://randyzwitch.com/removing-powered-by-wordpress-scrappy/ &quot;Remove Powered By WordPress Scrappy theme&quot;) from the footer of the Scrappy theme.  You might also want to add a &quot;Back to Top&quot; link in the footer, especially if your blog has a lot of vertical distance from the top to the bottom.  Here's how to do it...

## Step 1:  Modifying the Scrappy header.php file

The first step in creating our 'Back to Top' link is to modify the _header.php_ file in our &lt;a title=&quot;WordPress Child Theme&quot; href=&quot;http://randyzwitch.com/twenty-eleven-child-theme-creating-css-file/&quot; target=&quot;_blank&quot;&gt;WordPress child theme&lt;/a&gt; by adding an &lt;a title=&quot;HTML anchor tag&quot; href=&quot;http://www.w3schools.com/tags/tag_a.asp&quot; target=&quot;_blank&quot;&gt;empty HTML link&lt;/a&gt; using the `&lt;a&gt;` tag.  Although you can place an &quot;anchor&quot; like this anywhere you want on your site, we'll add this empty link to the very top of the page for this tutorial.

Find the line of code in your `header.php` file that says `&lt;div id=page class=&quot;hfeed site&quot;&gt;`.  Here we'll add our extra line of code with the &lt;a&gt; tag (line 5 of the code snippet):

{% highlight php linenos %}
&lt;/head&gt;

&lt;body &lt;?php body_class(); ?&gt;&gt;
&lt;div id=&quot;page&quot; class=&quot;hfeed site&quot;&gt;
&lt;a name= &quot;TopOfPage&quot;/a&gt;
        &lt;?php do_action( 'before' ); ?&gt;
        &lt;div class=&quot;wrapper&quot;&gt;
                &lt;header id=&quot;masthead&quot; class=&quot;site-header&quot; role=&quot;banner&quot;&gt;
{% endhighlight %}

Normally when using an `'&lt;a&gt;'` tag, we would also use an `href` in order to create a link.  However, in this case we're just defining an empty element in the page that we can refer to later using our 'Back to Top' link.

## Step 2: Modifying the Scrappy footer.php file

With our anchor in place, we can now add our link.  For this tutorial, we're going to place the link right above the widget area in the Scrappy footer.

Opening up our `footer.php` file, we need to look for the code `&lt;div class = &quot;footer-sidebars&quot;&gt;`.  Underneath this line, we'll add another `&lt;a&gt;` tag, but this time, we'll add an `href` tag in order to have a link to send the page back to the top (line 5 of the code snippet):

{% highlight php linenos %}

                       &lt;/div&gt;&lt;!-- #main --&gt;
       &lt;/div&gt;&lt;!-- .wrapper --&gt;
       &lt;footer id=&quot;colophon&quot; class=&quot;site-footer&quot; role=&quot;contentinfo&quot;&gt;
               &lt;div class=&quot;footer-sidebars&quot;&gt;
                       &lt;a href=#TopOfPage&gt; Your Text Goes Here &lt;/a&gt;
                       &lt;?php get_sidebar( 'footer1' );
                                 get_sidebar( 'footer2' );
                                 get_sidebar( 'footer3' ); ?&gt;
                       &lt;div class=&quot;stripes&quot;&gt;&amp;nbsp;&lt;/div&gt;
               &lt;/div&gt;
{% endhighlight %}

Notice that the link we have here uses the same &quot;TopOfPage&quot; reference as we did in Step 1, this time with a # sign in front of the word. This lets the page code that we want to point to the &quot;TopOfPage&quot; anchor elsewhere on site. Note also that we don't need to make any domain-specific references like we would do with a &quot;normal&quot; http://www.-type of link.

Obviously, feel free to change the reference to &quot;Your Text Goes Here&quot; to be whatever message you'd like the link to say 🙂

## Success!

Once you are done with these two changes, the bottom of your Scrappy WordPress theme should look similar to this:

![scrappy-wordpress-theme-back-to-top](/wp-content/uploads/2012/11/scrappy-wordpress-theme-back-to-top.png)
&lt;p class=&quot;wp-caption-text&quot;&gt;
'Back to Top' link added to the bottom of the Scrappy WordPress theme
&lt;/p&gt;

The styling of the link should be right-aligned to your main article width and the link styling will be handled automatically based on the rules set in your CSS file.</content>
      </item>
      
    
      
      <item>
        <title>Video: Overlay Histogram in R (Normal, Density, Another Series)</title>
        
          <description>&lt;p&gt;This video explains how to overlay histogram plots in R for 3 common cases: overlaying a histogram with a normal curve, overlaying a histogram with a density curve, and overlaying a histogram with a second data series plotted on a secondary axis.&lt;/p&gt;

</description>
        
        <pubDate>Fri, 09 Nov 2012 08:40:01 -0500</pubDate>
        <link>
        http://randyzwitch.com/overlay-histogram-in-r/</link>
        <guid isPermaLink="true">http://randyzwitch.com/overlay-histogram-in-r/</guid>
        <content type="html" xml:base="/overlay-histogram-in-r/">This video explains how to overlay histogram plots in R for 3 common cases: overlaying a histogram with a normal curve, overlaying a histogram with a density curve, and overlaying a histogram with a second data series plotted on a secondary axis.

Note: Towards the end of the video (maybe minute 14 or so), I make a language error when talking about the `padj` parameter in the mtext function...the setting doesn't &quot;left truncated&quot; the label, I meant &quot;right align&quot;, &quot;left align&quot;, etc.

&lt;iframe width=&quot;640&quot; height=&quot;480&quot; src=&quot;http://www.youtube.com/embed/C67KNai92Mo&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;

{% highlight r linenos %}
#Step 0:  load/prepare data

#Read in data
sample_data &lt;- read.csv(&quot;~/Desktop/test_data.csv&quot;)

# &quot;Explode&quot; counts by age back to unsummarized &quot;raw&quot; data
age.exploded &lt;- rep.int(sample_data$age, sample_data$count)


#1. Histogram with normal distributon overlaid or density curve


#1A.  Create histogram
hist(age.exploded, xlim= c(0,20), ylim= c(0,.2), breaks=seq(min(age.exploded),
  max(age.exploded), length=22), xlab = &quot;Age&quot;, ylab= &quot;Percentage of Accounts&quot;,
  main = &quot;Age Distribution of Accounts\n (where 0 &lt;= age &lt;= 20)&quot;,
  prob= TRUE, col= &quot;lightgray&quot;)

#1B.  Do one of the following, either put the normal distribution on the histogram
#     or put the smoothed density function

#Calculate normal distribution having mean/sd equal to data plotted in the
#histogram above
points(seq(min(age.exploded), max(age.exploded), length.out=500),
       dnorm(seq(min(age.exploded), max(age.exploded), length.out=500),
             mean(age.exploded), sd(age.exploded)), type=&quot;l&quot;, col=&quot;red&quot;)

#Add smoothed density function to histogram, smoothness toggled using
#&quot;adjust&quot; parameter
lines(density(age.exploded, adjust = 2), col = &quot;blue&quot;)

#2 Histogram with line plot overlaid

#2A.  Create histogram with extra border space on right-hand side

#Extra border space &quot;2&quot; on right  (bottom, left, top, right)
par(oma=c(0,0,0,2))

hist(age.exploded, xlim= c(0,20), ylim= c(0,.2),
     breaks=seq(min(age.exploded), max(age.exploded), length=22), xlab = &quot;Age&quot;,
     ylab= &quot;Percentage of Accounts&quot;, main = &quot;Age Distribution of Accounts vs. Subscription Rate \n (where reported age &lt;= 20)&quot;,
     prob= TRUE, col= &quot;lightgray&quot;)

#2B.  Add overlaid line plot, create a right-side numeric axis
par(new=T)
plot(sample_data$subscribe_pct, xlab= &quot;&quot;, ylab=&quot;&quot;, type = &quot;b&quot;, col = &quot;red&quot;, axes=FALSE)  
axis(4)

#2C.  Add right-side axis label

mtext(text=&quot;Subscription Rate&quot;,side=4, outer=TRUE, padj=1)
{% endhighlight %}

File Download:

&lt;a title=&quot;Histogram overlay in R&quot; href=&quot;http://randyzwitch.com/wp-content/uploads/2012/11/histogram-overlay-r.zip&quot; target=&quot;_blank&quot;&gt;Histogram overlay in R code and sample data file&lt;/a&gt;</content>
      </item>
      
    
      
      <item>
        <title>Video:  R, RStudio, Rcmdr &amp; rattle</title>
        
          <description>&lt;p&gt;I did a screencast for my co-workers to show how to get started with R, specifically what a base installation of R looks like, then showing how to improve your workflow using RStudio, Rcmdr or rattle.  The examples are somewhat pedestrian, but it gives a feel for what using R actually looks like.&lt;/p&gt;

</description>
        
        <pubDate>Fri, 07 Sep 2012 08:07:45 -0400</pubDate>
        <link>
        http://randyzwitch.com/video-r-rstudio-rcmdr-rattle/</link>
        <guid isPermaLink="true">http://randyzwitch.com/video-r-rstudio-rcmdr-rattle/</guid>
        <content type="html" xml:base="/video-r-rstudio-rcmdr-rattle/">I did a screencast for my co-workers to show how to get started with R, specifically what a base installation of R looks like, then showing how to improve your workflow using RStudio, Rcmdr or rattle.  The examples are somewhat pedestrian, but it gives a feel for what using R actually looks like.

If you have any questions, comments, or jeers about how bad I am at R, feel free to leave a comment in the comments section!

&lt;iframe src=&quot;https://player.vimeo.com/video/48599583&quot; width=&quot;640&quot; height=&quot;400&quot; frameborder=&quot;0&quot; webkitallowfullscreen mozallowfullscreen allowfullscreen&gt;&lt;/iframe&gt;
&lt;p&gt;&lt;a href=&quot;https://vimeo.com/48599583&quot;&gt;R Demo - Randy Zwitch&lt;/a&gt; from &lt;a href=&quot;https://vimeo.com/user13204299&quot;&gt;Keystone Solutions&lt;/a&gt; on &lt;a href=&quot;https://vimeo.com&quot;&gt;Vimeo&lt;/a&gt;.&lt;/p&gt;</content>
      </item>
      
    
      
      <item>
        <title>Getting Started Using R, Part 2: Rcmdr</title>
        
          <description>&lt;p&gt;In my first post in this series, I discussed &lt;a title=&quot;Getting Started Using R, Part 1:  RStudio&quot; href=&quot;http://randyzwitch.com/getting-started-using-rstudio/&quot; target=&quot;_blank&quot;&gt;RStudio&lt;/a&gt;, an IDE that adds significant functionality and consistency to a basic installation of R.  In this post, I will discuss &lt;a title=&quot;Rcmdr download at CRAN&quot; href=&quot;http://cran.r-project.org/web/packages/Rcmdr/index.html&quot; target=&quot;_blank&quot;&gt;Rcmdr&lt;/a&gt;, a GUI that provides the ability to do basic business statistics without having to code in R.&lt;/p&gt;

</description>
        
        <pubDate>Mon, 06 Aug 2012 06:54:18 -0400</pubDate>
        <link>
        http://randyzwitch.com/getting-started-using-r-rcmdr/</link>
        <guid isPermaLink="true">http://randyzwitch.com/getting-started-using-r-rcmdr/</guid>
        <content type="html" xml:base="/getting-started-using-r-rcmdr/">In my first post in this series, I discussed &lt;a title=&quot;Getting Started Using R, Part 1:  RStudio&quot; href=&quot;http://randyzwitch.com/getting-started-using-rstudio/&quot; target=&quot;_blank&quot;&gt;RStudio&lt;/a&gt;, an IDE that adds significant functionality and consistency to a basic installation of R.  In this post, I will discuss &lt;a title=&quot;Rcmdr download at CRAN&quot; href=&quot;http://cran.r-project.org/web/packages/Rcmdr/index.html&quot; target=&quot;_blank&quot;&gt;Rcmdr&lt;/a&gt;, a GUI that provides the ability to do basic business statistics without having to code in R.

## Rcmdr (&quot;R Commander&quot;)

![rcmdr](/wp-content/uploads/2012/08/rcmdr1.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
Example Rcmdr window with the &quot;Statistics&quot; menu expanded
&lt;/p&gt;

&lt;a title=&quot;Rcmdr download at CRAN&quot; href=&quot;http://cran.r-project.org/web/packages/Rcmdr/index.html&quot; target=&quot;_blank&quot;&gt;Rcmdr&lt;/a&gt; is a package for R that was &lt;a title=&quot;Rcmdr main site&quot; href=&quot;http://socserv.mcmaster.ca/jfox/Misc/Rcmdr/&quot; target=&quot;_blank&quot;&gt;created by John Fox&lt;/a&gt; at McMaster University in Canada as a means of providing the basic statistics functionality for classroom use.  In this way, &lt;a title=&quot;Rcmdr download at CRAN&quot; href=&quot;http://cran.r-project.org/web/packages/Rcmdr/index.html&quot; target=&quot;_blank&quot;&gt;Rcmdr&lt;/a&gt; is somewhat similar to [SAS Enterprise Guide](http://www.sas.com/technologies/bi/query_reporting/guide/ &quot;SAS Enterprise Guide&quot;), a GUI that allows quick access to the power of SAS without the requirement of writing code.

While using &lt;a title=&quot;Rcmdr download at CRAN&quot; href=&quot;http://cran.r-project.org/web/packages/Rcmdr/index.html&quot; target=&quot;_blank&quot;&gt;Rcmdr&lt;/a&gt; won't allow you to tap into every single advanced feature that R provides, it does provide a lot of great &quot;general&quot; functionality that can be used in everyday business such as summary statistics, t-tests, ANOVA, linear regression modeling, graphing and data re-coding.

## Using Rcmdr

For the most part, the &lt;a title=&quot;Rcmdr download at CRAN&quot; href=&quot;http://cran.r-project.org/web/packages/Rcmdr/index.html&quot; target=&quot;_blank&quot;&gt;Rcmdr&lt;/a&gt; dialog boxes all look very similar.  Only the most useful options are provided, such as the variable(s) you are looking to interrogate, variable(s) you'd like to break down your analysis by, what statistics you want the output to display (mean, median, mode, etc.) and so on.  The dialog boxes vary depending on whether you are estimating a model or plotting a graph, but in my preliminary usage I haven't found any dialog boxes that were so confusing that I needed to check the &quot;Help&quot; files.

For example, suppose I wanted to make a boxplot of my data, income by job type. To do so, I would go to the &quot;Graphs&quot; menu and select &quot;Boxplot&quot;, which provides me with the following dialog box:

![rcmdr-boxplot-dialog-box](/wp-content/uploads/2012/08/rcmdr-boxplot-dialog-box.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
Rcmdr options for creating a Boxplot
&lt;/p&gt;

![rcmdr-boxplot](/wp-content/uploads/2012/08/rcmdr-boxplot.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
Boxplot output created by Rcmdr
&lt;/p&gt;

Within this dialog box, there are only 3 choices:  variable to plot (income), variable to break down the graph by (type), and &quot;Identify outliers with mouse&quot;, which allows for the user to point at the resulting graph to designate outliers to be labeled on the graph.  When I click &quot;OK&quot; in the dialog box, the result is the boxplot shown above. We can see that the &quot;bc&quot; (blue-collar) group has a lower mid-point to the  income range than &quot;prof&quot; (professors) and &quot;wc&quot; (white-collar).

One of the best features of &lt;a title=&quot;Rcmdr download at CRAN&quot; href=&quot;http://cran.r-project.org/web/packages/Rcmdr/index.html&quot; target=&quot;_blank&quot;&gt;Rcmdr&lt;/a&gt; is that not only do we get the output we requested, but the code window also shows the code that was necessary to create the boxplot.  In this example, the underlying R code is relatively simple:

{% highlight r linenos %}
boxplot(income~type, ylab=&quot;income&quot;, xlab=&quot;type&quot;, data=Duncan)
{% endhighlight %}

By providing the underlying code, [Rcmdr](http://cran.r-project.org/web/packages/Rcmdr/index.html &quot;Rcmdr download at CRAN&quot;) serves as a teaching tool to move the beginning user towards coding in R directly, or at least, modifying the tool-generated code to include titles or whatever options the user wants to add to the original analysis/output.

## Installation of Rcmdr

Sadly, [Rcmdr](http://cran.r-project.org/web/packages/Rcmdr/index.html &quot;Rcmdr download at CRAN&quot;) is one of those add-ins that seems to work better on Windows than Mac OSX, at least for the installation portion. I've been able to successfully install Rcmdr on my relatively old MacBook Pro, but it did take a bit of time to figure out.  Luckily, the instructions to install &lt;a title=&quot;Rcmdr download at CRAN&quot; href=&quot;http://cran.r-project.org/web/packages/Rcmdr/index.html&quot; target=&quot;_blank&quot;&gt;Rcmdr&lt;/a&gt; on a Mac are fairly well laid out in [this article](http://wiki.math.yorku.ca/index.php/R:Installing_R_and_Rcmdr_on_a_MAC &quot;Rcmdr on Mac OSX&quot;).

However, once you get over the hurdle of downloading tcltk and XQuartz (X11 emulator), the program seems to work the same on both platforms.</content>
      </item>
      
    
      
      <item>
        <title>Getting Started Using R, Part 1:  RStudio</title>
        
          <description>&lt;p&gt;&lt;del&gt;Despite &lt;a href=&quot;http://randyzwitch.com/learning-r-sas/&quot; title=&quot;Learning R has really made me appreciate SAS&quot;&gt;my preference for SAS over R&lt;/a&gt;,&lt;/del&gt; there are some add-ons to “basic” R that I’ve found that have made my learning process way easier. While I’m still in my infancy in learning R, I feel like once I found these additional tools, my ability to use R to get work done improved significantly.&lt;/p&gt;

</description>
        
        <pubDate>Sat, 04 Aug 2012 07:58:14 -0400</pubDate>
        <link>
        http://randyzwitch.com/getting-started-using-rstudio/</link>
        <guid isPermaLink="true">http://randyzwitch.com/getting-started-using-rstudio/</guid>
        <content type="html" xml:base="/getting-started-using-rstudio/">&lt;del&gt;Despite [my preference for SAS over R](http://randyzwitch.com/learning-r-sas/ &quot;Learning R has really made me appreciate SAS&quot;),&lt;/del&gt; there are some add-ons to &quot;basic&quot; R that I've found that have made my learning process way easier. While I'm still in my infancy in learning R, I feel like once I found these additional tools, my ability to use R to get work done improved significantly.

In this first post of three, I'll discuss &lt;a title=&quot;R Studio main site&quot; href=&quot;http://rstudio.org/&quot; target=&quot;_blank&quot;&gt;RStudio&lt;/a&gt;, a more friendly access point to the default installation of R.  My second post will discuss &lt;a title=&quot;Rcmdr download at CRAN&quot; href=&quot;http://cran.r-project.org/web/packages/Rcmdr/index.html&quot; target=&quot;_blank&quot;&gt;Rcmdr&lt;/a&gt;, a GUI developed for students taking a basic college-level course in Statistics.  The third post will cover &lt;a title=&quot;rattle download CRAN&quot; href=&quot;http://cran.r-project.org/web/packages/rattle/index.html&quot; target=&quot;_blank&quot;&gt;rattle&lt;/a&gt;, a GUI specifically designed for data mining (as opposed to more general statistics like &lt;a title=&quot;Rcmdr download at CRAN&quot; href=&quot;http://cran.r-project.org/web/packages/Rcmdr/index.html&quot; target=&quot;_blank&quot;&gt;Rcmdr&lt;/a&gt;).

## RStudio

![r-studio](/wp-content/uploads/2012/08/r-studio.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
R Studio is an IDE that dramatically improves the R experience
&lt;/p&gt;

&lt;a title=&quot;R Studio download&quot; href=&quot;http://rstudio.org/download/&quot; target=&quot;_blank&quot;&gt;RStudio&lt;/a&gt; is an open-source Integrated Development Environment (IDE) that provides a more consistent user experience to R.  There are many great features of &lt;a title=&quot;R Studio download&quot; href=&quot;http://rstudio.org/download/&quot; target=&quot;_blank&quot;&gt;RStudio&lt;/a&gt; over &quot;basic&quot; R, including:

  * Consistent windowing between sessions (customizable by the user)
  * Point-and-click exploration of data frames and other data objects
  * Importing data files through dialog box functionality
  * Customizable code syntax highlighting, auto-complete, and Help menu access from the code editor
  * Ability to see all installed packages, turn on packages using a checkbox, and download libraries (and their dependencies) without having to write any code
  * Version Control using GitHub

While &lt;a title=&quot;R Studio download&quot; href=&quot;http://rstudio.org/download/&quot; target=&quot;_blank&quot;&gt;RStudio&lt;/a&gt; doesn't provide a GUI that will help you run a regression model or build a graph, it provides a more &quot;friendly&quot; environment to work in as compared to the command-line interface of a default installation of R.  I find that by having elements like the currently active data objects and available/active packages with links to the Help files &quot;exposed&quot; at all times, [RStudio](http://rstudio.org/download/ &quot;R Studio download&quot;) reminds me of where my analysis has been and gives me a quick way to think about &quot;What Else?&quot; to pursue if I hit a roadblock.

## Installation of RStudio

&lt;a title=&quot;R Studio download&quot; href=&quot;http://rstudio.org/download/&quot; target=&quot;_blank&quot;&gt;RStudio&lt;/a&gt; installs like any other program for Windows or Mac OSX.  As far as I can tell, there are no advantages to using &lt;a title=&quot;R Studio download&quot; href=&quot;http://rstudio.org/download/&quot; target=&quot;_blank&quot;&gt;RStudio&lt;/a&gt; in either environment, both the Windows and OSX versions seem to work equally well.  The most important consideration is that RStudio is just an &quot;add-on&quot; so-to-speak, it does not include R itself.  So be sure to go to one of the &lt;a title=&quot;CRAN downloads for R&quot; href=&quot;http://cran.cs.wwu.edu/&quot; target=&quot;_blank&quot;&gt;Comprehensive R Archive Network (CRAN) sites&lt;/a&gt; to download R first.</content>
      </item>
      
    
      
      <item>
        <title>Learning R Has Really Made Me Appreciate SAS</title>
        
          <description>&lt;p&gt;EDIT, 9/9/2016: Four years later, this blog post is a comical look back in time. It’s hard to believe that I could think this way! Having used R (and Python, Julia), I will never return back to the constraints of using SAS. The inflexible nature of everything having to be a Dataset in SAS vs. the infinite flexibility of data structures in programming-oriented languages makes it no contest.&lt;/p&gt;

</description>
        
        <pubDate>Wed, 25 Jul 2012 07:34:03 -0400</pubDate>
        <link>
        http://randyzwitch.com/learning-r-sas/</link>
        <guid isPermaLink="true">http://randyzwitch.com/learning-r-sas/</guid>
        <content type="html" xml:base="/learning-r-sas/">EDIT, 9/9/2016: Four years later, this blog post is a comical look back in time. It's hard to believe that I could think this way! Having used R (and Python, Julia), I will never return back to the constraints of using SAS. The inflexible nature of everything having to be a Dataset in SAS vs. the infinite flexibility of data structures in programming-oriented languages makes it no contest.

But I'll leave this here to remind myself how today's frustration leads to tomorrow's breakthroughs.

&lt;hr&gt;

For the past 18 months, it seems like all I've heard about in the digital marketing industry is &quot;big data&quot;, and with that, mentions of using Hadoop and R to solve these sorts of problems.  Why are these tools the most often mentioned?  Because they are open source, i.e. free of charge!

But as I've tried to learn R, I keep asking myself...are all of my colleagues out of their minds?  Or, am I just beyond learning something new?  As of right now, R is just one big hack on top of a hack to me, and the software is only &quot;free&quot; if you don't consider lost productivity.

## Need new functionality, just download another R package!

One of the biggest &quot;pros&quot; I see thrown around for R relative to a tool like SAS is that when new statistical techniques are invented, someone will code it in R immediately.  A company like SAS make take 5 years to implement the feature, or it may not get implemented at all.  That's all fine and good, but the problem I've found is that there are 10 ways to do something in R, and I spend more time downloading packages (along with other packages that are dependencies) than I do learning A SINGLE WAY to do something correctly.

For example, take trying to get summary statistics by group.  In SAS, you use a Proc Summary statement, with either a BY group statement or a CLASS statement.  It's fairly simple and it works.

&gt; proc summary data= hs0; var \_numeric\_; class prgtype; output out=results mean= /autolabel autoname inherit; run;

In R, I ran the following code, which should be roughly equivalent:

&gt; by(hs0, hs0$prgtype, mean)

Very simple, fewer lines...and technically wrong, throwing a 6 unhelpful errors for a single line of code.  Because it was decided that &quot;mean&quot; as a function would be deprecated in R.  WHY???  It's so simple, why modify the language like that?

According to the error message, I'm supposed to use colMeans instead...but once you get to how, you're on your own, the Help documentation is garbage.  Some combination of &quot;by&quot; and &quot;colMeans&quot; might work, but I don't have an example to follow.

Google sent me to the &lt;a title=&quot;Quick-R website&quot; href=&quot;http://www.statmethods.net/&quot; target=&quot;_blank&quot;&gt;Quick-R&lt;/a&gt; website, and I found a &quot;&lt;a title=&quot;Descriptive Statistics in R&quot; href=&quot;http://www.statmethods.net/stats/descriptives.html&quot; target=&quot;_blank&quot;&gt;descriptive statistics&lt;/a&gt;&quot; article with by group processing...with the recommendation of using the &quot;psych&quot; package or the &quot;doBy&quot; package.  But &lt;a title=&quot;Comprehensive R Archive Network&quot; href=&quot;http://cran.cs.wwu.edu/&quot; target=&quot;_blank&quot;&gt;CRAN&lt;/a&gt; won't let me download all of the dependencies, so again, stuck trying to do the simplest thing in statistics.

## Let's be fast and run everything in RAM!

My next favorite hassle in R is that you are expected to continuously monitor how many data elements you have active in a workspace.  R runs completely in RAM (as opposed to SAS which runs a combination of RAM for processing and hard disks for storage), so if you want to do something really &quot;big&quot;, you will quickly choke your computer.  I tried to work with a _single day_ of Omniture data from the raw data feed, and my MacBook Pro with 6GB of memory was shot.  I believe the file was 700,000 rows by 300 columns, but I could be mis-remembering.  That's not even enough data to think about performance-tuning a program in SAS, any slop code will run quickly.

How does one solve these memory errors in R?  Port to Amazon cloud seems to be the most commonly given suggestion.  But that's more setup time, getting an R instance over to Amazon, your data over to Amazon..and now you are renting hardware.

## R is great for data visualization!

From what I've seen from the demo(graphics) tutorial, R does have some pretty impressive visualization capabilities.  Contour maps, histograms, boxplots...there seems to be a lot of capability here beyond the realm of a tool like Excel (which, besides not being free, isn't really for visualization).  SAS has some graphics capabilities, but they are a bit hard to master.

But for all of the hassle to get your data formatted properly, downloading endless packages, avoiding memory errors, you could just pay for Tableau and get working.  Then, once you have your visualizations done in Tableau, if you are using Tableau server you can share interactive dashboards with others.  As far as I know, R graphics are static image exports, so you're stuck with &quot;flat&quot; presentations.

## Maybe, it's just me

For R diehards, the above verbiage probably just sounds like whining from someone who is too new to appreciate the greatness of R or too stuck in the &quot;old SAS way&quot;.  That's certainly possible.  But from my first several weeks of trying to use R, the level of frustration is way beyond anything I experienced when I was learning SAS.

Luckily, I don't currently have any consulting projects that require R or SAS at the moment, so I can continue to try and learn why everyone thinks R is so great.  But from where I sit right now, the licensing fee from SAS doesn't seem so bad when it allows me to get to doing productive work instead of building my own statistics software piece-by-piece.</content>
      </item>
      
    
      
      <item>
        <title>My Top 20 Least Useful Omniture Reports</title>
        
          <description>&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2012/07/data-squirrel-262x300.png&quot; alt=&quot;data-squirrel&quot; /&gt;&lt;/p&gt;

</description>
        
        <pubDate>Tue, 17 Jul 2012 20:14:04 -0400</pubDate>
        <link>
        http://randyzwitch.com/least-useful-omniture-reports/</link>
        <guid isPermaLink="true">http://randyzwitch.com/least-useful-omniture-reports/</guid>
        <content type="html" xml:base="/least-useful-omniture-reports/">![data-squirrel](/wp-content/uploads/2012/07/data-squirrel-262x300.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
Just because data CAN be captured doesn't mean it SHOULD be!
&lt;/p&gt;

In a prior post about [customizing the SiteCatalyst menu interface](http://randyzwitch.com/customize-adobe-sitecatalyst-menu/ &quot;For maximum user understanding, customize the SiteCatalyst menu&quot;), I discussed how simple changes such as hiding empty Omniture variables/reports and re-organizing the menu structure will help improve understanding within your organization.  In the spirit of even further interface optimization, here are 20 reports within Omniture that I feel that can be hidden due to their lack of business-actionable information.

Here are my Top 20, in no particular order:

  * Mobile:  Color Depth
  * Mobile:  Information Services
  * Mobile:  Decoration Mail Support
  * Mobile:  PTT
  * Mobile:  Device Number Transmit
  * Mobile:  Browser URL Length
  * Mobile:  DRM
  * Mobile:  Mail URL Length
  * Mobile:  Java version
  * Mobile:  Manufacturer
  * Technology:  Connection Types
  * Technology:  Monitor Color Depth
  * Technology:  JavaScript Version
  * Technology:  Monitor Resolutions
  * Visitor Profile:  Top-Level Domains
  * Visitor Profile:  Domains
  * Visitor Profile:  Geosegmentation
  * Traffic Sources:  All Search Page Ranking
  * Traffic Sources: Original Referring Domains
  * Custom Variable:  s.server report

## Mobile reports

For the most part, the information in the separate reports can determined just by knowing the device (which is also a default Omniture report). So, a single report can take the place of 10.

There's also the pesky issue that the reports more often than not show &quot;Unknown&quot; for 90%+ of the mobile traffic (at least, in the U.S.).  So not only can the data be determined from knowing the mobile device being used, the additional reports aren't even well populated.

## Technology reports

The &quot;Connection Type&quot; report, along with &quot;Monitor Color Depth&quot;, measure things that haven't been an issue in too many years to continue reporting on. LAN, 16-bit or higher.

&quot;Monitor resolution&quot; is irrelevant in the face of also having &quot;Browser Width&quot; &amp; &quot;Browser Height&quot; reports (the true size of the web page &quot;real estate&quot; on screen).

Finally, JavaScript version?  The JavaScript report with &quot;Enabled/Disabled&quot; is likely more than enough information.  Or, you can just include jQuery in your website and know with 100% certainty what version is being used.

## Visitor Profile reports

My dislike of the identified Visitor Profile reports are due to halfway implementation.  The &quot;GeoSegmentation report shows a nice map representation, but only of traffic metrics like Page Views and Visits.  Why not open this up to conversion variables and really make the visualization useful, instead of needing to rely on the &quot;flat&quot;, non-map Visitor Zip (`s.zip`) report?

For the &quot;Domains&quot; and &quot;Top-Level Domains&quot; report, you have granularity issues; the &quot;Top-Level Domains&quot; report is sort-of a country-level report, but the U.S. has several line items.  The &quot;Domains&quot; report shows what ISP people are using to access the Internet (which I think is generally useless in itself), but again...it spans geography, so the ISP network someone is on may not even have the same technology.  So what are we really measuring in these reports?

## Traffic Sources reports

The &quot;All Search Page Ranking&quot; report seems like it could be useful, until you realize that 1) it aggregates all search engines (whose different algorithms provide different rankings and 2) with personalized search, rankings are no longer static. Literally every single person could see a different link position for the same search term.  So while this report may have made sense for SEO measurement in the past, it's really past it's prime...use the right SEO tool for the job (Conductor, SEOmoz, and the like).

The &quot;Original Referring Domains&quot; report is weird in its own way...the absolute first URL that referred you to the site.  Really?  As Avinash has said, giving 100% credit to the first touchpoint is like giving your first girlfriend credit for you marrying your wife (paraphrased).  This report is very limited in its usefulness IMO, especially given the advances in attribution modeling in the past several years.

## Custom Variable:  s.server report

The only custom variable report I have on this list is the `s.server` report; hopefully, all of your other custom variables are capturing only business-useful information!

The reason I dislike the `s.server` variable/report is the same reason I dislike the &quot;All Search Page Ranking&quot; report; use the right tool for the job.  This is a lazy way of monitoring server volume for load balancing.  But if you're doing the job well on the back-end, shouldn't every server have the same level of volume?

Even if the answer to the previous question is no (I'm not a network engineer, clearly), having an _operational_ report like this doesn't make much sense to me in a _marketing_ reporting tool.

## Hide in the menu, don't restrict access

By hiding reports in the Omniture menu interface, this doesn't mean the info stops being collected or becomes unavailable to all users.  Rather, the option to use the reports isn't immediately obvious (since they don't show up in the menu).  Power Users can still find these reports using the search box if necessary to answer an oddball question.

But in my experience, the information in these reports are generally not business useful, or are lacking in some critical way.  If you can't make _regular, high impact decisions_ with the info, then you're better off never looking at it at all.</content>
      </item>
      
    
      
      <item>
        <title>Apple Has Earned a Customer for Life</title>
        
          <description>&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2012/06/macbook-pro-broken-hinge-screen.jpg&quot; alt=&quot;macbook pro broken hinge&quot; /&gt;&lt;/p&gt;

</description>
        
        <pubDate>Mon, 25 Jun 2012 16:38:57 -0400</pubDate>
        <link>
        http://randyzwitch.com/broken-macbook-pro-hinge-fixed-free/</link>
        <guid isPermaLink="true">http://randyzwitch.com/broken-macbook-pro-hinge-fixed-free/</guid>
        <content type="html" xml:base="/broken-macbook-pro-hinge-fixed-free/">![macbook pro broken hinge](/wp-content/uploads/2012/06/macbook-pro-broken-hinge-screen.jpg)

 &lt;p class=&quot;wp-caption-text&quot;&gt;
    Broken MacBook Pro hinge (due to glue failure)
 &lt;/p&gt;

I used to think that when people talked about the &quot;legendary Apple customer service&quot; that there was plenty of hyperbole thrown in for good measure.  Until it happened to me with my broken MacBook Pro hinge.

## &quot;Broken MacBook Pro Hinge&quot; - Plenty of search results

When the screen on my late 2008 15&quot; MacBook Pro started separating from the hinge, the first thing I did was search Google.  There I found more than enough &lt;a title=&quot;Broken MacBook Pro Google search results&quot; href=&quot;http://www.google.com/#hl=en&amp;sclient=psy-ab&amp;q=broken+Macbook+pro+hinge&amp;oq=broken+Macbook+pro+hinge&amp;aq=f&amp;aqi=g-K1g-bK1g-bsK1g-bK1&amp;aql=&amp;gs_l=hp.3..0i30j0i8i30j0i8i10i30j0i8i30.1489.8286.0.8403.32.24.2.4.4.0.429.4546.1j13j7j1j1.23.0...1.0.d5zdW3pAo3g&amp;pbx=1&amp;bav=on.2,or.r_gc.r_pw.r_qf.,cf.osb&amp;fp=c1e99b5acbebabce&amp;biw=1600&amp;bih=702&quot; target=&quot;_blank&quot;&gt;search results&lt;/a&gt; to make me believe this was a widespread issue with this vintage of laptop.  And since the laptop was out of warranty, most of the results talked about re-gluing the aluminum screen cover to the hinge.

After trying to re-attach the hinge to the screen using epoxy, I headed over to the Apple store in King of Prussia, PA.  To say this first encounter at the Genius Bar was frustrating is an understatement.

## You should've bought AppleCare

Apple &lt;del&gt;cashiers&lt;/del&gt; &quot;Geniuses&quot; and fanboys alike are very big on pushing the AppleCare warranty, selling you with tales that Apple will fix _anything_ in that extended time period.  While that may be true, extended warranties generally don't pay off for the consumer, and as such, I don't buy them.

Not that it would have mattered for me anyway.  My MacBook Pro is well beyond 3 years old, one of the first unibody models that came out.  You think the Apple &quot;Genius&quot; would've known that after checking the serial number, but instead just kept repeating robotically:

&gt; &quot;You should've bought AppleCare.  You should've bought AppleCare.&quot;

Even when I asked, &quot;A glue failure doesn't seem like a manufacturers defect?&quot; or &quot;I should've paid $349 for an extended warranty to protect against $0.05 of faulty glue?&quot;

&gt; &quot;You should've bought AppleCare.&quot;

At that point, after being asked if I dropped the laptop, given a series of robotic answers, suggested that I should've spend $349 that wouldn't have fixed my problem, and generally treated like a monkey, I felt like smashing the laptop right on the Genius Bar just to make a scene.  Instead, I walked out feeling worse than when I arrived, with crippled MacBook Pro in hand.

## Maybe an Apple Certified Repair facility can help

Since I wasn't going back for a second round of stupidity at King of Prussia Apple Store, I decided to look up an independent shop to see what the cost of repair would be.  The repair guy immediately said &quot;Oh, I've seen this a few times recently...it's probably around $500-$600 to fix.&quot;

$%^$&amp;%*(#!  For $600, I'd be about 30-35% of a new 15&quot; MacBook Pro.  Again I left a store without doing anything, and feeling worse than when I arrived.  I either need to pay $600 or pay $2000+ to get the newer equivalent of my laptop.

## One more trip to the Apple Store

Several weeks had passed and my laptop became pretty much unusable.  I decided to bite the bullet and pay to get the screen fixed.  I also decided to go back to an Apple Store (this time, in Ardmore, PA) to have them fix it.  I figured if I'd have to pay, might as well guarantee it would get fixed properly.

When I walked up to the Genius Bar, the Apple &quot;Genius&quot; still asked me if I dropped my laptop (_sidebar:  Is this part of the mind tricks they give everyone?  There isn't a scratch on the thing, let alone any dents_).  After the Apple employee looked over the laptop, I told him in my most dejected voice that I wanted to find out how much is was to replace the screen.

&gt; Apple Genius:  &quot;How about 'free'?&quot;

I damn near fell off the stool I was sitting on.  How could the Apple Store in King of Prussia been so unhelpful, and then 5 minutes into the same explanation I get an offer to get the screen fixed FREE at the Suburban Square Apple Store in Ardmore?

&gt; Apple Genius:  &quot;And we can probably get this back to you by tomorrow.&quot;

Needless to say, I didn't want to do anything except hit 'Accept' on the electronic repair form.  I've come too far to mess this gift up!

## Apple, you've earned yourself a lifetime customer

Maybe I got lucky.  Maybe it was perseverance.  Maybe this screen/hinge defect has shown up too many times in the last six weeks and Apple could no longer ignore it.

Maybe it's because I asked twice at two different Genius appointments. Or maybe Apple has realized I've spent several thousand dollars with them in the past several years, with this MacBook Pro, iMac, several iPhones and an iPad.  That level of spend probably doesn't even get me in the top 50% of non-business customers, but it's not negligible either.

Whatever the reason, by comping me the $492.41, Apple has &quot;bought&quot; themselves a customer for life.

![em209-mac-repair-order](/wp-content/uploads/2012/06/mac-repair-order.png)

 &lt;p class=&quot;wp-caption-text&quot;&gt;
    The cost of a broken MacBook Pro hinge? Apparently, $492.41!
 &lt;/p&gt;

Edit: To read the follow-up of what eventually ended up of this MacBook Pro, [click here](http://randyzwitch.com/apple-macbook-pro-model-a1286-late-2008-vintage/) for an article about me replacement battery interaction with Apple.</content>
      </item>
      
    
      
      <item>
        <title>Where's The Relevance, Twitter?</title>
        
          <description>&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2012/06/twitter-promoted-tweets-spam.png&quot; alt=&quot;twitter-promoted-tweets-spam&quot; /&gt;&lt;/p&gt;

</description>
        
        <pubDate>Thu, 14 Jun 2012 04:42:36 -0400</pubDate>
        <link>
        http://randyzwitch.com/twitter-promoted-tweets/</link>
        <guid isPermaLink="true">http://randyzwitch.com/twitter-promoted-tweets/</guid>
        <content type="html" xml:base="/twitter-promoted-tweets/">![twitter-promoted-tweets-spam](/wp-content/uploads/2012/06/twitter-promoted-tweets-spam.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
Left: #WWDC (Apple developer conference)           Right: #EdLoverDance
&lt;/p&gt;

Before we get started, let's get something out of the way:  _Yes, I know I'm complaining about a free service that I don't have to use.  And further, it's their platform to do whatever they want..._

I can't fault Twitter for wanting to monetize their platform.  Given that Twitter (and pretty much every social network) would collapse if they started charging users, selling ads is pretty much the de facto next move.  But given the recent declaration by &lt;a title=&quot;GM drops Facebook ads&quot; href=&quot;http://thenextweb.com/facebook/2012/05/15/gm-to-drop-facebook-advertising-citing-poor-results/&quot; target=&quot;_blank&quot;&gt;GM that they dropped Facebook ads&lt;/a&gt; because they &quot;don't work&quot;, why isn't Twitter making a better effort to provide relevance with their Promoted Tweets?

## Promoted Tweet 1:  Apple Worldwide Developer Conference vs. Pepsi/Nicki Minaj

Like many, I was interested in reading about the Apple Worldwide Developer Conference.  Rather than wait for Engadget to summarize the conference, I checked into Twitter to see what others were tweeting.

_BOOM!  Nicki Manaj!  Barbie shot a commercial, gave up her underground hood status!_

Uh, ok.  Can't think of anything further from a tech conference than a rapper whose name is a no-so-subtle reference to sex.

## Promoted Tweet 2:  Ed Lover Dance vs. Dell for Business

&lt;iframe width=&quot;560&quot; height=&quot;315&quot; src=&quot;https://www.youtube.com/embed/99lwNnrUNs8&quot; frameborder=&quot;0&quot; allowfullscreen&gt;&lt;/iframe&gt;

If you don't know the Ed Lover Dance, you're either not a fan of hip-hop or you're not as old as me.  Regardless, one thing the Ed Lover Dance is NOT is Dell for Business.  Unless, of course, someone has a video of Michael Dell doing the Ed Lover Dance, in that case, carry on...

## Let's do better, Twitter...I've given you 3,500 opportunities to learn about me

Twitter, I've given you 3,500 opportunities to learn about me through my tweets.  Yes, sometimes I write utter nonsense.  But even then, you can get at least some idea of who I am.

You also know the other Twitter accounts I interact with regularly.  Klout is making an effort to figure me out and what topics I'm influential about.  You've got the source data Twitter, start mining!

Start providing some relevance in the Promoted Tweets, and maybe my click-through rate will increase from 0.00% to 0.01%!</content>
      </item>
      
    
      
      <item>
        <title>Get Rich With Google AdSense And WordPress!</title>
        
          <description>&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2012/06/google-adsense-wordpress-300x240.png&quot; alt=&quot;Google Adsense graphic&quot; /&gt;&lt;/p&gt;

</description>
        
        <pubDate>Thu, 07 Jun 2012 04:37:28 -0400</pubDate>
        <link>
        http://randyzwitch.com/google-adsense-and-wordpress/</link>
        <guid isPermaLink="true">http://randyzwitch.com/google-adsense-and-wordpress/</guid>
        <content type="html" xml:base="/google-adsense-and-wordpress/">![Google Adsense graphic](/wp-content/uploads/2012/06/google-adsense-wordpress-300x240.png)

There are as many reasons to blog as there are people on Earth.  Whether it's to use a blog as a personal diary, a means to share something you are passionate about (like &lt;a title=&quot;Food blog&quot; href=&quot;http://zwitchen.com/&quot; target=&quot;_blank&quot;&gt;cooking&lt;/a&gt;, as my wife does), a &quot;voice&quot; for your professional career or something else, eventually the question comes up:  should I try and earn advertising income from Google AdSense from my readership?

If you're on the wordpress.com free site, the answer is easy:  you can't, no user-created JavaScript is allowed.  If you're on Google's Blogger, &lt;a title=&quot;Blogger and Google AdSense&quot; href=&quot;http://support.google.com/blogger/bin/answer.py?hl=en&amp;answer=42534&quot; target=&quot;_blank&quot;&gt;integrating AdSense code&lt;/a&gt; is easy.  And if you're self-hosting using WordPress, Joomla or whatever, you can do whatever you want. But the question remains, is it worth it to have Google AdSense ads?

In my opinion, unless you've got a massive &quot;clicky&quot; readership, probably not.

## How much traffic is &quot;enough&quot; to make money from Google AdSense?

As you can see from this blog (as of time of writing at least), I'm running Google AdSense on this blog, which is primarily WordPress and Web Analytics themed.  I'm also running ads on my other blog, &lt;a title=&quot;The Fuqua Experience&quot; href=&quot;http://the-fuqua-experience.com&quot; target=&quot;_blank&quot;&gt;The Fuqua Experience&lt;/a&gt;, which is truly a niche blog about the Duke Cross Continent MBA program. So two niche blogs, relatively speaking (i.e. not celebrity gossip, technology rumors, politics, or other general interest topics).

On average, there are 2-3 ads per page (primarily leaderboards and skyscrapers), which is the limit for Google.  So much money am I making?  Less than the cost of Deluxe Hosting with GoDaddy!

## CPM, CPC...what's the most efficient way to make money using Google AdSense?

When looking at the Google AdSense reporting, it's clear that &quot;Cost per Click&quot; is the way to make money with Google AdSense. A few thousand page views will get you a few pennies (Cost per Thousand impressions, or CPM), but an actual click-through to the advertisers website will get you something like 10x the CPM rate.  Here's a chart of my of weekly performance over 28 months or so:

![google-adsense-performance](/wp-content/uploads/2012/06/google-adsense-performance.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
Some weeks I make a few bucks, many I make nothing!
&lt;/p&gt;

It's easy to see that even with 3,000-6,000 page views per week across my two blogs, I'm not making a ton of money.  If my audience feels particularly &quot;clicky&quot; on the contextual ads Google AdSense serves, I make between $2-$5 per week.  GoDaddy Deluxe Hosting costs something like $6 per month for unlimited websites on a shared server, so clearly I'm not breaking the bank here!  If I'm lucky, I'm clearing a few dollars per month in profit (excluding the time I actually maintain the two blogs through writing, site development, etc.)

## So, who IS making money through Google AdSense advertising?

Monetizing a blog is a Catch-22.  If you don't have enough readership, you won't make a ton of money.  If you do have a huge readership like Drudge or Perez Hilton, you can sell ads directly to advertisers without needing the Google AdSense network.  Somewhere in-between, it MAY be worth adding Google AdSense or participating in other affiliate marketing programs.

Heck, maybe you're an SEO god with a whole network of MFA (Made-for-AdSense) blogs with highly targeted content.  I do have friends who seem to make enough money through these schemes to make it &quot;worth it&quot; to do.  Especially if you're willing to put in the time to make dozens, if not hundreds of individual blog sites.

That said, it's up to the individual blog owner what constitutes &quot;worth it&quot; in the trade-off between spending time to generate residual income.  For me, I leave the Google AdSense ads up as a learning experience; it's good in my industry (digital analytics) to understand all of the Google tools.  And really, that is why I blog at all; to practice &lt;a title=&quot;Google Analytics tutorials&quot; href=&quot;http://randyzwitch.com/tag/google-analytics/&quot; target=&quot;_blank&quot;&gt;implementing Google Analytics&lt;/a&gt;, learn PHP and JavaScript through &lt;a title=&quot;WordPress tutorials&quot; href=&quot;http://randyzwitch.com/category/wordpress-tutorials/&quot; target=&quot;_blank&quot;&gt;customizing WordPress&lt;/a&gt;, and occasionally pontificate on the &lt;a title=&quot;Digital Analytics&quot; href=&quot;http://randyzwitch.com/category/web-analytics/&quot; target=&quot;_blank&quot;&gt;digital analytics&lt;/a&gt; industry.</content>
      </item>
      
    
      
      <item>
        <title>For Maximum User Understanding, Customize the SiteCatalyst Menu</title>
        
          <description>&lt;blockquote class=&quot;twitter-tweet&quot; data-conversation=&quot;none&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;&lt;a href=&quot;https://twitter.com/nancyskoons&quot;&gt;@nancyskoons&lt;/a&gt; &lt;a href=&quot;https://twitter.com/randyzwitch&quot;&gt;@randyzwitch&lt;/a&gt; &lt;a href=&quot;https://twitter.com/shawncreed&quot;&gt;@shawncreed&lt;/a&gt; Best Practice #1: Customizing anything is better than customizing nothing. &lt;a href=&quot;https://twitter.com/hashtag/measure?src=hash&quot;&gt;#measure&lt;/a&gt; &lt;a href=&quot;https://twitter.com/hashtag/omniture?src=hash&quot;&gt;#omniture&lt;/a&gt;&lt;/p&gt;&amp;mdash; Jason Egan (@jasonegan) &lt;a href=&quot;https://twitter.com/jasonegan/status/210398632082538497&quot;&gt;June 6, 2012&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async=&quot;&quot; src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

</description>
        
        <pubDate>Wed, 06 Jun 2012 13:31:39 -0400</pubDate>
        <link>
        http://randyzwitch.com/customize-adobe-sitecatalyst-menu/</link>
        <guid isPermaLink="true">http://randyzwitch.com/customize-adobe-sitecatalyst-menu/</guid>
        <content type="html" xml:base="/customize-adobe-sitecatalyst-menu/">&lt;blockquote class=&quot;twitter-tweet&quot; data-conversation=&quot;none&quot; data-lang=&quot;en&quot;&gt;&lt;p lang=&quot;en&quot; dir=&quot;ltr&quot;&gt;&lt;a href=&quot;https://twitter.com/nancyskoons&quot;&gt;@nancyskoons&lt;/a&gt; &lt;a href=&quot;https://twitter.com/randyzwitch&quot;&gt;@randyzwitch&lt;/a&gt; &lt;a href=&quot;https://twitter.com/shawncreed&quot;&gt;@shawncreed&lt;/a&gt; Best Practice #1: Customizing anything is better than customizing nothing. &lt;a href=&quot;https://twitter.com/hashtag/measure?src=hash&quot;&gt;#measure&lt;/a&gt; &lt;a href=&quot;https://twitter.com/hashtag/omniture?src=hash&quot;&gt;#omniture&lt;/a&gt;&lt;/p&gt;&amp;mdash; Jason Egan (@jasonegan) &lt;a href=&quot;https://twitter.com/jasonegan/status/210398632082538497&quot;&gt;June 6, 2012&lt;/a&gt;&lt;/blockquote&gt;
&lt;script async src=&quot;//platform.twitter.com/widgets.js&quot; charset=&quot;utf-8&quot;&gt;&lt;/script&gt;

![stock-menu](/wp-content/uploads/2012/06/stock-menu-109x300.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
Default Omniture report menu
&lt;/p&gt;

Visits vs. Visitors vs. Unique Visitors...click-throughs, view-throughs, bounces...these concepts in digital analytics are fairly abstract, and many in business and marketing never really grasp the concepts fully.  Knowing the enormous amount of learning that needs to take place for digital success, why do we make our internal stakeholders hunt for data that's organized by TOOL definitions, instead of by business function?

In this case, the &quot;tool&quot; that I'm referring to here is Omniture SiteCatalyst.  To be clear, there's nothing excessively _wrong_ about the default menu structure in Omniture, just that in my experience, understanding by end-users can be greatly enhanced by customizing the Omniture menu.

Simple modifications such as 1) Hiding Omniture variables and products not in use, 2) organizing reports by logical business function, and 3) placing custom reports and calculated metrics next to the standard SiteCatalyst reports will get users to making decisions with their data that much faster.

## 1)  Hide Omniture variables and products not being used

Do your users a favor and hide the Omniture products such as Test &amp; Target, Survey, and Genesis if you aren't using them.  Same thing with any custom traffic (props) and custom conversion variables (eVars) that aren't being used.  Nothing will distract your users faster than clicking on folders with advertisements (T&amp;T, Survey) or worse, frustrate the user by making them wonder &quot;What data is _supposed to be_ in this report?&quot;

Just by hiding or disabling these empty reports and tools advertisements, you should see an increased confidence in data quality.  Or at the very least, keep the conversation from taking a detour.

## 2)  Organize SiteCatalyst reports by logical business function

Your internal users aren't thinking about Omniture variable structures when they are trying to find the answer to their business questions.  So why do we keep our data artificially separated by &quot;Custom Events&quot;, &quot;Custom Conversions&quot; and &quot;Custom Traffic&quot;?

Worse yet, who remembers that the number of Facebook Likes can be found at &quot;_Site Metrics -&gt; Custom Events -&gt; Custom Events 21-30_?&quot;  And why are Facebook Likes next to &quot;Logins&quot;?  Does that mean Facebook Logins?  Probably not.

Wouldn't it be better for our users to organize reports by business function, such as:

  * **Financial/Purchase Metrics** (Revenue, Discounts, Shipping, AOV, Units, Revenue Per Visit)
  * **Usability** (Browser, Percent of Page Viewed, Operating System)
  * **SEO** (Non-campaign visits, Referring Domains)
  * **Mobile** (Device, browser, resolution)
  * **Site Engagement** (Page Views, Internal Campaigns, Logins)
  * **Site Merchandising** (Products Viewed, Cart Add Ratio, Cross-Sell)
  * **Social** (Facebook Likes, Pinterest Pins, Visits from Social domains)
  * **Paid Campaigns** (Email, Paid Search, Display)
  * **Traffic** (Total Visits, Geosegmentation)

The list above isn't meant to be exhaustive, or necessarily how you should organize your SiteCatalyst menus.  But for me, organizing the reports by the business function keeps my business thinking flowing, rather than trying to remember how Omniture was implemented by variable type.

## 3)  Place custom reports and calculated metrics next to the standard SiteCatalyst reports

This is probably more like &quot;2b&quot; to the above, but there's no reason to keep custom reports and calculated metric reports segregated either.  Custom reports happen because of a specific business need, and the same thing with calculated metrics.  By placing these reports along with the out-of-the-box reports from SiteCatalyst, you take away the artificial distinction between data natively in SiteCatalyst and business-specific data populated by a web developer.

## Why you wouldn't want to customize?

Shawn makes two great points in &lt;a title=&quot;Dont customize SiteCatalyst&quot; href=&quot;http://shawncreed.com/blog/sitecatalyst-menu-customization.htm&quot; target=&quot;_blank&quot;&gt;his post&lt;/a&gt; about (not) customizing the SiteCatalyst menu: users require special training and menu customization isn't scalable.

### _Users need special training_

Users need to be trained anyway.  I don't think either of us is suggesting moving all of the menus around after an implementation has been in place for years...but if you're a company just starting out, why not start off customized?

Fellow Keystoner Tim Patten also commented to me via Twitter DM about power users being used to &quot;default&quot;, and it's annoying have to learn a new menu when switching companies; I'm not really worried about power users, I'm thinking about the hundreds of users in thousands of organizations who can't get beyond page views and visits.  Power users can pick up a new menu quickly, switch back to default, or use the search box.

### _Menu Customization isn't scalable_

This is very much true.  The larger the company, and the more complex and varied the tracking, inevitably menu customization isn't particularly scalable.  This is probably an area where specific dashboards are a much better strategy than customizing the menus.

## Summary

For me, one of the first things I look for when working with a company looking to get their digital analytics program off the ground is whether they've customized their Omniture menu structure.  As a free customization, it's something that companies should at least _consider_.  Organizing reports by business function requires a business to think about the questions they want to regularly answer, will keep novice users from focusing on implementation concepts, and overall is just better because it's how I think 🙂

_This blog post is a continuation of a &lt;a title=&quot;Original Tweet about SiteCatalyst Menu Customization&quot; href=&quot;https://twitter.com/randyzwitch/status/210042295859417090&quot; target=&quot;_blank&quot;&gt;Twitter conversation&lt;/a&gt; with Shawn C. Reed (&lt;a title=&quot;Shawn C. Reed Twitter account&quot; href=&quot;https://twitter.com/#!/shawncreed&quot; target=&quot;_blank&quot;&gt;@shawncreed&lt;/a&gt;), Jason Egan (&lt;a title=&quot;Jason Egan Twitter&quot; href=&quot;https://twitter.com/#!/jasonegan&quot; target=&quot;_blank&quot;&gt;@jasonegan&lt;/a&gt;), Tim Patten (&lt;a title=&quot;Tim Patten Twitter&quot; href=&quot;https://twitter.com/#!/timpatten&quot; target=&quot;_blank&quot;&gt;@timpatten&lt;/a&gt;) and others.  Shawn's counter-argument can be found &lt;a title=&quot;Why Shawn C. Reed prefers not to customize SiteCatalyst&quot; href=&quot;http://shawncreed.com/blog/sitecatalyst-menu-customization.htm&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;.  Jason wrote about &lt;a title=&quot;Jason Egan blog post&quot; href=&quot;http://www.jasonegan.net/2009/09/26/omniture-sitecatalyst-menu-customization-and-custom-reports/&quot; target=&quot;_blank&quot;&gt;Omniture menu customization&lt;/a&gt; a few years back.  And finally, if you want to read more pros-and-cons about SiteCatalyst menu customization, see the Adobe blog posts &lt;a title=&quot;Adobe post 1&quot; href=&quot;http://blogs.adobe.com/digitalmarketing/analytics/taking-sitecatalyst-menus-to-the-masses-part-i/&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt; and &lt;a title=&quot;Adobe post 2&quot; href=&quot;http://blogs.adobe.com/digitalmarketing/analytics/taking-sitecatalyst-menus-to-the-masses-part-ii/&quot; target=&quot;_blank&quot;&gt;here&lt;/a&gt;._</content>
      </item>
      
    
      
      <item>
        <title>Effect Of Modified Bounce Rate In Google Analytics</title>
        
          <description>&lt;p&gt;A few months back, Justin Cutroni posted on his &lt;a title=&quot;Justin Cutroni web analytics blog&quot; href=&quot;http://cutroni.com/blog&quot; target=&quot;_blank&quot;&gt;blog&lt;/a&gt; some jQuery code that &lt;a title=&quot;Modifying Bounce Rate and Time on Site in Google Analytics&quot; href=&quot;http://cutroni.com/blog/2012/02/21/advanced-content-tracking-with-google-analytics-part-1/&quot; target=&quot;_blank&quot;&gt;modifies how Google Analytics tracks content&lt;/a&gt;.  Specifically, the code snippet changes how bounce rate and time on site are calculated, creates a custom variable to classify whether visitors are “Readers” vs. “Scanners” and adds some Google Analytics events to track how far down the page visitors are reading.&lt;/p&gt;

</description>
        
        <pubDate>Thu, 10 May 2012 04:05:02 -0400</pubDate>
        <link>
        http://randyzwitch.com/bounce-rate-modification-google-analytics-cutroni/</link>
        <guid isPermaLink="true">http://randyzwitch.com/bounce-rate-modification-google-analytics-cutroni/</guid>
        <content type="html" xml:base="/bounce-rate-modification-google-analytics-cutroni/">A few months back, Justin Cutroni posted on his &lt;a title=&quot;Justin Cutroni web analytics blog&quot; href=&quot;http://cutroni.com/blog&quot; target=&quot;_blank&quot;&gt;blog&lt;/a&gt; some jQuery code that &lt;a title=&quot;Modifying Bounce Rate and Time on Site in Google Analytics&quot; href=&quot;http://cutroni.com/blog/2012/02/21/advanced-content-tracking-with-google-analytics-part-1/&quot; target=&quot;_blank&quot;&gt;modifies how Google Analytics tracks content&lt;/a&gt;.  Specifically, the code snippet changes how bounce rate and time on site are calculated, creates a custom variable to classify whether visitors are &quot;Readers&quot; vs. &quot;Scanners&quot; and adds some Google Analytics events to track how far down the page visitors are reading.

Given that this blog is fairly technical and specific in nature, I was interested in seeing how the standard Google Analytics metrics would change if I implemented this code and how my changes &lt;a title=&quot;Justin Cutroni bounce rate code results&quot; href=&quot;http://cutroni.com/blog/2012/02/23/advanced-content-tracking-with-google-analytics-part-2/&quot; target=&quot;_blank&quot;&gt;compared to Justin's&lt;/a&gt;.  I've always suspected my bounce rate in the 80-90% range didn't really represent whether people were finding value in my content.  The results were quite surprising to say the least!

## Bounce Rate - Dropped through the floor!

![bounce-rate-graph-google-analytics](/wp-content/uploads/2012/05/bounce-rate-graph-google-analytics-1024x212.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
Starting April 24th, Bounce Rate drops considerably!
&lt;/p&gt;

As expected, implementing the content tracking code caused a significant drop in bounce rate, due to counting scrolling as a page &quot;interaction&quot; using Google Analytics events. Thus, the definition of bounce rate changed from _single page view visits_ to _visitors that don't interact with the page by scrolling at least 150 pixels_.

In the case of my blog, the bounce rate dropped from **80-90%** to **5-15%**!  This result tells me that people who arrive on-site aren't arriving by accident, that they are specifically interested in the content.  Sure, I could've validated this using incoming search term research, but this provides a second data point.  The content I provide not only ranks well in Google, but once on-site also causes readers to want to see what the article contains.

## Readers vs. Scanners

Even with the bounce rate drop above, I really don't get a good feeling about whether people are actually reading the content.  Sure, people are scrolling 150px or more, but due to the ADHD nature of the web, plenty of people scroll without reading just to see what else is on the page!  That's where the &quot;Readers vs. Scanners&quot; report comes in:

![google-analytics-reader-vs-scanner](/wp-content/uploads/2012/05/google-analytics-reader-vs-scanner.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
62% of visits only scan instead of read - Need to do better here!
&lt;/p&gt;

The report above shows that only 38% of visits to the site actually READ an article, rather than just quickly scroll.  This is disappointing, but now that I've got the information being tracked, I can set up a goal in Google Analytics with the aim of improving the ratio of actual readers vs. quick scrollers.

## Average Visit Duration - Still useless

Like the bounce rate definition change above, average visit duration and average time on page also change definitions when using the jQuery content tracking code.  Given that Google Analytics calculates time metrics by measuring the time between page views or events, by adding more events on the page, all time on site metrics have to increase (by definition).

![avg-visit-duration-google-analytics](/wp-content/uploads/2012/05/avg-visit-duration-google-analytics-1024x230.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
Hard to see because of the Y-axis, but Avg. Visit Duration increases significantly as well.
&lt;/p&gt;

That said, average visit duration is still a pretty useless metric, given that an increase/decrease in this metric &lt;a title=&quot;Avinash:  You are what you measure&quot; href=&quot;http://www.kaushik.net/avinash/measure-choose-smarter-kpis-incentives/&quot; target=&quot;_blank&quot;&gt;doesn't immediately tell you&lt;/a&gt; &quot;good&quot; or &quot;bad&quot;...

## Content Consumption &quot;Funnel&quot;

Finally, the last change that occurs when you implement the content tracking code is a series of Google Analytics events that measure how far down the page visitors are actually seeing.  This report, in combination with the Readers vs. Scanners report, helps understand reader engagement better than any generic &quot;Time on Site&quot; metric can do.

![content-consumption-google-analytics](/wp-content/uploads/2012/05/content-consumption-google-analytics-1024x145.png)

From this report, I can see that of the 2,102 articles loaded:

  * **89.4%** of the articles have a &quot;StartReading&quot; event fired
  * **89.8%** of those who start to read an article reach the bottom of the article.
  * **19.7%** of those who reach the end of the article scroll past the comments to reach the true end of page

The first metric above is analogous to subtracting the bounce rate from 1, the percentage of articles viewed that don't bounce.  The second metric (complete articles seen), with a success rate of 89.8% is ripe for segmentation.  I stated above that only 38% actually READ an article, so segmenting the above report by &quot;Readers&quot; vs. &quot;Scanners&quot; will surely lower the success rate in the &quot;Readers&quot; population.

Finally, that &lt;20% actually touch the true bottom of page is surprising to me, since this blog really doesn't get many comments!  If there were thousands of comments and the pages were really long, ok, no one sees the bottom...but here?  I'll have to think about this a bit.

## Great update to Google Analytics default settings!

Overall, my impression of the &lt;a title=&quot;jQuery Google Analytics content tracking snippet&quot; href=&quot;http://cutroni.com/blog/2012/02/21/advanced-content-tracking-with-google-analytics-part-1/&quot; target=&quot;_blank&quot;&gt;jQuery code snippet&lt;/a&gt; developed by Justin and others is that it is _extremely useful_ in understand interaction of visitors to content sites.  The only downside I see here is that it changes the definition of bounce rate within Google Analytics, which could be confusing to others who 1) aren't aware of the code snippet running on-site or 2) don't quite understand the subtleties of Google Analytics implementation with respect to Events and the &lt;a title=&quot;Google Analytics Non-Interaction Events&quot; href=&quot;https://developers.google.com/analytics/devguides/collection/gajs/eventTrackerGuide#non-interaction&quot; target=&quot;_blank&quot;&gt;non-interaction setting&lt;/a&gt;.

But since this is my personal blog, I don't need to worry about others mis-interpreting my Google Analytics data, so I'm going to keep this functionality installed!

_Update 7/25/12:  Google Analytics published a similar method to the one described above, using &quot;setTimeout&quot; to &lt;a title=&quot;Google Analytics Modified Bounce Rate article&quot; href=&quot;http://analytics.blogspot.com/2012/07/tracking-adjusted-bounce-rate-in-google.html&quot; target=&quot;_blank&quot;&gt;modify bounce rate&lt;/a&gt; based solely on time-on-page_.</content>
      </item>
      
    
      
      <item>
        <title>Adobe Discover 3:  First Impressions</title>
        
          <description>&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2012/04/adobe-discover-logo.png&quot; alt=&quot;Adobe Discover&quot; /&gt;&lt;/p&gt;

</description>
        
        <pubDate>Fri, 27 Apr 2012 04:24:21 -0400</pubDate>
        <link>
        http://randyzwitch.com/adobe-discover-3-first-impressions/</link>
        <guid isPermaLink="true">http://randyzwitch.com/adobe-discover-3-first-impressions/</guid>
        <content type="html" xml:base="/adobe-discover-3-first-impressions/">![Adobe Discover](/wp-content/uploads/2012/04/adobe-discover-logo.png)

With yesterday's code release, &lt;del&gt;Omniture&lt;/del&gt; Adobe released version 3 of their &quot;Discover&quot; tool, THE way to perform web analysis within the Adobe Digital Marketing Suite.  While SiteCatalyst has its place for basic reporting, to really dig deep into your data for actionable insights there's no substitute to using Discover.

But as with every product overhaul, there is the potential to change things that users liked and while not make enough improvement to excite the user base...but luckily, that's not the case with Discover 3.  Here's how I see the new features and design changes.

## New &quot;Darth Vader&quot; interface

![adobe-discover-3-screenshot](/wp-content/uploads/2012/04/adobe-discover-3-screenshot.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
&quot;Ooh, tough looking. Just like hardcore web analysts!&quot;
&lt;/p&gt;

Of all the cool things about Discover 3, I'm not sure the new color palette is one of them.  &lt;a title=&quot;Adobe Discover 3 announcement&quot; href=&quot;http://blogs.adobe.com/digitalmarketing/analytics/discover-3-0-the-new-ui-might-just-be-as-cool-as-the-analysts-who-use-it/&quot; target=&quot;_blank&quot;&gt;Several reasons were given by Adobe&lt;/a&gt; for choosing the carbon colored interface, from trying to match analyst's personalities (yuck!), reducing eye strain (ok), and consistent branding (eh).  Of the three, I'll say that reducing eye strain is a worthy goal, although Discover 3 never struck me as &quot;eye-burning&quot; in the past.

Maybe I'll grow to like it, but right now, it seems really dark.  The light gray text on dark gray background needs a bit more contrast, and in general, the interface feels kinda depressing.

## Calendars - No more #^%&amp;$ sliders!

Now we're getting somewhere.  The slider interface in Discover 2 never made sense to me.  You pick your time period up front, open a report, and then to modify the time period within an individual report you needed to move a bunch of jerky sliders around.

In Discover 3, we now have the same style calendar interface as SiteCatalyst.  Makes sense from a consistency standpoint within the Adobe Digital Marketing Suite and a general UX standpoint.  Pointing at two dates on the calendar is way easier and faster than moving endpoints of a slider!

## Heterogeneous Pathing

This is so completely badass and the best new feature of Discover 3.  No longer are you confined to a fallout report that only includes just one Omniture variable type.  So if I want to do a funnel that measures visits containing a few different pages, then triggering a Facebook 'Like' event, a Cart Open, then an Exit Link, I can now do so!

You can also switch from &quot;Visit-level&quot; to &quot;Visitor-level&quot; on the fly, which can also be useful depending on how your view your business.  Some people like to think about every visit being an opportunity to convert on-site, whereas Avinash advocates in his &lt;a title=&quot;Web Analytics 2.0 link&quot; href=&quot;http://www.amazon.com/gp/product/0470529393/ref=as_li_ss_tl?ie=UTF8&amp;tag=thefuquexpe-20&amp;linkCode=as2&amp;camp=1789&amp;creative=390957&amp;creativeASIN=0470529393&quot; target=&quot;_blank&quot;&gt;Web Analytics 2.0 book&lt;/a&gt; that using &lt;a title=&quot;Avinash Visitors Conversion Rate&quot; href=&quot;http://www.kaushik.net/avinash/excellent-analytics-tip5-conversion-rate-basics-best-practices/&quot; target=&quot;_blank&quot;&gt;Visitors as the denominator for conversion rate&lt;/a&gt; is the proper thought model.  I won't weigh in on the difference in this post, but it's cool that we can now change back-and-forth to see what the differences in the data are.

## Table Builder

![adobe-discover-3-table-builder](/wp-content/uploads/2012/04/adobe-discover-3-table-builder.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
Nice drag-and-drop options, very PivotTable like
&lt;/p&gt;

Finally, the last really obvious difference between Discover 2 and Discover 3 is the table builder while using ranked reports.  Like the eye-strain issue talked about above, the amount of time that it took for reports to build never really seemed like an issue to me.  Perhaps that's the &lt;del&gt;SAS&lt;/del&gt; programmer side of me that often waits hours to return a result of a complex set of commands.

But now that I've used the table builder, it's definitely an improvement on how data tables get built.  You get to specify each element you want in the table first, THEN the data gets retrieved.  It may sound like a small change, but when you already know what you want, not having to wait for the table to build while you keep dragging in metrics does _feel like_ it's way faster to get the table you are looking for.

## Adobe Discover 3 - Definitely an improvement

There are probably 20 other things I haven't noticed yet in the new Discover 3 interface, but from what I have used so far, this is a great upgrade in functionality!  It feels faster to get things completed with the table builder and the new pathing functionality across all variable types is a long time coming.  Now, if only there was a different color palette I could choose, it'd be perfect...maybe something like this?

![omniture-discover-1.5](/wp-content/uploads/2012/04/omniture-discover-1.5.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
You should prefer green, not carbon.
&lt;/p&gt;</content>
      </item>
      
    
      
      <item>
        <title>Removing &quot;Powered by WordPress&quot; From Scrappy</title>
        
          <description>&lt;p&gt;My wife started a &lt;a title=&quot;Zwitchen.com | What's Cookin'?&quot; href=&quot;http://zwitchen.com&quot; target=&quot;_blank&quot;&gt;food blog&lt;/a&gt; this weekend and decided to use the &lt;a title=&quot;Scrappy WordPress theme&quot; href=&quot;http://wordpress.org/extend/themes/scrappy&quot; target=&quot;_blank&quot;&gt;“Scrappy” WordPress theme&lt;/a&gt; by &lt;a href=&quot;http://carolinethemes.com/2011/11/04/scrappy/&quot; target=&quot;_blank&quot;&gt;Caroline Moore&lt;/a&gt;.  But like all WordPress themes, there are a few customizations that are usually made to personalize the theme, the first of which is removing “Powered by WordPress” in the footer.&lt;/p&gt;

</description>
        
        <pubDate>Mon, 23 Apr 2012 17:23:37 -0400</pubDate>
        <link>
        http://randyzwitch.com/removing-powered-by-wordpress-scrappy/</link>
        <guid isPermaLink="true">http://randyzwitch.com/removing-powered-by-wordpress-scrappy/</guid>
        <content type="html" xml:base="/removing-powered-by-wordpress-scrappy/">My wife started a &lt;a title=&quot;Zwitchen.com | What's Cookin'?&quot; href=&quot;http://zwitchen.com&quot; target=&quot;_blank&quot;&gt;food blog&lt;/a&gt; this weekend and decided to use the &lt;a title=&quot;Scrappy WordPress theme&quot; href=&quot;http://wordpress.org/extend/themes/scrappy&quot; target=&quot;_blank&quot;&gt;&quot;Scrappy&quot; WordPress theme&lt;/a&gt; by &lt;a href=&quot;http://carolinethemes.com/2011/11/04/scrappy/&quot; target=&quot;_blank&quot;&gt;Caroline Moore&lt;/a&gt;.  But like all WordPress themes, there are a few customizations that are usually made to personalize the theme, the first of which is removing &quot;Powered by WordPress&quot; in the footer.

This blog post is essentially the same as my prior blog post about &lt;a title=&quot;Removing Powered by WordPress from Twenty Eleven theme&quot; href=&quot;http://randyzwitch.com/removing-powered-by-wordpress-twenty-eleven/&quot; target=&quot;_blank&quot;&gt;removing &quot;Powered by WordPress&quot; from the Twenty Eleven theme&lt;/a&gt;, but of course, for the Scrappy theme!  This post assumes you've created a &lt;a title=&quot;Creating a WordPress child theme&quot; href=&quot;http://randyzwitch.com/twenty-eleven-child-theme-creating-css-file/&quot; target=&quot;_blank&quot;&gt;child theme&lt;/a&gt; already, so that the changes you make persist even if the theme gets updated by the original author.

## Copy the `footer.php` file from the Scrappy theme

The first thing to do to customize the Scrappy footer is make a copy of the `footer.php` file from the Scrappy theme and place it in your child theme folder.  It is this file that we will modify in order to remove &quot;Powered by WordPress&quot; and the reference to the Scrappy theme name.

## Comment out code within the &quot;site-info&quot; section of the footer

Within the footer code, there is a `&lt;div&gt;` section that references `site-info`:

{% highlight php linenos %}
&lt;div&gt;
         &lt;?php do_action( 'scrappy_credits' ); ?&gt;
            &lt;a href=&quot;&lt;?php echo esc_url( __( 'http://wordpress.org/', 'scrappy' ) ); ?&gt;&quot; title=&quot;&lt;?php esc_attr_e( 'A Semantic Personal Publishing Platform', 'scrappy' ); ?&gt;&quot; rel=&quot;generator&quot;&gt;&lt;?php printf( __( 'Proudly powered by %s', 'scrappy' ), 'WordPress' ); ?&gt;&lt;/a&gt;
            &lt;span&gt; | &lt;/span&gt;
            &lt;?php printf( __( 'Theme: %1$s by %2$s', 'scrappy' ), 'Scrappy', '&lt;a href=&quot;http://carolinemoore.net/&quot; rel=&quot;designer&quot;&gt;Caroline Moore&lt;/a&gt;' ); ?&gt;
        &lt;/div&gt;&lt;!-- .site-info --&gt;
{% endhighlight %}

What we want to do is comment out the code, starting at the first php reference and ending after the &quot;Caroline Moore&quot; line.  We can do this using the `&lt;!-` and `-&gt;` &lt;a title=&quot;HTML Tag Comments article&quot; href=&quot;http://www.w3schools.com/tags/tag_comment.asp&quot; target=&quot;_blank&quot;&gt;codes&lt;/a&gt;.  When the code is commented out correctly, it will look like the following:

{% highlight php linenos %}
&lt;!--        &lt;?php do_action( 'scrappy_credits' ); ?&gt;
            &lt;a href=&quot;&lt;?php echo esc_url( __( 'http://wordpress.org/', 'scrappy' ) ); ?&gt;&quot; title=&quot;&lt;?php esc_attr_e( 'A Semantic Personal Publishing Platform', 'scrappy' ); ?&gt;&quot; rel=&quot;generator&quot;&gt;&lt;?php printf( __( 'Proudly powered by %s', 'scrappy' ), 'WordPress' ); ?&gt;&lt;/a&gt;
            &lt;span&gt; | &lt;/span&gt;
            &lt;?php printf( __( 'Theme: %1$s by %2$s', 'scrappy' ), 'Scrappy', '&lt;a href=&quot;http://carolinemoore.net/&quot; rel=&quot;designer&quot;&gt;Caroline Moore&lt;/a&gt;' ); ?&gt; --&gt;
{% endhighlight %}

Be sure to hit &quot;Save&quot; to your `footer.php` file after making the comment changes, and you're all done:  no more &quot;Powered by WordPress&quot;!

![wordpress-scrappy-footer-original](/wp-content/uploads/2012/04/wordpress-scrappy-footer-original.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
Original &quot;Scrappy&quot; footer - Powered by WordPress
&lt;/p&gt;

![WordPress Scrappy footer modified](/wp-content/uploads/2012/04/wordpress-scrappy-footer-modified.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
Modified WordPress Scrappy footer (with slightly different pattern!)
&lt;/p&gt;

## Adding your own footer text

If you want to add your own footer text, like a copyright statement, you can add any HTML code you want within the same section of code where we just commented out code.  For an example of how to &lt;a href=&quot;http://randyzwitch.com/removing-powered-by-wordpress-twenty-eleven/&quot; target=&quot;_blank&quot;&gt;add a copyright statement&lt;/a&gt;, see the bottom of my prior blog post about modifying the footer of the WordPress Twenty Eleven theme.

That's it!  One step closer to making the WordPress Scrappy theme your own.</content>
      </item>
      
    
      
      <item>
        <title>How To Install Optimizely on WordPress</title>
        
          <description>&lt;p&gt;If you’ve spent any time working in digital marketing or analytics, you’re already familiar with the power of A/B testing.  A/B testing (and it’s more complicated brother &lt;a href=&quot;http://www.smashingmagazine.com/2011/04/04/multivariate-testing-101-a-scientific-method-of-optimizing-design/&quot; target=&quot;_blank&quot;&gt;multivariate testing&lt;/a&gt;) allows site owners to find out optimal combinations of site design and content for their visitors &lt;em&gt;without having to directly ask/inconvenience the user&lt;/em&gt;.  All it takes to improve a website is forming a hypothesis of something that could work better, creating multiple versions of a page (or other content), setting up the experiment…and the money flows in faster than you can count it.  At least, that’s the hope!&lt;/p&gt;

</description>
        
        <pubDate>Thu, 19 Apr 2012 03:45:22 -0400</pubDate>
        <link>
        http://randyzwitch.com/optimizely-wordpress-install/</link>
        <guid isPermaLink="true">http://randyzwitch.com/optimizely-wordpress-install/</guid>
        <content type="html" xml:base="/optimizely-wordpress-install/">If you've spent any time working in digital marketing or analytics, you're already familiar with the power of A/B testing.  A/B testing (and it's more complicated brother &lt;a href=&quot;http://www.smashingmagazine.com/2011/04/04/multivariate-testing-101-a-scientific-method-of-optimizing-design/&quot; target=&quot;_blank&quot;&gt;multivariate testing&lt;/a&gt;) allows site owners to find out optimal combinations of site design and content for their visitors _without having to directly ask/inconvenience the user_.  All it takes to improve a website is forming a hypothesis of something that could work better, creating multiple versions of a page (or other content), setting up the experiment...and the money flows in faster than you can count it.  At least, that's the hope!

At the enterprise level, there are plenty of testing tools such as &lt;a href=&quot;http://www.omniture.com/en/products/conversion/test-and-target&quot; target=&quot;_blank&quot;&gt;Omniture Test &amp; Target&lt;/a&gt;, &lt;a href=&quot;http://www.sitespect.com/&quot; target=&quot;_blank&quot;&gt;SiteSpect&lt;/a&gt;, &lt;a href=&quot;http://webtrends.com/products/optimize/&quot; target=&quot;_blank&quot;&gt;WebTrends Optimize&lt;/a&gt;, and &lt;a href=&quot;http://www.monetate.com&quot; target=&quot;_blank&quot;&gt;Monetate&lt;/a&gt;, but these tools are cost-prohibitive to all but the largest websites.  Google provides &lt;a href=&quot;www.google.com/websiteoptimizer&quot; target=&quot;_blank&quot;&gt;Google Website Optimizer&lt;/a&gt; (for free!), but that has often been viewed as difficult to manage, especially for dynamically created websites.  That's where &lt;a href=&quot;http://www.optimizely.com&quot; target=&quot;_blank&quot;&gt;Optimizely&lt;/a&gt; comes in.

Optimizely's tagline is &quot;A/B testing software you'll actually use&quot;, a reference to complication (and I think indirectly, the expense) of other testing tools in the marketplace.  Optimizely claims is that you can start testing after adding a single line of JavaScript code...and here's how you do it.

## Getting Started - Sign up

The first step to installing Optimizely is to sign up. A 30-day trial is provided, but you do need to put in a credit card to activate the free trial.

After doing so, when you click on the &quot;Implementation&quot; button, a pop-up shows you your customized &quot;single line of code&quot; to implement.

![optimizely-implementation](/wp-content/uploads/2012/04/optimizely-implementation.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
Clicking on the &quot;Implementation&quot; link gives you your JavaScript code
&lt;/p&gt;

## Installing Optimizely code snippet in WordPress Header

When you click on the 'Implementation' link, you'll get code that looks like the following:

&lt;pre&gt;&amp;lt;script src=&quot;//cdn.optimizely.com/js/38######.js&quot;&amp;gt;&amp;lt;/script&amp;gt;&lt;/pre&gt;

What we want to do is install this code in our &lt;a title=&quot;Twenty Eleven Child Theme: Custom Header&quot; href=&quot;http://randyzwitch.com/custom-header-twenty-eleven-child-theme/&quot; target=&quot;_blank&quot;&gt;WordPress header&lt;/a&gt;, pretty much after the first &lt;head&gt; tag.  By placing the code as high as possible in the WordPress header, this gives Optimizely the ability to affect your website as soon as the page starts loading, and assures that your site visitors don't notice anything happening before their eyes!

![optimizely-jquery-menu](/wp-content/uploads/2012/04/optimizely-jquery-menu-300x200.png)

The only consideration you need to make when installing the code snippet is whether your site is already running jQuery or not.  You can determine this by looking in the header of your WordPress site to see if there are any scripts that say something like `jquery-xxxx`.js.  If you don't see any jQuery references in the header, you don't need to do anything: Optimizely already includes jQuery!  If you already have jQuery installed, be sure to place the Optimizely code snippet after the WordPress jQuery reference.  Then, go to the Optimizely website, click on the  `My Experiments&quot; -&gt; &quot;View All Experiments&quot; - &gt; &quot;Project Code&quot;` menu to change the jQuery settings to &quot;Do not include&quot;.

After you place the code snippet line in the header and hit save, that's it!  You'll be able to start using Optimizely to create experiments.

## Integrating Optimizely with Google Analytics

In order to track your Optimizely experiments within Google Analytics, no code changes need to be made.  The only thing you need to do is make sure you have the asynchronous version of the Google Analytics code installed and a free custom variable slot.  Then, within the Optimizely interface, you can choose which custom variable to write your data to and you're done.</content>
      </item>
      
    
      
      <item>
        <title>Using Omniture SiteCatalyst Target Report To Calculate YOY growth</title>
        
          <description>&lt;p&gt;Of the hundreds of stock reports and capabilities present within Adobe (Omniture) SiteCatalyst, calculating year-over-year growth isn’t the easiest thing to do.  And while conversion reports (eVars) have the “Compare Dates” functionality within the calendar menu, we can’t quickly plot the difference between two time periods within a dashboard.  This is where the Omniture SiteCatalyst Target report comes in handy.&lt;/p&gt;

</description>
        
        <pubDate>Wed, 22 Feb 2012 03:15:18 -0500</pubDate>
        <link>
        http://randyzwitch.com/omniture-sitecatalyst-target-report/</link>
        <guid isPermaLink="true">http://randyzwitch.com/omniture-sitecatalyst-target-report/</guid>
        <content type="html" xml:base="/omniture-sitecatalyst-target-report/">Of the hundreds of stock reports and capabilities present within Adobe (Omniture) SiteCatalyst, calculating year-over-year growth isn't the easiest thing to do.  And while conversion reports (eVars) have the &quot;Compare Dates&quot; functionality within the calendar menu, we can't quickly plot the difference between two time periods within a dashboard.  This is where the Omniture SiteCatalyst Target report comes in handy.

## Setting up your &quot;Goal&quot;

Within the Omniture Knowledge Base &lt;a href=&quot;https://omniture-help.custhelp.com/app/answers/detail/a_id/2153/kw/targets&quot; target=&quot;_blank&quot;&gt;KB2153&lt;/a&gt;, I think Omniture does a disservice by stating:

&gt; _Targets_ are quantifiable goals that you can place within the SiteCatalyst interface and compare against reports.

While this is a true statement, I think one of the reasons that the Omniture SiteCatalyst Target report isn't more widely used is that it doesn't _have to be_ a future &quot;goal&quot; per se, any set numbers can be used.  When last year's numbers are used, the report becomes a year-over-year comparison!

For this example, I'm going to be comparing page views year-over-year.  Here's what the page views summary by month looks like for 2011:

![omniture-page-views-report](/wp-content/uploads/2012/02/omniture-page-views-report.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
Omniture Page Views Report - 2011
&lt;/p&gt;

Using this report to set a year-over-year target, we can see the early months are in the few thousands of page views, increasing to 12,000 -14,000 later in the year.

## Omniture SiteCatalyst Target interface - Inputting our numbers

![Screen Shot 2012-02-22 at 7.49.46 AM](/wp-content/uploads/2012/02/Screen-Shot-2012-02-22-at-7.49.46-AM.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
Monthly Target Setup within Omniture Interface
&lt;/p&gt;

Assuming you are using Omniture SiteCatalyst v15, you set up a Target report under `Favorites -&gt; Targets -&gt; Manage Targets`, then choose `Add New` once you're in the Targets menu.  I'll be setting up a monthly target for Page Views, so I'll just type it in instead of using the file upload capability.  For this example, we want to apply this target to &quot;Entire Site&quot; for the &quot;Page Views&quot; Metric.  The date range will be all of 2012, with &quot;Monthly&quot; granularity.  This will give you 12 boxes to type in the 2011 Page View results, and once we hit &quot;Ok&quot; to save, we'll have our year-over-year report set up.

## Getting the Year-over-Year graph within Omniture

Showing the results of our newly created Target report is as easy as going to `Favorites -&gt; Targets`, then choosing the appropriate Target.  By default, the report will show like a normal metric report, with a green overlay for your targets:

![Omniture-target-report-default](/wp-content/uploads/2012/02/Omniture-target-report-default.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
Default view of Omniture SiteCatalyst Target report
&lt;/p&gt;

The above report shows that Page Views for January 2012 are well above our January 2011 &quot;Target&quot; and that February has already exceeded the goal as well...which is great since we've got 7 more days left in the month!

If we want to show the year-over-year delta, however, we can choose the &quot;Variance&quot; report option at the top of the graph.  Doing so will show the following report:

![omniture-target-variance](/wp-content/uploads/2012/02/omniture-target-variance.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
Omniture SiteCatalyst Target Report - &quot;Variance&quot; option
&lt;/p&gt;

By placing this report in a dashboard, we can quickly evaluate whether Page Views have grown by month year-over-year.  It's disappointing that the only graph option Adobe provides is the raw metric higher/lower than the target instead of a percentage difference view, but the percentage difference is calculated as part of the data table view that goes along with this report.

## Conclusion

When talking about &quot;growth&quot; many businesses aren't content with just year-over-year growth, usually aiming for 10%, 20%...10,000% growth.  These are goals that work well to track within the Omniture SiteCatalyst Target report.  But year-over-year growth can be worth monitoring too, and the Omniture SiteCatalyst Target report is a great way to do so.</content>
      </item>
      
    
      
      <item>
        <title>Claim Your Newest Klout Perk:  An SEC Investigation!</title>
        
          <description>&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2011/12/klout-perk-wahooly.png&quot; alt=&quot;klout-perk-wahooly&quot; /&gt;&lt;/p&gt;

</description>
        
        <pubDate>Fri, 16 Dec 2011 05:41:26 -0500</pubDate>
        <link>
        http://randyzwitch.com/klout-perk-wahooly/</link>
        <guid isPermaLink="true">http://randyzwitch.com/klout-perk-wahooly/</guid>
        <content type="html" xml:base="/klout-perk-wahooly/">![klout-perk-wahooly](/wp-content/uploads/2011/12/klout-perk-wahooly.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
&quot;Ownership in startup companies for nothing...sign up today!&quot;
&lt;/p&gt;

I'm not sure what it is about Klout that keeps me coming back.  I tell myself that as a digital marketer/analyst that I need to monitor the newest trends in the market, blah blah blah.  But every time I interact with Klout, they find a new way to abuse me, leaving me feeling dirty for having looked to see what value they assign to me based on the data that I _willingly_ provide.

## Klout Perks - Send a lot of Tweets, get free sh-!

In a previous post on &lt;a title=&quot;Klout will never be FICO&quot; href=&quot;http://randyzwitch.com/social-influence-another-fico/&quot; target=&quot;_blank&quot;&gt;Klout&lt;/a&gt;, I wrote that &quot;Klout Perks&quot; don't seem like the most effective way for a business to market.  Especially given that fact that these advertisement 'perks' are often locked when a user clicks on them (and in some cases, weren't ever available...).  Showing a customer an offer then saying &quot;just kidding&quot; is nothing more than a low stakes bait-and-switch, and a way to sully brand reputation.

That said, trying to get &quot;influential&quot; people using your goods is a tried-and-true advertising technique, and one that companies pay handsomely for.  You see it in car commercials, luxury goods like watches and clothing, and even home goods retail. So again, I can't fault large companies that are testing into new markets, as top-line growth can often be hard to find (and expensive!)

But once you start moving into investment offers, venture capital, and startups, now you're moving into a whole other world of due diligence...

## I qualify for &quot;Wahooly&quot;...what's that?

After signing into Klout this most recent time, I see that I now qualify for a perk from Wahooly, which from the banner ad seems to be some sort of influence-peddling for equity shares in startups.  WTF?  Equity to people who've never met the founders, just that their Klout score is above 35!

Rather than flex my MBA muscles on the value of equity in unknown companies, I decided to just Google 'Wahooly' and see what comes up.  One of the results that popped up was from &lt;a title=&quot;Ad Age Wahooly&quot; href=&quot;http://adage.com/article/digital/wahooly-offer-startup-equity-stakes-influential-social-media-users/230916/&quot; target=&quot;_blank&quot;&gt;Ad Age&lt;/a&gt;, an industry publication I generally trust.  Imagine my surprise when I see the lede:

&gt; Set to Launch in January, Wahooly Walks a Grey Area In Securities Law

Hmmm, nice partnership you've got going here Klout...interesting that you think I might be interested, or at the very least be able to help.

Wanting to know more, I dug farther into the article to find these two gems:

&gt; ...But according to Linda Goldstein, chair of the advertising, marketing and media division at the law firm Manatt, Phelps &amp; Phillips, the fact that the [equity] stakes have a potential future value constitutes a material connection that would make them subject to the regulations.

&gt; **&quot;If you're an equity stakeholder in a company, I can guarantee that the FTC is going to think that's a relationship that needs to be disclosed,&quot;** she said...

&gt;
&gt; ...Wahooly is looking to structure itself like a venture capital fund, in which it would be the sole shareholder and represent the interests of its users, and thereby avoid registering itself and the startups it represents with the Securities and Exchange Commission as public companies...

&gt; However, **the SEC has a [history of cracking down](http://www.sec.gov/news/headlines/webstock.htm &quot;SEC&quot;) on so-called &quot;free stock&quot; offerings that seek to be exempted from registration on that basis.**

After reading the Ad Age article, I looked to see what disclosures were presented on the Klout website, similar to something like this:

_&quot;Your newest Klout perk may require your pursuit of legal representation.  Please consult a financial adviser, lawyer, and anyone else that may be relevant before accepting this offer&quot;_

Why am I not surprised there aren't any?

## Key to investing success:  Stick with what you know

I'm sure that it goes without saying that I'll be passing on this newest &quot;Perk&quot;.  As much as I like money and enjoy arguing the merits of different legal interpretations, I need to talk with the Federal Trade Commission and the Securities and Exchange Commission like I need a kick in the groin.

And yet after all this abuse, I don't delete my Klout account.  Maybe I DO need a kick in the groin...</content>
      </item>
      
    
      
      <item>
        <title>100 Miles For charity:water - 12/1 Update</title>
        
          <description>&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2011/12/100milesfor-charitywater.png&quot; alt=&quot;charity:water campaign progress&quot; /&gt;&lt;/p&gt;

</description>
        
        <pubDate>Thu, 01 Dec 2011 03:26:24 -0500</pubDate>
        <link>
        http://randyzwitch.com/100-miles-for-charitywater-update-1/</link>
        <guid isPermaLink="true">http://randyzwitch.com/100-miles-for-charitywater-update-1/</guid>
        <content type="html" xml:base="/100-miles-for-charitywater-update-1/">![charity:water campaign progress](/wp-content/uploads/2011/12/100milesfor-charitywater.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
Donations come in spurts, but the running continues!
&lt;/p&gt;

About halfway into my charity:water campaign (&lt;a title=&quot;100 miles for charity:water&quot; href=&quot;http://mycharitywater.org/100milesforcharitywater&quot; target=&quot;_blank&quot;&gt;donate here&lt;/a&gt;), I'm pleased to say that I've raised $675!  Translated into miles, that's 67.5 miles, of which I've run 37.95 thus far.

The running is starting to get easier, but I'm starting to lose momentum on the donations side.  I've thoroughly worked the Facebook Friends network, and with a little &quot;Embarrass Randy&quot; motivation, my co-workers have put up quite a bit of money.  It's funny how offering to run on the company treadmill on my normal work-from-home day can get people to cough up the dough.  That, and the fact that they can drink beer and mock me during said running...

But with those two personal networks tapped out, I'm going to have to start twisting arms pretty soon.  Unless there are some [charitable readers](http://mycharitywater.org/100milesforcharitywater &quot;100 miles for charity:water&quot;) out there 🙂</content>
      </item>
      
    
      
      <item>
        <title>100 Miles For charity:water</title>
        
          <description>&lt;p&gt;&lt;a href=&quot;http://mycharitywater.org/100milesforcharitywater&quot;&gt;&lt;img class=&quot;alignright&quot; style=&quot;border: 1px solid black;&quot; src=&quot;http://i1.wp.com/www.charitywater.org/media/banners/220x220_8glasses.jpg?resize=220%2C220&quot; alt=&quot;&quot; border=&quot;1&quot; data-recalc-dims=&quot;1&quot; /&gt;&lt;/a&gt;&lt;/p&gt;

</description>
        
        <pubDate>Thu, 27 Oct 2011 10:09:09 -0400</pubDate>
        <link>
        http://randyzwitch.com/charitywater-campaign/</link>
        <guid isPermaLink="true">http://randyzwitch.com/charitywater-campaign/</guid>
        <content type="html" xml:base="/charitywater-campaign/">[&lt;img class=&quot;alignright&quot; style=&quot;border: 1px solid black;&quot; src=&quot;http://i1.wp.com/www.charitywater.org/media/banners/220x220_8glasses.jpg?resize=220%2C220&quot; alt=&quot;&quot; border=&quot;1&quot; data-recalc-dims=&quot;1&quot; /&gt;](http://mycharitywater.org/100milesforcharitywater)

In the past couple of weeks, I've started a project through the &lt;a title=&quot;What is the Analysis Exchange?&quot; href=&quot;http://www.webanalyticsdemystified.com/ae/what-is-analysis-exchange.asp&quot; target=&quot;_blank&quot;&gt;Analysis Exchange&lt;/a&gt; working with &lt;a title=&quot;Jason Thompson&quot; href=&quot;http://emptymind.org/&quot; target=&quot;_blank&quot;&gt;Jason Thompson&lt;/a&gt; and &lt;a title=&quot;charity:water&quot; href=&quot;http://www.charitywater.org/&quot; target=&quot;_blank&quot;&gt;charity:water&lt;/a&gt;.  If you're not familiar with the Analysis Exchange, it's a program where aspiring web analysts and grizzled veterans* volunteer to provide analytics services to charities.  Through the Analysis Exchange, charities are helped, students are mentored, and the World is a slightly better place than when the project was started.

But just because I'm volunteering my time doesn't mean I can't raise some money also.  As we go into the holiday season, I've started a charity:water campaign:

&lt;p style=&quot;text-align: center;&quot;&gt;
  &lt;strong&gt;&lt;a title=&quot;charity:water website&quot; href=&quot;http://mycharitywater.org/100milesforcharitywater&quot;&gt;100 miles for charity:water.&lt;/a&gt;&lt;/strong&gt;
&lt;/p&gt;

Find out about the details at [my fundraising website](http://mycharitywater.org/100milesforcharitywater &quot;charity:water website&quot;).  As the weeks go by, I'll be publishing a dashboard of the progress, with pictures of each of my runs for proof.  I'll also be including such embarrassing KPI's as avg. time per mile, current weight, and anything else I think of.

I'm already in for 5 miles.  There's only 95 to go.

*Jason is the aspiring web analyst, and I'm the grizzled veteran.  I think.

&lt;span style=&quot;text-decoration: underline;&quot;&gt;Update:&lt;/span&gt;  After Day 1, already 27% of the way to $1000!  I'll start paying off the miles this weekend, but there's still plenty more opportunity to contribute.</content>
      </item>
      
    
      
      <item>
        <title>Page Navigation For WordPress Twenty Eleven</title>
        
          <description>&lt;p&gt;By default, most WordPress themes (including Twenty Eleven) have relatively weak page-to-page navigation.  Users can improve content structure on their WordPress blogs by using categories, tags, and custom menus, but one navigation element that’s a bit more difficult to modify are the “Next Page/Previous Page” links.  These links are great if you want to move a page at a time, but if your blog has lots of pages, users won’t easily be able to navigate very deeply into your content.  That’s where a page navigation menu comes in handy.&lt;/p&gt;

</description>
        
        <pubDate>Fri, 21 Oct 2011 07:18:47 -0400</pubDate>
        <link>
        http://randyzwitch.com/page-navigation-wordpress-twenty-eleven/</link>
        <guid isPermaLink="true">http://randyzwitch.com/page-navigation-wordpress-twenty-eleven/</guid>
        <content type="html" xml:base="/page-navigation-wordpress-twenty-eleven/">By default, most WordPress themes (including Twenty Eleven) have relatively weak page-to-page navigation.  Users can improve content structure on their WordPress blogs by using categories, tags, and custom menus, but one navigation element that's a bit more difficult to modify are the &quot;Next Page/Previous Page&quot; links.  These links are great if you want to move a page at a time, but if your blog has lots of pages, users won't easily be able to navigate very deeply into your content.  That's where a page navigation menu comes in handy.

Here's how to implement page navigation, using a plugin and a simple modification to the WordPress Twenty Eleven theme.

## Using a plugin?  For shame!

Usually, I'm not a big fan of using plugins for every little modification on my blogs. While there are some amazing WordPress developers out there, there are many _less than perfect_ ones as well, and by loading up your blog with their buggy code can really slow (or brick!) your blog.  However, for something as complex as page navigation, I've opted to use the [WP-PageNavi](http://wordpress.org/extend/plugins/wp-pagenavi/ &quot;WordPress Page Navigation plugin&quot;) plugin.  With over a million downloads, I'm fairly confident that this plugin's popularity indicates its code quality.  Since I've installed it, I've seen no performance deterioration...

So the first step for this tutorial is to install this plugin!

## Modifying your Twenty Eleven child theme for Page Navigation

When you install this plugin and refresh your blog, the first thing you'll realize is that nothing happened!  This occurs because the plugin itself doesn't know where you want to put the page navigation; this needs to be done with some minor PHP coding.

In your WordPress Twenty Eleven [child theme](http://randyzwitch.com/tag/child-theme/ &quot;Child Theme posts&quot;), we need to add a new PHP file if it's not already available:  _index.php_.  It is this file that controls the front page container of the blog, and is where the &quot;Next Page/Previous Page&quot; code resides.

In the _index.php_ file, find the following code:

&lt;pre&gt;&amp;lt;?php twentyeleven_content_nav( 'nav-below' ); ?&amp;gt;&lt;/pre&gt;

We want to comment this code out by doing the following:

&lt;pre&gt;&amp;lt;?php /*twentyeleven_content_nav( 'nav-below' );*/ ?&amp;gt;&lt;/pre&gt;

Finally, we want to add a call to the page navigation function by placing the following code directly under the code we just commented out:

&lt;pre&gt;&amp;lt;?php wp_pagenavi(); ?&amp;gt;&lt;/pre&gt;

Once you hit save and refresh your blog, you should now see the page navigation at the bottom of your blog where the default navigation option used to reside!

## WP-PageNavi Settings

The default settings for this plugin should be sufficient for most users, but if you want to change how many page numbers show in the navigation, or any of the other options, you can go to the Settings menu and click on &quot;PageNavi&quot;.  The only setting I've changed so far is the &quot;Number of Pages to Show&quot;, which I set to 10 because I like round numbers.  Given that this blog only has 8 pages as of the time of the tutorial, this means all pages are showing in the page navigation.

## Customizing the Page Navigation with CSS

The plugin by default has its own CSS turned on, which makes the menu look pretty stylish on its own.  Because the page navigation are links, the plugin will keep the same styling as the rest of your page.  Pretty cool!

The only CSS modification I made was to put a small amount of padding on the bottom of the navigation, because it was sitting on top of the gray horizontal line for the footer.  To correct this problem, place the following code in your [style.css](http://randyzwitch.com/2011/07/twenty-eleven-child-theme-creating-css-file/ &quot;Creating Custom CSS file&quot;) file:

&lt;pre&gt;div.wp-pagenavi { padding-bottom: 10px;}&lt;/pre&gt;

You can change the number from &quot;10&quot; to any amount of pixels you'd like, as your design eye sees fit.

And that's it!  Any questions? Leave a comment 🙂</content>
      </item>
      
    
      
      <item>
        <title>Google Analytics Individual Qualification (IQ) - Passed!</title>
        
          <description>&lt;div id=&quot;attachment_856&quot; style=&quot;width: 310px&quot; class=&quot;wp-caption alignright&quot;&gt;
  &amp;lt;img class=&quot;size-full wp-image-856&quot; title=&quot;google-iq-certificate&quot; /wp-content/uploads/2011/10/google-iq-certificate.png?fit=300%2C231&quot; alt=&quot;&quot; srcset=&quot;http://i0.wp.com/randyzwitch.com/wp-content/uploads/2011/10/google-iq-certificate.png?w=300 300w, http://i0.wp.com/randyzwitch.com/wp-content/uploads/2011/10/google-iq-certificate.png?resize=150%2C115 150w&quot; sizes=&quot;(max-width: 300px) 100vw, 300px&quot; data-recalc-dims=&quot;1&quot; /&amp;gt;

&lt;/div&gt;
</description>
        
        <pubDate>Tue, 18 Oct 2011 03:21:31 -0400</pubDate>
        <link>
        http://randyzwitch.com/google-analytics-individual-qualification/</link>
        <guid isPermaLink="true">http://randyzwitch.com/google-analytics-individual-qualification/</guid>
        <content type="html" xml:base="/google-analytics-individual-qualification/">&lt;!DOCTYPE html&gt;
&lt;html lang=&quot;en-US&quot;&gt;
&lt;meta charset=&quot;utf-8&quot;&gt;
&lt;title&gt;Redirecting…&lt;/title&gt;
&lt;link rel=&quot;canonical&quot; href=&quot;/tags/#google_analytics&quot;&gt;
&lt;meta http-equiv=&quot;refresh&quot; content=&quot;0; url=/tags/#google_analytics&quot;&gt;
&lt;h1&gt;Redirecting…&lt;/h1&gt;
&lt;a href=&quot;/tags/#google_analytics&quot;&gt;Click here if you are not redirected.&lt;/a&gt;
&lt;script&gt;location=&quot;/tags/#google_analytics&quot;&lt;/script&gt;
&lt;/html&gt;</content>
      </item>
      
    
      
      <item>
        <title>iPhone 4S: A Worthy Upgrade Clouded By Activation Issues</title>
        
          <description>&lt;p&gt;When the iPhone 4S was announced on October 4th, there was much gnashing of teeth by Apple fanboys and stock market &lt;a title=&quot;Yahoo Finance Apple 4S&quot; href=&quot;http://finance.yahoo.com/blogs/breakout/apple-iphone-4s-opens-door-competitors-analyst-210516885.html&quot; target=&quot;_blank&quot;&gt;analysts&lt;/a&gt; about how this “lackluster” iPhone refresh was going to allow competitors to overtake Apple. That without a re-designed case and bigger screen and more storage and…and…and. All of the rumors (and the subsequent disappointment) ignored one of the simple economic realities of the cell phone market:&lt;/p&gt;

</description>
        
        <pubDate>Fri, 14 Oct 2011 11:07:29 -0400</pubDate>
        <link>
        http://randyzwitch.com/iphone-4s-activation-issues/</link>
        <guid isPermaLink="true">http://randyzwitch.com/iphone-4s-activation-issues/</guid>
        <content type="html" xml:base="/iphone-4s-activation-issues/">When the iPhone 4S was announced on October 4th, there was much gnashing of teeth by Apple fanboys and stock market &lt;a title=&quot;Yahoo Finance Apple 4S&quot; href=&quot;http://finance.yahoo.com/blogs/breakout/apple-iphone-4s-opens-door-competitors-analyst-210516885.html&quot; target=&quot;_blank&quot;&gt;analysts&lt;/a&gt; about how this &quot;lackluster&quot; iPhone refresh was going to allow competitors to overtake Apple. That without a re-designed case and bigger screen and more storage and...and...and. All of the rumors (and the subsequent disappointment) ignored one of the simple economic realities of the cell phone market:

&lt;p style=&quot;padding-left: 30px;&quot;&gt;
  &lt;em&gt;When buyers are locked in a two-year upgrade cycle, you don't need to change the product landscape with every model.&lt;/em&gt;
&lt;/p&gt;

So if you're like me and using an iPhone 3GS (or older!), the iPhone 4S should definitely be a worthy upgrade.

## iPhone 4S Hardware: Many Improvements over the iPhone 3GS

One of the biggest reasons I've been looking to upgrade from the iPhone 3GS is that my phone has just started acting &quot;tired&quot; in the last 6-9 months.  When I purchased the 3GS, that was before iOS4 and iOS5, and with each iOS update I'm sure the processor usage has increased.  Skipping the iPhone 4 and waiting for the 4S, that's two processor upgrades compared to my current phone, which I'm sure will make the phone feel that much speedier.

But it's not just the processor here that's a huge improvement...moving to the 4S gives a better camera with flash, front-facing camera to use with FaceTime, the Retina display for improved screen clarity, longer battery life, and 1080P video capturing.  I opted to purchase the white iPhone 4S (which doesn't provide any additional functionality!) and upgraded to the 32GB storage instead of 16GB on my 3GS.

I think it's safe to say that even without a case re-design, the iPhone 4S is in a different world hardware-wise than my current 3GS.

## iPhone 4S Activation Issues

At this point, you're probably expecting that I would cover how the iOS5 software is working on said hardware upgrade.  But if you've been following any of the major tech blogs like &lt;a title=&quot;CNET iPhone 4S story&quot; href=&quot;http://news.cnet.com/8301-13506_3-20120568-17/iphone-4s-buyers-complain-of-at-t-activation-issues/?tag=mncol&quot; target=&quot;_blank&quot;&gt;CNET&lt;/a&gt;, you see that there are widespread reports of users being unable to activate their iPhone 4S's.  At first, I thought it was just AT&amp;T's generally poor service and network, but &lt;a title=&quot;InformationWeek iPhone 4S&quot; href=&quot;http://www.informationweek.com/news/mobility/smart_phones/231900844&quot; target=&quot;_blank&quot;&gt;InformationWeek&lt;/a&gt; is reporting that all carriers are experiencing this phenomenon.  There may have been gnashing of teeth during the iPhone 4S announcement, but with Apple's &lt;a title=&quot;Apple sells out iPhone 4S pre-sale&quot; href=&quot;http://www.usatoday.com/tech/news/story/2011-10-08/apple-iphone-pre-orders/50706650/1&quot; target=&quot;_blank&quot;&gt;announcement&lt;/a&gt; of a pre-sale sell out, clearly there are ton of buyers out there.

One thing I can blame on AT&amp;T though is a lack of foresight of proper error messaging.  When you sign in through iTunes, here's the error message you get:

![iphone-4S-activation-error-att](/wp-content/uploads/2011/10/iphone-4S-activation-error-att.png)

Well, which is it AT&amp;T and Apple?  If my activation is still pending, and you are going to send me an email, why do I need to try again later?  I have to believe that some of the activation server problems stem from not being clear about what to do.  With users (like myself) re-sending the activation over-and-over again, and potentially a script inside of AT&amp;T trying to activate all these iPhones, there's got to be a ton of redundant processor cycles being burned.

I've also tried to activate straight from my iPhone via Wi-Fi and over the 3G cell network, and even called AT&amp;T.  Not surprisingly, the customer &quot;service&quot; rep at AT&amp;T claimed there's no internal report of an activation problem, but I should try iTunes to see if that will activate my phone.

Thanks for nothing.

## Once the Activation Issues disappear...

Eventually, I'll be really happy with my purchase.  Until then, I've got the equivalent of a Christmas present that comes &quot;Some Assembly Required&quot; or &quot;Needs Batteries (not included)&quot;.  What was the promising start to the weekend with my new iPhone 4S is now just a blackout period, waiting for AT&amp;T to get their shit together.

## Update:  New error message

As if the first confusing error message wasn't enough, now AT&amp;T/Apple have switched it to say &quot;Turn your phone on and off to retry&quot;.  Wow, the old &quot;Did you reboot your PC?&quot; trick!

7 hours in, who knows how many attempts...no activation.

![iphone-4s-activate-off-on](/wp-content/uploads/2011/10/iphone-4s-activate-off-on.png)

## Update 2:  iPhone 4S Activation Complete!

After 7 hours of trying, finally I have a working iPhone 4S!

![iphone-4S-activation-complete](/wp-content/uploads/2011/10/iphone-4S-activation-complete.png)</content>
      </item>
      
    
      
      <item>
        <title>Google Analytics SEO reports: Not Ready For Primetime?</title>
        
          <description>&lt;p&gt;On October 4th, Google announced that the &lt;a href=&quot;http://analytics.blogspot.com/2011/10/webmaster-tools-in-google-analytics-for.html&quot;&gt;Webmaster Tools/Google Analytics integration&lt;/a&gt; was now available to all users. The three new reports (Queries, Landing Pages, and Geographical Summary) are intended to allow site owners and content creators to monitor how well their content is indexed in Google for their keywords of interest across time, all within the Google Analytics interface.  However, based on my preliminary research from the first few days of data, I have to question the current algorithm’s accuracy.&lt;/p&gt;

</description>
        
        <pubDate>Thu, 06 Oct 2011 04:40:58 -0400</pubDate>
        <link>
        http://randyzwitch.com/google-analytics-seo-reports-inaccurate/</link>
        <guid isPermaLink="true">http://randyzwitch.com/google-analytics-seo-reports-inaccurate/</guid>
        <content type="html" xml:base="/google-analytics-seo-reports-inaccurate/">On October 4th, Google announced that the [Webmaster Tools/Google Analytics integration](http://analytics.blogspot.com/2011/10/webmaster-tools-in-google-analytics-for.html) was now available to all users. The three new reports (Queries, Landing Pages, and Geographical Summary) are intended to allow site owners and content creators to monitor how well their content is indexed in Google for their keywords of interest across time, all within the Google Analytics interface.  However, based on my preliminary research from the first few days of data, I have to question the current algorithm's accuracy.

## Google Analytics SEO reports:  Impressions, Clicks,  Average Position, CTR

![google-seo-query-report](/wp-content/uploads/2011/10/google-seo-query-report.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
    Google Analytics SEO Report: Queries
&lt;/p&gt;

All three reports have the same format, showing Impressions in Google search, Clicks, Average Link position (Organic) and Click-through Rate (CTR).  You can show this data by:

  * Search query: to understand how specific search terms are ranking
  * Landing page: to show how well individual pages (and their position) lead to clicks
  * Geography:  to understand how well your pages are ranking in your target market(s)

To avoid problems of false precision, these reports appear to round impressions to the nearest 10 for numbers less than 1000, and then to the nearest hundred when impressions are &gt; 1000. Similarly, clicks aren't reported when there are less than 10, although a CTR is reported...which is it Google, not enough data or is it measured precisely?

## I rank WHERE for a female body part?

![google-seo-report-womens-body-part](/wp-content/uploads/2011/10/google-seo-report-womens-body-part.png)

 &lt;p class=&quot;wp-caption-text&quot;&gt;
    Ranked 8th on average for these keywords Google? I think not.
 &lt;/p&gt;

In the web analytics world, if you aren't comfortable with imprecision and incomplete data, you'll get burned out pretty quickly.  My above example of the exact click-through rate calculated from an inexact display of clicks is a minor nitpick.  However, when I see data from the table above being written into my account, I have to wonder just how precisely Google is measuring their impressions data.

The table above is from my other blog about the &lt;a title=&quot;Duke Cross Continent MBA blog&quot; href=&quot;http://the-fuqua-experience.com/&quot; target=&quot;_blank&quot;&gt;Duke MBA&lt;/a&gt;; I'm QUITE certain it doesn't rank, on average, 8th for anything to do with female body parts!  I'd be the most in demand SEO in the world if I could pull that off, without even having the words on my page.  I would've been comfortable chalking that result up to a weird bug, had the page the query references was mangled.  It turns out, they all link to the &lt;a title=&quot;Small town girl with BIG ambitions&quot; href=&quot;http://the-fuqua-experience.com/blog/2011/06/30/small-town-girl-with-big-ambitions/&quot; target=&quot;_blank&quot;&gt;same exact blog post&lt;/a&gt;, the story of a current (female, naturally) student's journey from a small town in India to attending business school.  From what I can tell, the error is persistent as well, showing a small number of impressions every day.

## Web Analytics:  Again, it all comes down to the Analyst

The above example is somewhat tongue-in-cheek; obviously it's a data error, and I'm not running a multi-million dollar e-commerce website.  Heck, I'm not even paying for Google Analytics.  But had I been part of the Beta test of the Google Analytics/Webmaster Tools integration, I think I would've provided the following comments:

  * &lt;span style=&quot;text-decoration: underline;&quot;&gt;&lt;em&gt;Don't show search terms where there are low number of impressions&lt;/em&gt;:&lt;/span&gt; if you are getting 50 impressions per day and you rank 213th, you're not really ranking for that term
  * &lt;span style=&quot;text-decoration: underline;&quot;&gt;&lt;em&gt;Accuracy vs. Precision&lt;/em&gt;:&lt;/span&gt;  Either round the numbers or don't.  Rounding impressions, putting &lt;10 for clicks, then dividing the two to provide a CTR doesn't provide confidence in the data
  * &lt;span style=&quot;text-decoration: underline;&quot;&gt;&lt;em&gt;Provide the same reporting drill-down capabilities from Webmaster Tools within Google Analytics&lt;/em&gt;:&lt;/span&gt; To find out which page is ranking for this error term, I started in Google Analytics, but needed to go to Webmaster Tools.  Kinda defeats the purpose of having the data in the Google Analytics interface.

I'm confident now that Google has decided to step into the paid web analytics arena that these reports will only improve over time.  For now, I'll be taking a sharp eye to the results, manually typing the queries into Google where necessary to see if I'm truly ranking where it says I am.

(And yes, I verified I don't rank 8th for pornography terms ;))</content>
      </item>
      
    
      
      <item>
        <title>An Afternoon With Edward Tufte</title>
        
          <description>&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2011/10/tufte-data-visualization-300x225.jpg&quot; alt=&quot;tufte-data-visualization&quot; /&gt;&lt;/p&gt;

</description>
        
        <pubDate>Wed, 05 Oct 2011 04:22:22 -0400</pubDate>
        <link>
        http://randyzwitch.com/an-afternoon-with-edward-tufte/</link>
        <guid isPermaLink="true">http://randyzwitch.com/an-afternoon-with-edward-tufte/</guid>
        <content type="html" xml:base="/an-afternoon-with-edward-tufte/">![tufte-data-visualization](/wp-content/uploads/2011/10/tufte-data-visualization-300x225.jpg)

&lt;p class=&quot;wp-caption-text&quot;&gt;
There had to be 400+ people in the seminar!
&lt;/p&gt;

Yesterday, I had the opportunity to attend the &quot;Presenting Data and Information&quot; seminar hosted by Edward Tufte in Philadelphia.  A world-renowned expert in the field of data presentation/visualization, Edward Tufte has written seven books outlining terrible and fantastic examples of data display (and how to make sure your charts and tables fall into the latter!)

Unfortunately, as great as each of these books are at explaining methods for data visualization, the seminar was little more than a topical discussion of his book material, rather than a concise summary of what pitfalls to avoid. However, for the relatively low cost of the seminar ($380) and receiving hardcover editions of 4 of 7 of Tufte's works, there are many worse ways to spend your time and money if you are a data enthusiast!

## Course materials

Each attendee received a copy of the following books:

  * [The Visual Display of Quantitative Information](http://www.amazon.com/gp/product/0961392142/ref=as_li_ss_tl?ie=UTF8&amp;tag=thefuquexpe-20&amp;linkCode=as2&amp;camp=217145&amp;creative=399369&amp;creativeASIN=0961392142) - The book that started it all.  I've read this book in the past, and I'm glad I now have my own copy.
  * [Envisioning Information](http://www.amazon.com/gp/product/0961392118/ref=as_li_ss_tl?ie=UTF8&amp;tag=thefuquexpe-20&amp;linkCode=as2&amp;camp=217145&amp;creative=399369&amp;creativeASIN=0961392118)&lt;img style=&quot;border: none !important; margin: 0px !important;&quot; src=&quot;http://www.assoc-amazon.com/e/ir?t=thefuquexpe-20&amp;l=as2&amp;o=1&amp;a=0961392118&amp;camp=217145&amp;creative=399369&quot; alt=&quot;&quot; width=&quot;1&quot; height=&quot;1&quot; border=&quot;0&quot; /&gt;
  * [Visual Explanations: Images and Quantities, Evidence and Narrative](http://www.amazon.com/gp/product/0961392126/ref=as_li_ss_tl?ie=UTF8&amp;tag=thefuquexpe-20&amp;linkCode=as2&amp;camp=217145&amp;creative=399369&amp;creativeASIN=0961392126)
  * [Beautiful Evidence](http://www.amazon.com/gp/product/0961392177/ref=as_li_ss_tl?ie=UTF8&amp;tag=thefuquexpe-20&amp;linkCode=as2&amp;camp=217145&amp;creative=399369&amp;creativeASIN=0961392177)

These books are so dense with information that it will probably take me a month or more to read each book!

## &lt;span class=&quot;Apple-style-span&quot; style=&quot;color: #000000; font-weight: bold;&quot;&gt;If your data are boring, you've got the wrong data&lt;/span&gt;

&lt;div&gt;
  &lt;p&gt;
    Of the many positives of this seminar, I appreciated how Tufte hammered on a few main topics, the most important of which is &lt;em&gt;'If your data is boring, you've got the wrong data'&lt;/em&gt;.  I think this is often overlooked when thinking about success in business; if your meetings are dull and people dread when you send out a meeting request, you need better content!  It's (usually) not a visualization problem, and in many ways, it's not a presentation style problem.  If you've got great data, people will overlook an annoying presenter.  But without content that speaks to what the audience is interested in knowing, you might as well not give a presentation at all.&lt;br /&gt; &lt;br /&gt;
  &lt;/p&gt;

  &lt;h2&gt;
    Don't fall into the PowerPoint trap!
  &lt;/h2&gt;

  &lt;p&gt;
    The other main point that Tufte really hammered on was if you let the limitations of a tool like PowerPoint dictate how you perform and present analysis, then you've failed as an analyst.  Humans have an extraordinary ability to process dense amounts of information; by limiting yourself to presenting your analysis in 3 bullets and 10 words per page, you are just perpetuating the 'stupidity' (his words) of that 'authoritarian form of communication'.
  &lt;/p&gt;

  &lt;p&gt;
    As an alternative to PowerPoint style charts and graphs, the seminar really focused on &lt;a title=&quot;Galileo's Sunspot drawings&quot; href=&quot;http://galileo.rice.edu/images/things/tres_epistolae.gif&quot; target=&quot;_blank&quot;&gt;hand-drawn illustrations&lt;/a&gt; from Galileo's Sunspot &lt;a title=&quot;Galileo Sunspots text&quot; href=&quot;http://galileo.rice.edu/sci/observations/sunspots.html&quot; target=&quot;_blank&quot;&gt;discovery&lt;/a&gt; and examples from cartographers about how to present multi-variate data structures.  Even though paper (or a PowerPoint slide on-screen) is limited in two dimensions, there are many ways to increase the information density to six or more dimensions.  &lt;a title=&quot;Sparkline discussion&quot; href=&quot;http://www.edwardtufte.com/bboard/q-and-a-fetch-msg?msg_id=0001OR&quot; target=&quot;_blank&quot;&gt;Sparklines&lt;/a&gt; were also discussed in detail, to keep the data in-line with text and to be able to show data trends where the actual numbers aren't necessarily important (or are presented elsewhere).
  &lt;/p&gt;

  &lt;h2&gt;
    Simultaneously negative and pie-in-the-sky
  &lt;/h2&gt;

  &lt;p&gt;
    I'm not going to focus too much on the negatives here, but one thing that really surprised me about this seminar was how negative in tone the presentation seemed.  I realize part of it was sarcasm (and possibly an affectation), but I would've preferred approaching the topic as what can/should be done to advance the cause, instead of what 'sucks'. Everyone in the room is acutely aware of what sucks in the PowerPoint culture of the business world; moving past that is what everyone was there to learn.
  &lt;/p&gt;

  &lt;p&gt;
    Simultaneously, when talking about improvements, most of them seemed to be unrealistic to actually implement in the real world (not all of us live in Ivory Tower academia, Dr. Tufte 🙂 ).  Suggestions like stripping slides of 'administrative overhead' like corporate logos and style sheets, bringing the level of a presentation WAY up as if everyone is as smart as the presenter, and writing long prose instead of highlighting comments are just unrealistic for most workers.  Most of the suggestions are corporate culture issues, and ones that a lowly data analyst isn't going to be able to change.
  &lt;/p&gt;

  &lt;h2&gt;
    Summary: The right data should be able to 'sell' any presentation
  &lt;/h2&gt;

  &lt;p&gt;
    In the end, a 5-hour seminar isn't going to change the business world or turn anyone into a super-analyst. But hearing Dr. Tufte speak about elegant design in data visualizations reminded me that I'm the one that controls the outcome of any presentation.  With the right data, shown properly, I should be able to 'sell' anyone on an idea without having to do any salesmanship at all.  The data is what sells an idea, not slick talking and 3 bullets per page.
  &lt;/p&gt;
&lt;/div&gt;</content>
      </item>
      
    
      
      <item>
        <title>Google Analytics Custom Variables: A Page-Level Example</title>
        
          <description>&lt;p&gt;Once you’ve &lt;a title=&quot;Google Analytics for WordPress: Two Methods&quot; href=&quot;http://randyzwitch.com/2011/08/google-analytics-for-wordpress/&quot; target=&quot;_blank&quot;&gt;implemented&lt;/a&gt; Google Analytics on your WordPress blog, you’ll likely find that the default reports aren’t providing the site-specific information you are looking for…or, maybe just not at the level of aggregation you’d prefer.   Google Analytics custom variables provide a method of capturing your site-specific information, depending on whether the information changes once per visitor, once per session, or once per page.  Examples of custom variable usage includes:&lt;/p&gt;

</description>
        
        <pubDate>Sun, 02 Oct 2011 07:10:37 -0400</pubDate>
        <link>
        http://randyzwitch.com/google-analytics-custom-variables/</link>
        <guid isPermaLink="true">http://randyzwitch.com/google-analytics-custom-variables/</guid>
        <content type="html" xml:base="/google-analytics-custom-variables/">Once you've &lt;a title=&quot;Google Analytics for WordPress: Two Methods&quot; href=&quot;http://randyzwitch.com/2011/08/google-analytics-for-wordpress/&quot; target=&quot;_blank&quot;&gt;implemented&lt;/a&gt; Google Analytics on your WordPress blog, you'll likely find that the default reports aren't providing the site-specific information you are looking for...or, maybe just not at the level of aggregation you'd prefer.   Google Analytics custom variables provide a method of capturing your site-specific information, depending on whether the information changes once per visitor, once per session, or once per page.  Examples of custom variable usage includes:

  * Demographic information, such as Gender (_Visitor-level, never changes_)
  * Visitor logs in to your website (_Session-level, may not log in during future visits_)
  * Each section of the website a visitor &quot;touches&quot; (_Page-level, changing multiple times during a session_)

This tutorial will cover the Page-level custom variable type, capturing the WordPress Category for each blog post.  With this information, we'll be able to see which categories of posts are most popular on your WordPress blog over time.

## Setting a Google Analytics custom variable

To set a Google Analytics custom variable, we need to use the following syntax:

{% highlight javascript linenos %}
_setCustomVar(index, name, value, opt_scope)
{% endhighlight %}

The `index` section of the variable indicates which of the five allowable custom variables we want to use to record our information (slot 1-5).  `name` indicates what we want to call our variable.  `value` is going to be the actual value we are looking to save.  And finally, `opt_scope` represents whether we want the variable to be page-level, session-level, or visitor-level.

## Recording WordPress category into a custom variable

In order to capture the WordPress category in a Google Analytics custom variable, we're going to use a combination of PHP, WordPress &lt;a title=&quot;WordPress functions&quot; href=&quot;http://codex.wordpress.org/Function_Reference&quot; target=&quot;_blank&quot;&gt;functions&lt;/a&gt;, and Google Analytics code.  Here's the code snippet we're going to use:

{% highlight php linenos %}
if (is_single () ) {

$category = get_the_category();
echo &quot;_gaq.push(['_setCustomVar', 2,'Category','&quot;. $category[0]-&gt;cat_name. &quot;', 3]);&quot;;

} else {

}
{% endhighlight %}

The `is_single` part of the code is a WordPress function, which evaluates whether or not a given page is a single post.  Since only single post pages have categories, we use this function to set the Google Analytics custom variable only when there is going to be a category value available on the page.  The `$category` part of the code is a PHP variable that stores the entire array of WordPress info that goes along with the `get_the_category` function. Finally, the part of the code that starts `echo` is the PHP code needed to build the Google Analytics custom variable string we want to have. 

Within this code, you can see the `_setCustomVar` code described in the first part of the tutorial; we're setting the `index` value to `2`, which means we're using Google Analytics Custom Variable 2.  The `name` of the variable will be `Category`, the `Value` to be set is the WordPress category value (from the `&quot;'&quot;. $category[0]-&gt;cat\_name. &quot;'` variable), and the `opt_scope` value is set to `3`, which means page-level.

## Incorporating custom variable code into Google Analytics tracking code

According to &lt;a title=&quot;More info on Google Analytics custom variables&quot; href=&quot;http://cutroni.com/blog/2011/05/18/mastering-google-analytics-custom-variables/&quot; target=&quot;_blank&quot;&gt;Justin Cutroni&lt;/a&gt;, who literally wrote the book on [Google Analytics](http://www.amazon.com/gp/product/0596158009/ref=as_li_ss_tl?ie=UTF8&amp;tag=thefuquexpe-20&amp;linkCode=as2&amp;camp=217145&amp;creative=399369&amp;creativeASIN=0596158009), we want to put our custom variable code BEFORE the `_trackPageview` portion of the Google Analytics tracking code whenever possible.  This is because on the last page of a visit, if your custom variable code is after the `_trackPageview` code, Google Analytics won't &quot;see&quot; the custom variable code, since the data has to tag along with a `_trackPageview` call. Here's what the final set of code will look like (place in your &lt;a title=&quot;Installing Google Analytics tracking code&quot; href=&quot;http://randyzwitch.com/2011/08/google-analytics-for-wordpress/&quot; target=&quot;_blank&quot;&gt;header.php&lt;/a&gt; file):

{% highlight php linenos %}
&lt;script type=&quot;text/javascript&quot;&gt;

var _gaq =_gaq || [];
 _gaq.push(['_setAccount', 'UA-XXXXXXXX-X']);

if (is_single () ) {

$category = get_the_category();
echo &quot;_gaq.push(['_setCustomVar', 2,'Category','&quot;. $category[0]-&gt;cat_name. &quot;', 3]);&quot;;

} else {

}

_gaq.push(['_trackPageview']);
_gaq.push(['_trackPageLoadTime']);

(function() {

 var ga = document.createElement('script');
 ga.type = 'text/javascript';
 ga.async = true;

 ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
 var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga,s);
})();

&lt;/script&gt;
{% endhighlight %}

## Example of Custom Variable report

Here's what the report will look like in Google Analytics.  To see the report, go to `Visitors -&gt; Demographics -&gt; Custom Variables`.

![google-analytics-custom-variables](/wp-content/uploads/2011/10/google-analytics-custom-variables-1024x143.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
WordPress Categories in Google Analytics Custom Variable 2
&lt;/p&gt;</content>
      </item>
      
    
      
      <item>
        <title>How To Remove &quot;This entry was posted in&quot; on WordPress single posts</title>
        
          <description>&lt;p&gt;In prior &lt;a title=&quot;Removing “Powered by WordPress” in Twenty Eleven&quot; href=&quot;http://randyzwitch.com/removing-powered-by-wordpress-twenty-eleven/&quot; target=&quot;_blank&quot;&gt;posts&lt;/a&gt;, I’ve commented that I’m a fan of clean, sleek design when it comes to WordPress themes.  I’ve added the “breadcrumb” style navigation to the top of my posts, which makes the “This Entry was Posted in &lt;category&gt;&quot; and &quot;Bookmark the Permalink&quot; text at the bottom of each post redundant.&lt;/category&gt;&lt;/p&gt;

</description>
        
        <pubDate>Tue, 27 Sep 2011 11:29:42 -0400</pubDate>
        <link>
        http://randyzwitch.com/wordpress-remove-this-entry-was-posted-in/</link>
        <guid isPermaLink="true">http://randyzwitch.com/wordpress-remove-this-entry-was-posted-in/</guid>
        <content type="html" xml:base="/wordpress-remove-this-entry-was-posted-in/">In prior &lt;a title=&quot;Removing “Powered by WordPress” in Twenty Eleven&quot; href=&quot;http://randyzwitch.com/removing-powered-by-wordpress-twenty-eleven/&quot; target=&quot;_blank&quot;&gt;posts&lt;/a&gt;, I've commented that I'm a fan of clean, sleek design when it comes to WordPress themes.  I've added the &quot;breadcrumb&quot; style navigation to the top of my posts, which makes the &quot;This Entry was Posted in &lt;category&gt;&quot; and &quot;Bookmark the Permalink&quot; text at the bottom of each post redundant.

Here's how to remove/modify both of these messages through a simple change to the `content-single.php` file.

## Removing all text at the bottom of the single post

To make all of text disappear at the bottom of each post, all we need to do is comment out a few lines of code. Open your `content-single.php` file from your Twenty Eleven child theme and find the following lines of code:

{% highlight php linenos %}
&lt;footer class=&quot;entry-meta&quot;&gt;
&lt;?php
            /* translators: used between list items, there is a space after the comma */
{% endhighlight %}

We'll use our &lt;a title=&quot;Twenty Eleven Child Theme: Custom Header&quot; href=&quot;http://randyzwitch.com/custom-header-twenty-eleven-child-theme/&quot; target=&quot;_blank&quot;&gt;HTML comment tag&lt;/a&gt; to comment out the PHP code that starts the line below this one, and close the comment tag at the end of the PHP script.  When done correctly, the code will look like this:

{% highlight php linenos %}
&lt;!--     &lt;?php
            /* translators: used between list items, there is a space after the comma */
            $categories_list = get_the_category_list( __( ', ', 'twentyeleven' ) );
            /* translators: used between list items, there is a space after the comma */
            $tag_list = get_the_tag_list( '', __( ', ', 'twentyeleven' ) );
            if ( '' != $tag_list ) {
                $utility_text = __( 'This entry was posted in %1$s and tagged %2$s by &lt;a href=&quot;%6$s&quot;&gt;%5$s&lt;/a&gt;. Bookmark the &lt;a href=&quot;%3$s&quot; title=&quot;Permalink to %4$s&quot; rel=&quot;bookmark&quot;&gt;permalink&lt;/a&gt;.', 'twentyeleven' );
            } elseif ( '' != $categories_list ) {
                $utility_text = __( 'This entry was posted in %1$s by &lt;a href=&quot;%6$s&quot;&gt;%5$s&lt;/a&gt;. Bookmark the &lt;a href=&quot;%3$s&quot; title=&quot;Permalink to %4$s&quot; rel=&quot;bookmark&quot;&gt;permalink&lt;/a&gt;.', 'twentyeleven' );
            } else {
                $utility_text = __( 'This entry was posted by &lt;a href=&quot;%6$s&quot;&gt;%5$s&lt;/a&gt;. Bookmark the &lt;a href=&quot;%3$s&quot; title=&quot;Permalink to %4$s&quot; rel=&quot;bookmark&quot;&gt;permalink&lt;/a&gt;.', 'twentyeleven' );
            }
            printf(
                $utility_text,
                $categories_list,
                $tag_list,
                esc_url( get_permalink() ),
                the_title_attribute( 'echo=0' ),
                get_the_author(),
                esc_url( get_author_posts_url( get_the_author_meta( 'ID' ) ) )
            );
        ?&gt; --&gt;
{% endhighlight %}

Hit save and you're done, no more &quot;This Entry was Posted in&quot; or &quot;Bookmark the Permalink&quot; verbiage at the end of your posts!

## Modifying the text at the bottom of the post to just keep the Post Tags

Perhaps you don't want to remove the text entirely from the bottom of the post, but just want to leave the tags behind (for SEO purposes or whatever).  To do this, we'll modify the same piece of code, but instead of commenting out all of the PHP code, we'll comment out a smaller piece of code, then redefine the `$utility_text`  PHP variable.

The piece of code we want to comment out is shown below.  Note that because this code is within a PHP code block, we need to comment the code out using a &quot;forward slash-star, star-backslash&quot; comment tag:

{% highlight php linenos %}
/*  if ( '' != $tag_list ) {
				$utility_text = __( 'This entry was posted in %1$s and tagged %2$s by %5$s. Bookmark the permalink.', 'twentyeleven' );
			} elseif ( '' != $categories_list ) {
				$utility_text = __( 'This entry was posted in %1$s by %5$s. Bookmark the permalink.', 'twentyeleven' );
			} else {
				$utility_text = __( 'This entry was posted by %5$s. Bookmark the permalink.', 'twentyeleven' );
			}
*/
{% endhighlight %}

With this code commented out, we can now define the _$utility_text_ variable as we want.  To show just the text &quot;Tagged: &lt;tag list&gt;&quot;, add the following code just below the commented code above:

{% highlight php linenos %}
$utility_text = _( 'Tagged: %2$s');
{% endhighlight %}

Once you hit save, the bottom of each of your single posts will show the tags that the post belongs to.</content>
      </item>
      
    
      
      <item>
        <title>Xchange 2011: Think Tank and Harbor Cruise</title>
        
          <description>&lt;div id=&quot;attachment_558&quot; style=&quot;width: 637px&quot; class=&quot;wp-caption aligncenter&quot;&gt;
  &amp;lt;img class=&quot;size-full wp-image-558&quot; title=&quot;sunset-san-diego-harbor&quot; /wp-content/uploads/2011/09/sunset-san-diego-harbor.jpg?fit=627%2C338&quot; alt=&quot;&quot; srcset=&quot;http://i0.wp.com/randyzwitch.com/wp-content/uploads/2011/09/sunset-san-diego-harbor.jpg?w=627 627w, http://i0.wp.com/randyzwitch.com/wp-content/uploads/2011/09/sunset-san-diego-harbor.jpg?resize=300%2C161 300w, http://i0.wp.com/randyzwitch.com/wp-content/uploads/2011/09/sunset-san-diego-harbor.jpg?resize=500%2C269 500w&quot; sizes=&quot;(max-width: 627px) 100vw, 627px&quot; data-recalc-dims=&quot;1&quot; /&amp;gt;

&lt;/div&gt;
</description>
        
        <pubDate>Tue, 13 Sep 2011 07:06:58 -0400</pubDate>
        <link>
        http://randyzwitch.com/xchange-2011-think-tank/</link>
        <guid isPermaLink="true">http://randyzwitch.com/xchange-2011-think-tank/</guid>
        <content type="html" xml:base="/xchange-2011-think-tank/">&lt;!DOCTYPE html&gt;
&lt;html lang=&quot;en-US&quot;&gt;
&lt;meta charset=&quot;utf-8&quot;&gt;
&lt;title&gt;Redirecting…&lt;/title&gt;
&lt;link rel=&quot;canonical&quot; href=&quot;/tags/#Analytics&quot;&gt;
&lt;meta http-equiv=&quot;refresh&quot; content=&quot;0; url=/tags/#Analytics&quot;&gt;
&lt;h1&gt;Redirecting…&lt;/h1&gt;
&lt;a href=&quot;/tags/#Analytics&quot;&gt;Click here if you are not redirected.&lt;/a&gt;
&lt;script&gt;location=&quot;/tags/#Analytics&quot;&lt;/script&gt;
&lt;/html&gt;</content>
      </item>
      
    
      
      <item>
        <title>How to Disable Mobile Theme in WordPress</title>
        
          <description>&lt;p&gt;With smartphone usage on the rise, you might wonder why anyone would want to disable mobile support in WordPress (specifically, in the Twenty Eleven theme).  Answer: It’s poorly implemented, or at minimum sub-optimal in its default settings.&lt;/p&gt;

</description>
        
        <pubDate>Fri, 09 Sep 2011 07:53:04 -0400</pubDate>
        <link>
        http://randyzwitch.com/disable-mobile-theme-in-wordpress/</link>
        <guid isPermaLink="true">http://randyzwitch.com/disable-mobile-theme-in-wordpress/</guid>
        <content type="html" xml:base="/disable-mobile-theme-in-wordpress/">With smartphone usage on the rise, you might wonder why anyone would want to disable mobile support in WordPress (specifically, in the Twenty Eleven theme).  Answer: It's poorly implemented, or at minimum sub-optimal in its default settings.

![wordpress-mobile](/wp-content/uploads/2011/09/wordpress-mobile.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
Default WordPress mobile theme view
&lt;/p&gt;

While the readability of each post is improved by laying out the main post area as full width, by placing the sidebars at the bottom, your readers may miss any content you are hoping to share.  For this website, the search button, social networking buttons, and Twitter feed information are shown at the bottom of the site layout (see picture on left).  It's not the end of the world, but not ideal either.

To make things worse, if you start making changes to your CSS file to customize the theme and aren't careful, the mobile theme will get mangled.  In my case, I assume that because I specified some of my width parameters in pixels instead of percentages, the mobile theme doesn't know how to display them. But, since the traffic to this blog is mainly from desktop/laptop computers (which I know because I'm using &lt;a title=&quot;WordPress Stats or Google Analytics?  Yes!&quot; href=&quot;http://randyzwitch.com/2011/07/wordpress-stats-or-google-analytics/&quot; target=&quot;_blank&quot;&gt;Google Analytics&lt;/a&gt;!), I'll optimize the blog design to non-mobile devices at the risk of mangling the mobile theme.

Luckily, what makes the mobile theme sub-optimal when using the Twenty Eleven theme is what makes it so easy to _disable_ the mobile theme!

**Disable the &quot;viewport&quot;**

In order to turn off the mobile theme, all we need to do is comment out a single line of code within our `header.php` file.  The line we are looking for is a meta tag that refers to the &quot;viewport&quot;, which is a mobile META tag to specify dimensions for a mobile browser to use, and well as control/modify some functionality such as browser scaling (zoom).  Here's what the commented out code should look like:

&lt;pre&gt;&amp;lt;head&amp;gt;
&amp;lt;meta charset=&quot;&amp;lt;?php bloginfo( 'charset' ); ?&amp;gt;&quot; /&amp;gt;

&amp;lt;!-- &amp;lt;meta name=&quot;viewport&quot; content=&quot;width=device-width&quot; /&amp;gt; --&amp;gt;&lt;/pre&gt;

![css-modifications-ruin-wordpress-mobile-theme](/wp-content/uploads/2011/09/css-modifications-ruin-wordpress-mobile-theme-200x300.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
Any CSS mods and the mobile theme breaks down
&lt;/p&gt;

Inside of this META tag, the &quot;content&quot; parameter specifies that the mobile browser should set the width of the content equal to whatever the device width is currently set.  So in the case of iPhones where the width varies depending on the phone orientation, the browser will re-size the content appropriately.

**But, what if I want to just improve the mobile browser experience?**

Obviously, just because the default mobile functionality in this theme doesn't work very well, that doesn't mean your desktop theme is ideal either.  If you want to design a mobile theme, you can use this same &quot;Viewport&quot; line of code to play around with the settings, hopefully finding ones that not only make your site look great, but are also compatible with the range of smartphones on the market.

For an explanation of all of the parameters that can be changed as part of the Viewport META tag, see this [article](http://learnthemobileweb.com/2009/07/mobile-meta-tags/ &quot;Viewport META tag explanation&quot;) from 'Learn the Mobile Web'.</content>
      </item>
      
    
      
      <item>
        <title>Installing Google Plus Button on WordPress</title>
        
          <description>&lt;p&gt;If you’ve been anywhere on the internet in the past few months, you’ve seen the +1 button from Google right next to Facebook, Twitter, StumbleUpon and other social media sharing buttons.  It’s pretty clear that Google intends the “plus one” to be THE most important form of approval on the ‘net, integrating all those “plusses” into search results as well as showing them within the Google+ Plus network.&lt;/p&gt;

</description>
        
        <pubDate>Sat, 03 Sep 2011 07:33:11 -0400</pubDate>
        <link>
        http://randyzwitch.com/google-plus-button-wordpress-twenty-eleven/</link>
        <guid isPermaLink="true">http://randyzwitch.com/google-plus-button-wordpress-twenty-eleven/</guid>
        <content type="html" xml:base="/google-plus-button-wordpress-twenty-eleven/">If you've been anywhere on the internet in the past few months, you've seen the +1 button from Google right next to Facebook, Twitter, StumbleUpon and other social media sharing buttons.  It's pretty clear that Google intends the &quot;plus one&quot; to be THE most important form of approval on the 'net, integrating all those &quot;plusses&quot; into search results as well as showing them within the Google+ Plus network.

Here's how to integrate the Google Plus button on the WordPress Twenty Eleven theme.

&lt;span class=&quot;Apple-style-span&quot; style=&quot;font-weight: bold; color: #000000;&quot;&gt;Installing the Google Plus JavaScript code snippet&lt;/span&gt;

The first step to adding the Google Plus button to this theme is to add the JavaScript code snippet that &quot;talks&quot; to Google.  This code can be placed anywhere in the theme, but lets install it in our `footer.php` file, right before the the closing &lt;/body&gt; tag:

{% highlight javascript linenos %}
&lt;script type=&quot;text/javascript&quot;&gt;
  (function() {
    var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;
    po.src = 'https://apis.google.com/js/plusone.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  })();
&lt;/script&gt;

&lt;/body&gt;
{% endhighlight %}

## Adding Google Plus button on front page

To add the Google Plus button to each post displayed on the first page, we need to modify the `content.php` file.  Like all of the other modifications we've made to create a [child theme](http://randyzwitch.com/tag/child-theme/ &quot;WordPress Twenty Eleven Child Theme&quot;) like creating a [custom header](http://randyzwitch.com/2011/07/custom-header-twenty-eleven-child-theme/ &quot;Twenty Eleven Child Theme:  Custom Header&quot;) and [custom footer](http://randyzwitch.com/2011/08/removing-powered-by-wordpress-twenty-eleven/ &quot;Removing “Powered by WordPress” in Twenty Eleven&quot;), copy the `content.php` file into your child theme folder. To display the Google Plus button underneath the title, we need to place the following code, after the `&lt;div class=&quot;entry-content&quot;&gt;` and before the `&lt;?php the_content&gt;` tag:

{% highlight javascript linenos %}
&lt;div class=&quot;entry-content&quot;&gt;
&lt;!-- Place this tag where you want the +1 button to render --&gt;
&lt;g:plusone href=&quot;&lt;?php the_permalink(); ?&gt;&quot;&gt;&lt;/g:plusone&gt;
&lt;?php the_content( __( 'Continue reading &lt;span class=&quot;meta-nav&quot;&gt;&amp;rarr;&lt;/span&gt;', 'twentyeleven' ) ); ?&gt;
{% endhighlight %}

In the Google button code, the piece of code that references `the_permalink` is a WordPress function that passes the link of blog post to the Google Plus button.  Without this addition to the button, Google will use the URL of the page you are on…in my case, the 3 buttons on the front page would all reference www.randyzwitch.com, which means a visitor wouldn't be able to recommend an individual post.

## Adding a Google Plus button to an individual blog post

To add the Google Plus button on an individual article or blog post using the &quot;Standard&quot; format in Twenty Eleven, we can do the same basic process as above, except we'll modify the `content-single.php` file (again, copy this file into your child theme folder!).  We'll add the code after the &lt;h1&gt; tag and before the &quot;If&quot; statement that starts building the content:

{% highlight php linenos %}
&lt;h1 class=&quot;entry-title&quot;&gt;&lt;?php the_title(); ?&gt;&lt;/h1&gt;
&lt;!-- Place this tag where you want the +1 button to render --&gt;
&lt;g:plusone href=&quot;&lt;?php the_permalink(); ?&gt;&quot;&gt;&lt;/g:plusone&gt;

&lt;?php if ( 'post' == get_post_type() ) : ?&gt;
{% endhighlight %}

Technically, you don't need to specify the `the_permalink` reference, since there will only be a single button on an individual blog post, and the Google Plus button will use the URL of the page.  But, it doesn't hurt, so for documentation sake I've left it in.

Once you've added the button code on the individual page, you're done! Visitors to your blog can now recommend your content on Google search and on the Google+ network.

## Optional steps

For this tutorial, I only provided the code for modifying the &quot;Standard&quot; page format for a single blog post.  If you are using multiple formats across your blog, you'll need to modify each template you are using.  They all begin with `content-&lt;format&gt;.php`, and the button code goes after the &lt;h1&gt; tag and before any other code.

It's also possible to modify how large the Google Plus button is and what is displayed.  I'm using the default bubble annotation, but if you'd like to have a larger or smaller button, or just different options, you can use the code builder from Google to see the [button options](http://www.google.com/intl/en/webmasters/+1/button/index.html &quot;Google Plus button size&quot;).</content>
      </item>
      
    
      
      <item>
        <title>Prevent Google Analytics From Tracking Logged-in Users</title>
        
          <description>&lt;p&gt;In a previous &lt;a title=&quot;WordPress Stats or Google Analytics?  Yes!&quot; href=&quot;http://randyzwitch.com/2011/07/wordpress-stats-or-google-analytics/&quot; target=&quot;_blank&quot;&gt;post&lt;/a&gt;, I discussed how it’s useful to have &lt;em&gt;both&lt;/em&gt; WordPress Stats and Google Analytics installed on a WordPress blog; the former can be used as a quick dashboard to check in on the day’s traffic, and the latter can be used for more advanced analysis. But in order to keep our datasets comparable, we need to track users in a similar manner.&lt;/p&gt;

</description>
        
        <pubDate>Wed, 24 Aug 2011 04:22:15 -0400</pubDate>
        <link>
        http://randyzwitch.com/prevent-google-analytics-tracking-logged-in/</link>
        <guid isPermaLink="true">http://randyzwitch.com/prevent-google-analytics-tracking-logged-in/</guid>
        <content type="html" xml:base="/prevent-google-analytics-tracking-logged-in/">In a previous &lt;a title=&quot;WordPress Stats or Google Analytics?  Yes!&quot; href=&quot;http://randyzwitch.com/2011/07/wordpress-stats-or-google-analytics/&quot; target=&quot;_blank&quot;&gt;post&lt;/a&gt;, I discussed how it's useful to have _both_ WordPress Stats and Google Analytics installed on a WordPress blog; the former can be used as a quick dashboard to check in on the day's traffic, and the latter can be used for more advanced analysis. But in order to keep our datasets comparable, we need to track users in a similar manner.

## How do WordPress Stats and Google Analytics differ?

WordPress Stats doesn't track visits from logged in users, but Google Analytics doesn't provide this functionality out-of-the-box. It is possible to filter out traffic by IP address in Google Analytics using a &lt;a title=&quot;Custom Filters video - Google Analytics&quot; href=&quot;http://services.google.com/analytics/breeze/en/filters/index.html&quot; target=&quot;_blank&quot;&gt;Custom Filter&lt;/a&gt;, but this only works if you have a static IP address (many/most home users don't).

However, with some simple PHP code using the WordPress &lt;a title=&quot;WordPress Conditional Tags&quot; href=&quot;http://codex.wordpress.org/Conditional_Tags&quot; target=&quot;_blank&quot;&gt;conditional tag&lt;/a&gt; `is_user_logged_in()` we can mimic this behavior with Google Analytics, and no matter what browser you choose or what location you work from, as long as you are logged in to WordPress, Google Analytics won't track your visits.

## Modifying the header.php file

In order to add the conditional tag to our Google Analytics code, we need to modify the `header.php` file where we &lt;a title=&quot;Google Analytics for WordPress: Two Methods&quot; href=&quot;http://randyzwitch.com/2011/08/google-analytics-for-wordpress/&quot; target=&quot;_blank&quot;&gt;originally installed&lt;/a&gt; the Google tracking code.  What we're going to do is add the following code around our Google Analytics code, and WordPress will handle the &quot;logged in logic&quot; for us:

{% highlight php linenos %}
&lt;?php
if ( is_user_logged_in() ) {
} else {
}
?&gt;
{% endhighlight %}

The first clause of this code (the `IF` line) checks WordPress to see if you are logged in; if so, the code that is between the `{}` brackets will execute.  The second clause (the `ELSE` statement) is what happens if the user is NOT logged in.  Since we only want Google Analytics to track our visits when we're not logged in, we'll place our tracking code within the `ELSE` clause. When you have done this correctly, the code in your `header.php` file should look similar to the following:

{% highlight php linenos %}
&lt;?php
if ( is_user_logged_in() ) {
} else {
&lt;script type=&quot;text/javascript&quot;&gt;
var _gaq =_gaq || [];
 _gaq.push(['_setAccount', 'UA-XXXXXXXX-X']);
 _gaq.push(['_trackPageview']);
_gaq.push(['_trackPageLoadTime']);
(function() {
 var ga = document.createElement('script');
 ga.type = 'text/javascript';
 ga.async = true;
 ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
 var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga,s);
})();
&lt;/script&gt;
}
?&gt;
{% endhighlight %}

If you have an external JavaScript file holding your tracking code, the code would look like the following:

{% highlight php linenos %}
&lt;?php
if ( is_user_logged_in() ) {
} else {
 echo &quot;&lt;script type='text/javascript' src='http://yourdomain.com/your_ga_code_filename.js'&gt;&lt;/script&gt;&quot;;
}
?&gt;
{% endhighlight %}

## Are there any downsides to this method of implementing Google Analytics this way?

By implementing Google Analytics in this manner, you improve your data consistency between WordPress Stats and Google Analytics. You also avoid inflating your page views and visits, especially when doing blog development and re-designs or while previewing drafts of blog posts.

The one downside I've encountered so far is that if you are testing any tracking code changes themselves (such as adding `onclick` events to links/buttons to track additional data), you have to log in and log out every time you want to test whether your JavaScript code is firing your Google Analytics code properly.

Yes, that's a bit tedious, but I think it's small price to pay for better data consistency.

_Note:  If you are running a multi-user blog, you might consider using the [`is_admin()`](http://codex.wordpress.org/Function_Reference/is_admin &quot;is_admin WordPress Conditional Tag&quot;) conditional tag instead of `is_logged_in()`, especially if you want to understand how the non-Admin authors are interacting with your blog.  The possibilities are endless!_</content>
      </item>
      
    
      
      <item>
        <title>Removing &quot;Powered by WordPress&quot; in Twenty Eleven</title>
        
          <description>&lt;p&gt;So far in the child theme tutorials for the WordPress Twenty Eleven theme, we’ve created a &lt;a title=&quot;Twenty Eleven Child Theme: Creating CSS file&quot; href=&quot;http://randyzwitch.com/2011/07/twenty-eleven-child-theme-creating-css-file/&quot; target=&quot;_blank&quot;&gt;custom CSS&lt;/a&gt; file and &lt;a title=&quot;Twenty Eleven Child Theme:  Custom Header&quot; href=&quot;http://randyzwitch.com/2011/07/custom-header-twenty-eleven-child-theme/&quot; target=&quot;_blank&quot;&gt;custom header&lt;/a&gt;.  Now, let’s add a custom footer and remove “Powered by WordPress”.&lt;/p&gt;

</description>
        
        <pubDate>Sat, 20 Aug 2011 06:14:38 -0400</pubDate>
        <link>
        http://randyzwitch.com/removing-powered-by-wordpress-twenty-eleven/</link>
        <guid isPermaLink="true">http://randyzwitch.com/removing-powered-by-wordpress-twenty-eleven/</guid>
        <content type="html" xml:base="/removing-powered-by-wordpress-twenty-eleven/">So far in the child theme tutorials for the WordPress Twenty Eleven theme, we've created a &lt;a title=&quot;Twenty Eleven Child Theme: Creating CSS file&quot; href=&quot;http://randyzwitch.com/2011/07/twenty-eleven-child-theme-creating-css-file/&quot; target=&quot;_blank&quot;&gt;custom CSS&lt;/a&gt; file and &lt;a title=&quot;Twenty Eleven Child Theme:  Custom Header&quot; href=&quot;http://randyzwitch.com/2011/07/custom-header-twenty-eleven-child-theme/&quot; target=&quot;_blank&quot;&gt;custom header&lt;/a&gt;.  Now, let's add a custom footer and remove &quot;Powered by WordPress&quot;.

The first step to having a custom footer for your Twenty Eleven child theme is to copy the `footer.php` file from the parent Twenty Eleven theme into your directory.  You will find this file on your server here:

&lt;p style=&quot;padding-left: 30px;&quot;&gt;
  /wp-content/themes/twentyeleven
&lt;/p&gt;

Once you have copied this file over to your Twenty Eleven child theme folder (for me the location on my server is `/wp-content/themes/twentyeleven-child`), you are done!  You have a custom footer file...but on your blog, everything will look the same.  That's because while you technically have created a new footer for your blog, no code has actually changed. We'll fix that in the next section.

## Powered by WordPress!  Woo-hoo!

I can't fault WordPress for wanting to drive traffic to their website, especially given they provide the Twenty Eleven theme for free.  But that doesn't mean that &quot;Powered by WordPress&quot; in the footer doesn't look goofy.  Luckily, it's not too difficult to change.

Open up the &lt;del&gt;&lt;em&gt;header.php&lt;/em&gt;&lt;/del&gt; `footer.php` file from your child theme using your favorite text editor.  In this file, you'll find the following code:

{% highlight php linenos %}
&lt;?php do_action( 'twentyeleven_credits' ); ?&gt;
&lt;a href=&quot;&lt;?php echo esc_url( __( 'http://wordpress.org/', 'twentyeleven' ) ); ?&gt;&quot; title=&quot;&lt;?php esc_attr_e( 'Semantic Personal Publishing Platform', 'twentyeleven' ); ?&gt;&quot; rel=&quot;generator&quot;&gt;&lt;?php printf( __( 'Proudly powered by %s', 'twentyeleven' ), 'WordPress' ); ?&gt;&lt;/a&gt;
{% endhighlight %}

All we need to do to remove &quot;Powered by WordPress&quot; is to comment out this code using HTML comment tag, like the following:

{% highlight php linenos %}
&lt;!-- &lt;?php do_action( 'twentyeleven_credits' ); ?&gt;
&lt;a href=&quot;&lt;?php echo esc_url( __( 'http://wordpress.org/', 'twentyeleven' ) ); ?&gt;&quot; title=&quot;&lt;?php esc_attr_e( 'Semantic Personal Publishing Platform', 'twentyeleven' ); ?&gt;&quot; rel=&quot;generator&quot;&gt;&lt;?php printf( __( 'Proudly powered by %s', 'twentyeleven' ), 'WordPress' ); ?&gt;&lt;/a&gt; --&gt;
{% endhighlight %}

Once you hit save, you're all set.  No more &quot;Powered by WordPress&quot;, no more text at all. Just a blank footer.

## What if I want to put my own links in the footer?

Of course, just because you don't want &quot;Powered by WordPress&quot; in the footer doesn't mean you don't want any links in the footer.  If you want to add your own text/links, re-open your _footer.php_ file and place your text in the same location as the code we just commented out.

For example, if I wanted to put &quot;© Randy Zwitch - 2011&quot; in the footer, I would place this text between the opening and closing div tags for &quot;site generator&quot;:

{% highlight php linenos %}
&lt;div id=&quot;site-generator&quot;&gt;
&lt;!-- &lt;?php do_action( 'twentyeleven_credits' ); ?&gt;
&lt;a href=&quot;&lt;?php echo esc_url( __( 'http://wordpress.org/', 'twentyeleven' ) ); ?&gt;&quot; title=&quot;&lt;?php esc_attr_e( 'Semantic Personal Publishing Platform', 'twentyeleven' ); ?&gt;&quot; rel=&quot;generator&quot;&gt;&lt;?php printf( __( 'Proudly powered by %s', 'twentyeleven' ), 'WordPress' ); ?&gt;&lt;/a&gt; --&gt;

© Randy Zwitch - 2011
&lt;/div&gt;
{% endhighlight %}

It's as easy as that to have a custom footer message!  You can get even more tricky by widgets and ads and tag clouds...but there's something to be said for a clean, sleek design.  So I'll stop there 🙂</content>
      </item>
      
    
      
      <item>
        <title>Google Analytics for WordPress: Two Methods</title>
        
          <description>&lt;p&gt;There are two ways to install Google Analytics for WordPress:  manually in your theme or by using a plugin.  While most of this post will cover the manual installation of the tracking code, I will also briefly discuss some of the WordPress Google Analytics plugins I have used over the years.&lt;/p&gt;

</description>
        
        <pubDate>Tue, 16 Aug 2011 04:19:40 -0400</pubDate>
        <link>
        http://randyzwitch.com/google-analytics-for-wordpress/</link>
        <guid isPermaLink="true">http://randyzwitch.com/google-analytics-for-wordpress/</guid>
        <content type="html" xml:base="/google-analytics-for-wordpress/">There are two ways to install Google Analytics for WordPress:  manually in your theme or by using a plugin.  While most of this post will cover the manual installation of the tracking code, I will also briefly discuss some of the WordPress Google Analytics plugins I have used over the years.

Note:  This post isn't going to cover creating an account at Google, since most people have at least one already through Gmail, Google+, YouTube, Picasa, etc.  However, if you do need help setting up a Google Analytics account, there is a great video provided by Google as part of their Conversion University series:  [Creating a Google Analytics account](http://services.google.com/analytics/breeze/en/installing_ga_code/index.html &quot;Google Analytics Video&quot;)

## Finding your Google Analytics Account Number

Once you have your Google Analytics account set up, you've conquered probably 75% of your installation.  Hard to believe, right?  All you need now is your account number/profile from Google Analytics to plug in to the tracking code.

To find the proper code, sign in to Google Analytics and look for the following on the account home tab:

![google-analytics-account-number](/wp-content/uploads/2011/08/google-analytics-account-number.png)

I've obscured my number with red X's so that no one accidentally starts writing data to my account!  But if you see a code that starts with `UA-`, that's your account and profile number.

## Installing the default Google Analytics tracking code

With account number in hand, we're ready to install the tracking code in our WordPress header file, right before the `wp_head();` line (and definitely before the closing tag.)  Note: if you are using the Twenty Eleven theme, there are some comments about how removing the `wp_head` line will break WordPress plugins...this is the correct location.

Once you hit save to your `header.php` file, you're done. If all is well, you'll see the basic Google Analytics data starting to populate in your account in about 24 hours!

## Of course, you can always use the Google Analytics for WordPress plugin...

If it turns out this is too complicated (though, I hope not!), or your needs are more complicated than the 'standard' Google Analytics installation, there are myriad WordPress plugins that you can install.  In the past, I've used many different plugins for my Google Analytics tracking needs, but the one I feel is the best is provided by [Joost de Valk.](http://www.yoast.com &quot;Google Analytics for WordPress plugin&quot;)  His [plugin](http://yoast.com/wordpress/google-analytics/ &quot;Google Analytics for WordPress plugin&quot;), not surprisingly named &quot;Google Analytics for WordPress&quot;, will allow you to do some link tracking, custom variables, e-commerce tracking, and much more.  Of course, the downside to this is that you lose the flexibility of coding exactly what you what, how you want...but it's a good trade-off for those looking for simplicity.

I've also used &lt;a title=&quot;Google Analyticator&quot; href=&quot;http://ronaldheft.com/code/analyticator/&quot; target=&quot;_blank&quot;&gt;Google Analyticator&lt;/a&gt; in the past, and it too seemed to work pretty well.  I've never really done an A/B comparison of the two, but the feature set is pretty much the same, and both gentleman seem to be excellent WordPress web developers, so I doubt either plugin will cause any performance hit.

No matter which way you choose to get your Google Analytics fix on your WordPress blog, happy tracking!  If you're not measuring, you're not improving 🙂</content>
      </item>
      
    
      
      <item>
        <title>Removing Comment Bubble From Twenty Eleven Theme</title>
        
          <description>&lt;p&gt;If you’re like me, one of the things that seems odd about the WordPress Twenty Eleven theme is the comment bubble in the upper right of each post on the front page of the blog.  If you don’t know what I’m talking about, here’s a picture:&lt;/p&gt;

</description>
        
        <pubDate>Fri, 29 Jul 2011 17:23:16 -0400</pubDate>
        <link>
        http://randyzwitch.com/comment-bubble-twenty-eleven/</link>
        <guid isPermaLink="true">http://randyzwitch.com/comment-bubble-twenty-eleven/</guid>
        <content type="html" xml:base="/comment-bubble-twenty-eleven/">If you're like me, one of the things that seems odd about the WordPress Twenty Eleven theme is the comment bubble in the upper right of each post on the front page of the blog.  If you don't know what I'm talking about, here's a picture:

![wordpress-twenty-eleven-comment-bubble](/wp-content/uploads/2011/07/wordpress-twenty-eleven-comment-bubble.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
In the upper right, the comment bubble (highlighted blue) clutters up the front page
&lt;/p&gt;

Before we can remove the comment bubble, we first need to figure what this element is called within WordPress.  Luckily, this is quite easy using a tool that is included in all modern browsers.

## Finding the comment bubble's name is as easy as &quot;Point, Right-click, and Inspect Element&quot;

To find out what any HTML element is called on a page, all you need to do is to point to the element, then Right-Click on it and choose &quot;Inspect Element&quot;.  I'm using a Mac with the Firefox browser, so it looks like the following on my computer:

![firefox-5-inspect-element](/wp-content/uploads/2011/07/firefox-5-inspect-element.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
'Inspect Element' is one of the most useful tools in web development
&lt;/p&gt;

Once you click &quot;Inspect Element&quot;, you'll see that the comment bubble is located inside the `comment-link` section of our WordPress template code:

![comment-link](/wp-content/uploads/2011/07/comment-link.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
We need to look for &quot;comment-link&quot; in our WordPress template
&lt;/p&gt;

## Adding `content.php` to our WordPress child theme

Now that we've found what the comment bubble element is called, we need to find it in our WordPress child theme.  If you've been following along with the posts on this blog, we've already developed a [custom CSS file](http://randyzwitch.com/2011/07/twenty-eleven-child-theme-creating-css-file/ &quot;Twenty Eleven Child Theme: Creating CSS file&quot;) and a [custom header file](http://randyzwitch.com/2011/07/custom-header-twenty-eleven-child-theme/ &quot;Twenty Eleven Child Theme:  Custom Header&quot;).  To save you the agony, the WordPress file we need to modify is called `content.php`. Copy the `content.php` file from the base Twenty Eleven theme into your child theme directory.

Once you've copied this file to your directory, open it up in your favorite text editor.  What we're looking for is &quot;comment-link&quot;...using Command-F, we can see that it is contained in this file twice.  We want to modify the first one, as it controls the element at the top right of the post; the second &quot;comment-link&quot; refers to the link at the bottom of each post.

## When in doubt, comment it out!

Any time you are making changes in code, it's good practice to first 'comment out' the code you are modifying, so that you can 'uncomment-out' the code if the change isn't what you intended.  If you delete the code outright, before knowing what it does, you'll be scrambling when you delete the wrong line!

Using the HTML comment tag of &quot;Left arrow, exclamation point, two dashes&quot;, we can start commenting out at the first line of the code snippet (to the left of the &lt;? &lt;!--? symbol), and close the comment tag at the last line (to the right of the ?--&gt; symbol).  Use the &quot;Two dashes, Right arrow&quot; HTML comment tag to the right of the &quot;php endif&quot; statement.  When you are done, your code will look like the following:



Hit save and you're done!  No more comment bubble on your theme.  Of course, once you verify that the code is working correctly on your theme, you can delete these lines of code.  It's really up to you and your needs...if you think you might want to re-instate the comment bubble at a later date, leave the code.  If you're like me and don't like the comment bubble, then delete!</content>
      </item>
      
    
      
      <item>
        <title>WordPress Stats or Google Analytics? Yes!</title>
        
          <description>&lt;p&gt;To understand the success of your blog content and site design, you need actionable data on your visitors and how they are interacting with your site.  Whether to use WordPress Stats or Google Analytics (or both) to obtain this data depends on your goals.&lt;/p&gt;

</description>
        
        <pubDate>Fri, 22 Jul 2011 09:57:14 -0400</pubDate>
        <link>
        http://randyzwitch.com/wordpress-stats-or-google-analytics/</link>
        <guid isPermaLink="true">http://randyzwitch.com/wordpress-stats-or-google-analytics/</guid>
        <content type="html" xml:base="/wordpress-stats-or-google-analytics/">To understand the success of your blog content and site design, you need actionable data on your visitors and how they are interacting with your site.  Whether to use WordPress Stats or Google Analytics (or both) to obtain this data depends on your goals.

## WordPress Stats plugin

![wordpress-stats-dashboard](/wp-content/uploads/2011/07/wordpress-stats-dashboard.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
WordPress Stats dashboard
&lt;/p&gt;

If your goals are relatively basic in terms of understanding your blog's success, then there's no better place to get started than installing the WordPress Stats plugin. This plugin is part of the default &quot;Jetpack&quot; plugins installed with every version of WordPress...to activate it, go to the left-navigation menu under &quot;Jetpack&quot; and follow the instructions for activation.  You'll need an API key from WordPress.com, but they are free and easy to obtain.

Once installed, this plugin will let you know how many page views your content has generated on a daily/weekly/monthly basis.  It will also allow you to view which blog posts specifically are the most popular, which is great for understanding what your readers are interested in reading.

This plugin will also tell you what search terms readers are using to find your site in search engines, and any other blogs or websites that are linking to your site (known as &quot;referrers&quot;).  Like page views, knowing these search terms and referring sites will let you know the type of content visitors to your site are most interested in reading, because either a visitor was interested in learning more about a topic (search terms) or read an article on your site and wanted to share it with others (referring link).

As a casual blogger, you can do much worse than just monitoring these simple data points.  But if you want to really _analyze_ what's happening when visitors come to your site, you're going to need a bit more data collection.

## Google Analytics

Have you ever thought, &quot;I wonder where my readers are located geographically&quot; or &quot;Is my blog design compatible with different browsers, including mobile devices&quot;?  If so, then stepping up to Google Analytics might be worth your while.  While the amount of data provided by Google Analytics can be overwhelming in the beginning, once you start using the reporting interface for a few weeks, you'll gain a ton of insights.

![google-analytics-dashboard](/wp-content/uploads/2011/07/google-analytics-dashboard.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
Google Analytics dashboard from The Fuqua Experience
&lt;/p&gt;

For example, in a &lt;a title=&quot;Google Analytics geography report&quot; href=&quot;http://randyzwitch.com/measure-hashtag-twitter/&quot; target=&quot;_blank&quot;&gt;prior post&lt;/a&gt; I posted the geographical distribution of visitors to this blog after only 3 days.  By tagging my blog post link with Google campaign tracking, then posting that link to Twitter, I got amazing insight into how geographically diverse the contributors to the #measure hashtag are.  There were visitors from 17 countries that read my first blog post, something that would not have been possible to know without the extra horsepower that Google Analytics provides.  Sure, there's not a whole lot of _intent_ I can ascertain from the geographic distribution after 3 days, but the geographical distribution is something I can monitor over time to see what trends might be present.

## WordPress Stats or Google Analytics?  Yes!

Up to this point, I haven't been very precise about what constitutes a &quot;simple&quot; metric such as page views, or how to know when you need the extra &quot;horsepower&quot; that installing Google Analytics provides.  The reason for my imprecision is that the decision to install either tracking code shouldn't be an &quot;either/or&quot; decision, but rather an &quot;and&quot;.  If you're running a self-hosted WordPress blog, in my opinion you should be running both WordPress Stats &lt;span style=&quot;text-decoration: underline;&quot;&gt;and&lt;/span&gt; Google Analytics!

Yes, the data provided by Google Analytics is a superset of the information provided by WordPress Stats; thus, you don't gain any additional insight from having WordPress Stats installed.  What you do gain by having both installed is convenience, and as far as I can tell there is no performance degrade to a site having both running at the same time.

So when you need a quick snapshot of what your blog has done in the past several weeks, or you want to get an idea of your most popular content _while in your WordPress admin panel_, the WordPress Stats plugin will do just that.  When you want to get a deeper insight of how several factors are interacting to create your successful blog, sign in to Google Analytics.

But above all else, remember:  the data doesn't do anything because it's being recorded.  You need to _study it_ to unlock the value!</content>
      </item>
      
    
      
      <item>
        <title>Twenty Eleven Child Theme: Custom Header</title>
        
          <description>&lt;p&gt;In a previous &lt;a href=&quot;http://randyzwitch.com/twenty-eleven-child-theme-creating-css-file/&quot;&gt;post&lt;/a&gt; about creating a child theme for Twenty-Eleven, we created a custom CSS file using the &lt;a title=&quot;Import CSS rule&quot; href=&quot;http://www.w3.org/TR/CSS2/cascade.html#at-import&quot; target=&quot;_blank&quot;&gt;@import CSS rule&lt;/a&gt;.  Technically, the new CSS file is all that is needed to create a child theme in WordPress, but &lt;em&gt;just&lt;/em&gt; changing the colors and font styles isn’t that much fun.  If it’s worth making your own WordPress child theme, it’s worth changing _everything…_starting with creating a custom header file.&lt;/p&gt;

</description>
        
        <pubDate>Wed, 20 Jul 2011 09:16:53 -0400</pubDate>
        <link>
        http://randyzwitch.com/custom-header-twenty-eleven-child-theme/</link>
        <guid isPermaLink="true">http://randyzwitch.com/custom-header-twenty-eleven-child-theme/</guid>
        <content type="html" xml:base="/custom-header-twenty-eleven-child-theme/">In a previous [post](http://randyzwitch.com/twenty-eleven-child-theme-creating-css-file/) about creating a child theme for Twenty-Eleven, we created a custom CSS file using the &lt;a title=&quot;Import CSS rule&quot; href=&quot;http://www.w3.org/TR/CSS2/cascade.html#at-import&quot; target=&quot;_blank&quot;&gt;@import CSS rule&lt;/a&gt;.  Technically, the new CSS file is all that is needed to create a child theme in WordPress, but _just_ changing the colors and font styles isn't that much fun.  If it's worth making your own WordPress child theme, it's worth changing _everything..._starting with creating a custom header file.

## Creating a custom header file for our child theme

Creating a custom header file in WordPress is even simpler than creating a custom style sheet, because this time we'll just copy the file directly from the base Twenty-Eleven theme directly into our child theme.

  1. Go to your `/wp-content/themes/twentyeleven` directory and locate the file `header.php`
  2. Paste the `header.php` file to your `/wp-content/themes/twentyeleven-child` directory

That's it!  Not very exciting...so let's do some additional code modification while we're in the new file.

## Changing header image and size

Of all of the great things about the WordPress Twenty-Eleven theme, the one thing I can't understand is why WordPress decided to make the header image so damn big!  There is the option to change the image you display to any number of provided images, or upload your own as part of the Appearance &gt; Header menu.  But you can't change the size...if your upload is too short, the theme will stretch it for you, distorting the image.  If your upload is too large, you'll have to crop it. Luckily, with a minor modification we can change that.

First, go your newly created header.php file and find the following code:

What we're going to do is use the &lt;a title=&quot;Comment out code&quot; href=&quot;http://www.w3schools.com/tags/tag_comment.asp&quot; target=&quot;_blank&quot;&gt;commenting tag&lt;/a&gt; in HTML to &quot;comment out&quot; the portions of our header code that we no longer want to be active (i.e. the re-sizing feature in the Twenty-Eleven theme).  We can do this by adding the following tag (a left arrow, and exclamation point, and two dashes):

You're not required to put in the &quot;Comment out WordPress header checking&quot; text as I did, but it is good practice to notate why changes have been made to your code.  A little documentation now goes a long way a year from now, when you can't remember why you changed something! Now that we've got the opening tag for commenting, we need to find the end of the code we want to comment out, and put the closing tag.  Once we do this, all of the code between the commenting tags will not get executed by our browser.

The closing tag above is the two dashes, then the right arrow.  Again, you don't have to write &quot;End of RZ commenting out&quot;, but it's good for documentation.  Save your header.php file to make the completed changes.

## Hey, now I don't have ANY banner!

If you refreshed your blog at this point, you'll realize that not only did we get rid of the massive size of the banner, we got rid of the banner overall!  That's not the goal of this tutorial...so let's add the banner back, except now we'll control the sizing.

First thing you need to do is make sure you have the banner you want uploaded to your server, and that it is the size you want.  This blog is 1000 pixels wide at the body (white space), and I decided to make my banner 225 pixels tall, because that's where the picture looked good.

Once you have your banner location, we need to add the HTML link to our image, using the image tag.  For the &quot;alt&quot; section, you should put a description of what your banner represents, as this will be displayed if the users browser can't download the image.  In the &quot;src&quot; section, add the location of your banner file.

Once you hit save, that's it!  You now have control over your banner image.  Yes, modifying the code in this manner disables the Appearance &gt; Header functionality in the theme, but that's a small price to pay to get flexibility.  If you want to change your banner in the future to show a different image, you'll need to do so in the header.php file.  But since you've documented this change in your code, you'll know where to look and what to change when that time comes...

![randyzwitchdotcom-before](/wp-content/uploads/2011/07/randyzwitchdot-before.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
Before - 1000px by 288px image
&lt;/p&gt;

![randyzwitchdotcom-after](/wp-content/uploads/2011/07/randyzwitchdotcom-after-300x169.png)

&lt;p class=&quot;wp-caption-text&quot;&gt;
After: 1000px by 225 px
&lt;/p&gt;</content>
      </item>
      
    
      
      <item>
        <title>Have Social 'Influence' Scores Become Another FICO?</title>
        
          <description>&lt;p&gt;&lt;img src=&quot;/wp-content/uploads/2011/07/zwitch-klout-july-2011.png&quot; alt=&quot;zwitch-klout-july-2011&quot; /&gt;&lt;/p&gt;

</description>
        
        <pubDate>Wed, 13 Jul 2011 07:52:32 -0400</pubDate>
        <link>
        http://randyzwitch.com/social-influence-another-fico/</link>
        <guid isPermaLink="true">http://randyzwitch.com/social-influence-another-fico/</guid>
        <content type="html" xml:base="/social-influence-another-fico/">![zwitch-klout-july-2011](/wp-content/uploads/2011/07/zwitch-klout-july-2011.png)

  &lt;p class=&quot;wp-caption-text&quot;&gt;
    Klout thinks I'm a 'Networker'
  &lt;/p&gt;

Not a day goes by without another article being published about how social media will change business forever.  Several companies have sprung up in the past several years including &lt;a title=&quot;Klout&quot; href=&quot;http://klout.com&quot; target=&quot;_blank&quot;&gt;Klout&lt;/a&gt;, &lt;a title=&quot;Twitalyzer&quot; href=&quot;http://twitalyzer.com/&quot; target=&quot;_blank&quot;&gt;Twitalyzer&lt;/a&gt;, and &lt;a title=&quot;Peer Index&quot; href=&quot;http://peerindex.net&quot; target=&quot;_blank&quot;&gt;PeerIndex&lt;/a&gt; that attempt to measure the value of social media usage, or more broadly, 'social influence'.  As I read articles about how social influence is now used by companies to '&lt;a title=&quot;Facebook Fan Gating&quot; href=&quot;http://blog.milestoneinternet.com/industry-news/facebook-fan-gates/&quot; target=&quot;_blank&quot;&gt;Fan-gate&lt;/a&gt;' or '&lt;a title=&quot;Klout gate&quot; href=&quot;http://mashable.com/2011/06/22/klout-gate/&quot; target=&quot;_blank&quot;&gt;Klout-gate&lt;/a&gt;' their pages with special content and offers, I can't help but draw a comparison to the ubiquitous FICO credit score.

## FICO:  Likelihood a customer will go &lt;span style=&quot;text-decoration: underline;&quot;&gt;90 days delinquent within 2 years&lt;/span&gt;

What was once just use to determine credit-worthiness, FICO has morphed into a way to customize car insurance rates, evaluate candidates for job openings, decide whether to rent an apartment to a tenant, etc.  While arguments have be made that there is a _correlation_ between low FICO scores and a lot of undesirable behaviors, it's quite another to blindly segment customers using credit attributes for non-credit purposes.  Yet this behavior happens all the time...

## Social influence score:  &quot;The probability of...&quot;, what exactly?

The problem with trying to assign a value to social media interactions is that it's completely business-specific.  Unlike FICO, which at least has a strict definition (ignored as it may be), social 'influence' can mean any number of things, depending on whether the person uses social media for work or pleasure (and in many cases, both).  Even better, the number can be gamed depending on which accounts you allow to get scored as part of your 'influence' (although, adding accounts only leads to increases in your score...for now).

![twitalyzer-zwitch-july2011](/wp-content/uploads/2011/07/twitalyzer-zwitch-july2011.png)

Despite these different social influence score shortcomings, it is easy to see why companies like Audi, Subway, TNT Network, and others are willing to take a gamble that social influence (in this case, Klout score) has a correlation to _something_; as multi-million/billion dollar companies, the only way to get top-line growth is to experiment with new channels.  As it stands now, I'm having a hard time believing that the Klout 'Perks' is an effective way to market (that's a whole 'nother blog post!), but again, I can't fault companies for trying.

## Sanity still prevailing...

While I can see a parallel of social influence scores and FICO, luckily industry practitioners (the web analysts and marketers most likely experimenting in these new channels) are speaking out &lt;a title=&quot;Measure Mob focus on value&quot; href=&quot;http://www.measuremob.com/2011/06/stop-focusing-on-bullshit-social-metrics-and-start-focusing-on-real-value/&quot; target=&quot;_blank&quot;&gt;pretty loudly&lt;/a&gt; about understanding the positives and the cautions behind these scores.

I'm also glad to see (at least in the case of Twitalyzer), score providers [participating in the conversation](http://blog.twitalyzer.com/ &quot;Twitalyzer blog&quot;) to discuss the issues surrounding [the use of social influence scores](http://blog.twitalyzer.com/2011/02/companies-must-not-rely-on-a-single-score/ &quot;Dont rely on single influence score&quot;) in general.   Eric certainly has a lot of clout (pun intended!) in the web analytics community, so the message is definitely being heard there...but it's up to all of us measurement folks to get the message out further in the marketing community on the proper usage of any model score.

## Not FICO...not now, not ever

Ultimately, social influence scores will never achieve the level of widespread abuse that the FICO score has seen in the business world.  For one, there's the voluntary nature of social media, which keeps large populations of people from ever being scored.  There's also the fact that social influence is only calculated based on 'affirmative' activity (people 'Like' your contributions, they retweeted your articles, etc.), which cannot never be as predictive as also including negative interactions (like the FICO score does with missed payments).

But just because social influence scores _probably won't_ get abused in the same way, that doesn't mean that us digital measurers should relax.  It's up to us to make sure to keep stressing that just because companies _can_ do something, doesn't mean they _should_!  If anything, Kenneth Cole's PR disaster should show that not all 'influence' is good, even if it makes your [Klout score go up by 30 points](http://www.web-strategist.com/blog/2011/02/21/klout-for-business-a-sometimes-useful-metric-but-an-incomplete-view-of-customers/ &quot;Klout sometimes useful but not always&quot;)!

_&quot;Without any clear strategy around what you’re going to DO with all these fans – you’re really just kind of a **Facebook Marketing ho**, with no direction_.&quot;  - &lt;a title=&quot;Facebook Marketing ho&quot; href=&quot;http://digitalanalytics101.com/?p=103&quot; target=&quot;_blank&quot;&gt;digitalanalytics101.com&lt;/a&gt;

&lt;span style=&quot;text-decoration: underline;&quot;&gt;UPDATE - 10/27/2011:&lt;/span&gt;  With Klout &lt;a title=&quot;Accuracy AND Stability make for a good model&quot; href=&quot;http://corp.klout.com/blog/2011/10/a-more-accurate-transparent-klout-score/&quot; target=&quot;_blank&quot;&gt;making a change&lt;/a&gt; to their algorithm yesterday, and many heavy social media users seeing large drops in their scores, it seems like there ARE &lt;a title=&quot;Klout score is not FICO&quot; href=&quot;http://dannybrown.me/2011/10/26/a-klout-upside-the-head/&quot; target=&quot;_blank&quot;&gt;businesses and industry practitioners&lt;/a&gt; trying to use Klout as a pseudo-FICO score.  While my score dropped about 20% (from 51 to 40), I'm like most who see the whole &quot;social influence&quot; scoring as nothing more than an amusing game.</content>
      </item>
      
    
      
      <item>
        <title>Twenty Eleven Child Theme: Creating CSS File</title>
        
          <description>&lt;p&gt;With the Twenty Eleven child theme directory &lt;a title=&quot;Picking a WordPress Theme:  Fancy or Basic?&quot; href=&quot;http://randyzwitch.com/wordpress-theme-twentyeleven-athualpa/&quot; target=&quot;_blank&quot;&gt;created&lt;/a&gt; as “twentyeleven-child” in the ‘/wp-content/themes’ directory, we’re now ready to create the first file as part of the child theme: the CSS file.&lt;/p&gt;

</description>
        
        <pubDate>Sun, 10 Jul 2011 08:37:42 -0400</pubDate>
        <link>
        http://randyzwitch.com/twenty-eleven-child-theme-creating-css-file/</link>
        <guid isPermaLink="true">http://randyzwitch.com/twenty-eleven-child-theme-creating-css-file/</guid>
        <content type="html" xml:base="/twenty-eleven-child-theme-creating-css-file/">With the Twenty Eleven child theme directory &lt;a title=&quot;Picking a WordPress Theme:  Fancy or Basic?&quot; href=&quot;http://randyzwitch.com/wordpress-theme-twentyeleven-athualpa/&quot; target=&quot;_blank&quot;&gt;created&lt;/a&gt; as &quot;twentyeleven-child&quot; in the '/wp-content/themes' directory, we're now ready to create the first file as part of the child theme: the CSS file.

While we _could_ just copy the existing CSS file from the Twenty Eleven theme (`style.css` in the `/wp-content/themes/twentyeleven directory`) and modify that file directly, if we do that then we will lose track of the changes that were made relative to the original Twenty Eleven styling.  So what we're going to do is create a new CSS file to hold our changes, with a reference back to the original CSS file.

## Step 1:  Creating a blank CSS file

To start the process of creating the blank CSS file, open up your favorite plain text editor such as TextPad if you are using a PC or TextEdit if you are using a Mac.  It's important to use a plain text editor and not a word processor such as Microsoft Word, as Word can add strange characters into your file.

So with a new, blank text document open, save this file with the name `style.css`.  Your file MUST be named style.css in order to work correctly.

## Step 2:  Creating template/theme header

In the newly created style.css file, add the following code (including the /\*  and \*/ characters):

&lt;pre&gt;/* Theme Name: randyzwitch.com
Description: Child theme for the twentyeleven theme
Author: Randy Zwitch
Template: twentyeleven */&lt;/pre&gt;

For your theme, you can choose to put whatever you want in the &quot;Theme Name&quot;, &quot;Description&quot; and &quot;Author&quot; files.  The text you list here is what will be visible in the Appearance &gt; Themes menu in the WordPress admin panel.

If you are creating your child theme based on Twenty Eleven, then you are done, as no additional changes need to be made.  Otherwise, place the **directory** name (with no leading or trailing slashes) of the theme are using on the &quot;Template&quot; line.

**Step 3:  Include &quot;CSS import&quot; of Twenty Eleven CSS file

**

The final step in creating the CSS file for our new child theme is to import the CSS properties from the original Twenty Eleven theme.  Note that above, I discussed that we _could_ just copy the entire CSS file into our new file, but then we wouldn't be able to see our changes as a stand-alone.  What we can do instead is add the following code to our new file, directly under the header we created in Step 2:

&gt; @import url(&quot;../twentyeleven/style.css&quot;);

It's important to note that there can be no CSS code above this &quot;import&quot; line (other than the header file info).

And that's it!  Save your newly created CSS file, and be sure that it is located in the directory '/wp-content/themes/twentyeleven-child'

## Final comments

If you've done this correctly, the contents of your new CSS file should have the following format (with your information, obviously):

{% highlight php linenos %}
/* Theme Name: randyzwitch.com
Description: Child theme for the twentyeleven theme
Author: Randy Zwitch
Template: twentyeleven */

@import url(&quot;../twentyeleven/style.css&quot;);
{% endhighlight %}

Technically, this is all you need to have a fully-functional &quot;child theme&quot; in WordPress.  To make sure that everything is working correctly, go to the WordPress admin panel and select your new theme under Appearance &gt; Themes and hit 'Activate' your new theme!

_For more information about creating CSS files for child themes in WordPress, go to the &lt;a title=&quot;Wordpress Codex: Child Theme&quot; href=&quot;http://codex.wordpress.org/Child_Themes#The_required_style.css_file&quot; target=&quot;_blank&quot;&gt;CSS section&lt;/a&gt; of the WordPress Codex on this topic._</content>
      </item>
      
    
      
      <item>
        <title>Picking a WordPress Theme: Fancy or Basic?</title>
        
          <description>&lt;h2 id=&quot;a-complicated-wordpress-theme-may-look-better&quot;&gt;A complicated WordPress theme may LOOK better…&lt;/h2&gt;

</description>
        
        <pubDate>Sat, 09 Jul 2011 03:43:20 -0400</pubDate>
        <link>
        http://randyzwitch.com/wordpress-theme-twentyeleven-athualpa/</link>
        <guid isPermaLink="true">http://randyzwitch.com/wordpress-theme-twentyeleven-athualpa/</guid>
        <content type="html" xml:base="/wordpress-theme-twentyeleven-athualpa/">## A complicated WordPress theme may LOOK better...

As I referenced in a prior post&lt;/a&gt;, when I was getting [The Fuqua Experience](http://the-fuqua-experience.com) (TFE) off the ground, I tried to find good-looking themes to complement my writing.  For a while, I used the [Cutline 3-column split](http://cutline.tubetorial.com/) theme, because I liked the spartan nature of the theme.  I figured that all that white-space could be filled up with crazy tag-cloud widgets, Google Adsense ads, Twitter feeds...the web equivalent of Tufte's &quot;&lt;a title=&quot;Business Week Chartjunk&quot; href=&quot;http://images.businessweek.com/ss/09/06/0608_tufte/7.htm&quot; target=&quot;_blank&quot;&gt;chartjunk&lt;/a&gt;.&quot;

Once I moved away from the &quot;must put every widget and plugin everywhere&quot; mentality, I switched to the &lt;a title=&quot;Atahualpa theme&quot; href=&quot;http://wordpress.bytesforall.com/?p=102&quot; target=&quot;_blank&quot;&gt;Atahualpha&lt;/a&gt; theme.  This is an amazing theme that allowed me to get TFE into a more professional, magazine style layout. I started thinking about search engine optimization (due to the included functionality), as well as customizing the hell out of every single font and layout attribute.  I started getting into using WordPress &lt;a title=&quot;Wordpress shortcodes&quot; href=&quot;http://codex.wordpress.org/Shortcode&quot; target=&quot;_blank&quot;&gt;shortcodes&lt;/a&gt;, adding some additional social networking features like a Twitter widget and some special RSS buttons, and learning a lot along the way.

## ...but eventually you run out of flexibility

While complex themes like Atahualpa are great, eventually you come upon a problem that you can't solve.  For me, it was adding a JavaScript `onclick` event to the &quot;Read More&quot; tag in WordPress (for tracking in Google Analytics).  Because of how the theme is set up, you can go right into the options area and modify what text is shown on the link (say, &quot;_Continue Reading_&quot; or &quot;_Read this, Fool!_&quot; instead of &quot;Read More&quot;), but you can't put JavaScript in that box.

After contacting the theme developer, I still haven't solved that problem!  So rather than make the same mistake again, I sought out the easiest, cleanest theme I could:  [Twenty Eleven](http://theme.wordpress.com/themes/twentyeleven/ &quot;Twenty Eleven&quot;), designed by WordPress.  Compared to Cutline and (definitely) Atahualpa, Twenty Eleven looks like it is written in English instead of PHP!

## Creating a Child Theme

Before I discuss the modifications I've made to the Twenty Eleven theme thus far, it's important to talk about creating a &quot;child&quot; theme.  WordPress has a great functionality where you can use one theme as the base for another theme.  The benefit to doing this is that when the theme developer updates the theme, your modifications don't get over written.

If you're following along looking to create a child theme, the exact makeup of how child themes work doesn't matter at this point.  The only thing you need to do is:

  1. Navigate to the directory where you've installed your theme (likely /wp-content/themes)
  2. Create a new directory with a meaningful name (such as twentyeleven-child)

And that's it!  You're on your way to creating your own theme.  In the next post, I'll outline how you start creating a theme that reflects your style, starting with the CSS file.</content>
      </item>
      
    
  </channel>
</rss>
