---
id: 3763
title: A Million Text Files And A Single Laptop
date: 2016-01-28T09:53:42+00:00
author: Randy Zwitch
layout: post
guid: http://randyzwitch.com/?p=3763
permalink: /gnu-parallel-medium-data/
tweetbackscheck:
  - 1472934765
shorturls:
  - 'a:3:{s:9:"permalink";s:30:"http://randyzwitch.com/?p=3763";s:7:"tinyurl";s:26:"http://tinyurl.com/htak7zw";s:4:"isgd";s:19:"http://is.gd/RCjxqP";}'
twittercomments:
  - 'a:0:{}'
tweetcount:
  - 0
categories:
  - General Programming
tags:
  - Bash
  - Julia
  - Python
  - R
  - Scripting
  - Unix
---
<div id="attachment_3770" style="width: 410px" class="wp-caption alignright">
  <img class="wp-image-3770 size-full" src="http://i0.wp.com/randyzwitch.com/wp-content/uploads/2016/01/million-files-size.png?fit=400%2C227" alt="GNU Parallel Cat Unix" srcset="http://i0.wp.com/randyzwitch.com/wp-content/uploads/2016/01/million-files-size.png?w=400 400w, http://i0.wp.com/randyzwitch.com/wp-content/uploads/2016/01/million-files-size.png?resize=150%2C85 150w, http://i0.wp.com/randyzwitch.com/wp-content/uploads/2016/01/million-files-size.png?resize=300%2C170 300w" sizes="(max-width: 400px) 100vw, 400px" data-recalc-dims="1" />
  
  <p class="wp-caption-text">
    Wait&#8230;What? Why?
  </p>
</div>

More often that I would like, I receive datasets where the data has only been partially cleaned, such as the picture on the right: hundreds, thousands&#8230;even millions of tiny files. Usually when this happens, the data all have the same format (such as having being generated by sensors or other memory-constrained devices).

The problem with data like this is that 1) it&#8217;s inconvenient to think about a dataset as a million individual pieces 2) the data in aggregate are too large to hold in RAM but 3) the data are small enough where using Hadoop or even a relational database seems like overkill.

Surprisingly, with judicious use of <a href="http://www.gnu.org/software/parallel/" target="_blank">GNU Parallel</a>, stream processing and a relatively modern computer, you can efficiently process annoying, &#8220;medium-sized&#8221; data as described above.

<!--more-->

## Data Generation

For this blog post, I used a <a href="https://gist.github.com/randyzwitch/c44ff2a76d81fa1e77cb" target="_blank">combination of R and Python to generate the data</a>: the &#8220;Groceries&#8221; dataset from the _<a href="https://cran.r-project.org/web/packages/arules/vignettes/arules.pdf" target="_blank">arules</a>_ package for sampls ing transactions (with replacement), and the Python _<a href="https://github.com/joke2k/faker" target="_blank">Faker (fake-factory)</a>_ package to generate fake customer profiles and for creating the 1MM+ text files.

The contents of the data itself isn&#8217;t important for this blog post, but the <a href="https://gist.github.com/randyzwitch/c44ff2a76d81fa1e77cb" target="_blank">data generation code is posted as a GitHub gist</a> should you want to run these commands yourself.
  

  


## Problem 1: Concatenating (cat * >> out.txt ?!)

The <a href="http://man7.org/linux/man-pages/man1/cat.1.html" target="_blank">cat</a> utility in Unix-y systems is familiar to most anyone who has ever opened up a Terminal window. Take some or all of the files in a folder, concatenate them together&#8230;.one big file. But something funny happens once you get enough files&#8230;

<pre class="p1"><span class="s1">$ cat * &gt;&gt; out.txt</span></pre>

<pre class="p1"><span class="s1">-bash: /bin/cat: Argument list too long</span></pre>

That&#8217;s a fun thought&#8230;too many files for the computer to keep track of. As it turns out, many Unix tools will only accept about 10,000 arguments; the use of the asterisk in the \`cat\` command gets expanded before running, so the above statement passes 1,234,567 arguments to \`cat\` and you get an error message.

One (naive) solution would be to loop over every file (a completely serial operation):

<pre>for f in *; do cat "$f" &gt;&gt; ../transactions_cat/transactions.csv; done</pre>

Roughly **10,093 seconds** later, you&#8217;ll have your concatenated file. Three hours is quite a coffee break&#8230;

## Solution 1: GNU Parallel & Concatenation

Above, I mentioned that looping over each file gets you past the error condition of too many arguments, but it is a serial operation. If you look at your computer usage during that operation, you&#8217;ll likely see that only a fraction of a core of your computer&#8217;s CPU is being utilized. We can greatly improve that through the use of GNU Parallel:

<pre>ls | parallel -m -j $f "cat {} &gt;&gt; ../transactions_cat/transactions.csv"</pre>

The \`$f\` argument in the code is to highlight that you can choose the level of parallelism; however, you will not get infinitely linear scaling, as shown below ([graph code, Julia](https://gist.github.com/randyzwitch/ee0f738b5895e059fa2a)):

<div id="cat">
</div>

Given that the graph represents a single run at each level of parallelism, it&#8217;s a bit difficult to say _exactly_ where the parallelism gets maxed out, but at roughly 10 concurrent jobs, there&#8217;s no additional benefit. It&#8217;s also interesting to point out what the \`-m\` argument represents; by specifying \`m\`, you allow multiple arguments (i.e. multiple text files) to be passed as inputs into parallel. This _alone_ leads to an 8x speedup over the naive loop solution.

## Problem 2: Data > RAM

Now that we have a single file, we&#8217;ve removed the &#8220;one million files&#8221; cognitive dissonance, but now we have a second problem: at 19.93GB, the amount of data exceeds the RAM in my laptop (2014 MBP, 16GB of RAM). So in order to do analysis, either a bigger machine is needed or processing has to be done in a streaming or &#8220;chunked&#8221; manner (such as using the [&#8220;chunksize&#8221; keyword in pandas](http://pandas.pydata.org/pandas-docs/stable/io.html#iterating-through-files-chunk-by-chunk)).

But continuing on with our use of GNU Parallel, suppose we wanted to answer the following types of questions about our transactions data:

  1. How many unique products were sold?
  2. How many transactions were there per day?
  3. How many total items were sold per store, per month?

If it&#8217;s not clear from the list above, in all three questions there is an &#8220;embarrassingly parallel&#8221; portion of the computation. Let&#8217;s take a look at how to answer all three of these questions in a time- and RAM-efficient manner:

##### Q1: Unique Products

Given the format of the data file (transactions in a single column array), this question is the hardest to parallelize, but using a neat trick with the \`[tr](http://www.linfo.org/tr.html)\` (transliterate) utility, we can map our data to one product per row as we stream over the file:
  

   
The trick here is that we swap the comma-delimited transactions with the newline character; the effect of this is taking a single transaction row and returning multiple rows, one for each product. Then we pass that down the line, eventually using \`sort -u\` to de-dup the list and \`wc -l\` to count the number of unique lines (i.e. products).

In a serial fashion, it takes quite some time to calculate the number of unique products. Incorporating GNU Parallel, just using the defaults, gives nearly a 4x speedup!

##### Q2. Transactions By Day

If the file format could be considered undesirable in question 1, for question 2 the format is perfect. Since each row represents a transaction, all we need to do is perform the equivalent of a SQL \`Group By\` on the date and sum the rows:

Using GNU Parallel starts to become complicated here, but you do get a 9x speed-up by calculating rows by date in chunks, then &#8220;reducing&#8221; again by calculating total rows by date (a trick I picked up at this <a href="http://www.rankfocus.com/use-cpu-cores-linux-commands/" target="_blank">blog post</a>).

##### Q3. Total items Per store, Per month

For this example, it could be that my command-line fu is weak, but the serial method actually turns out to be the fastest. Of course, at a 14 minute run time, the real-time benefits to parallelization aren&#8217;t that great.

It may be possible that one of you out there knows how to do this correctly, but an interesting thing to note is that the serial version already uses 40-50% of the available CPU available. So parallelization might yield a 2x speedup, but seven minutes extra per run isn&#8217;t worth spending hours trying to the optimal settings.

## But, I&#8217;ve got MULTIPLE files&#8230;

The three examples above showed that it&#8217;s possible to process datasets larger than RAM in a realistic amount of time using GNU Parallel. However, the examples also showed that working with Unix utilities can become complicated rather quickly. Shell scripts can help move beyond the &#8220;one-liner&#8221; syndrome, when the pipeline gets so long you lose track of the logic, but eventually problems are more easily solved using other tools.

The data that I generated at the beginning of this post represented two concepts: transactions and customers. Once you get to the point where you want to do joins, summarize by multiple columns, estimate models, etc., loading data into a database or an analytics environment like R or Python makes sense. But hopefully this post has shown that a laptop is capable of analyzing WAY more data than most people believe, using many tools written decades ago.